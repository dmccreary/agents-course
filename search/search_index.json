{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Welcome to our website.</p>"},{"location":"checklist/","title":"Site Checklist","text":"<ol> <li>Customize the fields in your mkdocs.yml file</li> <li>Configure Google Analytics to use the right site ID</li> <li>Make sure that your .gitignore file includes the <code>site</code> directory</li> <li>Test the build</li> <li>Make sure the Edit button appears</li> <li>Make sure that code color heightening renders correctly</li> <li>run <code>git config advice.addIgnoredFile false</code></li> </ol>"},{"location":"code-highlight-test/","title":"Code Syntax Color Highlight Test","text":""},{"location":"code-highlight-test/#python","title":"Python","text":"<pre><code>hello_string = \"Hello World!\"\nprint(hello_string)\nx = 1\nif x == 1:\n    # indented four spaces\n    print(\"x is 1.\")\n</code></pre>"},{"location":"contact/","title":"Contact","text":"<p>Please contact me on LinkedIn</p> <p>Thanks! - Dan</p>"},{"location":"feedback/","title":"Feedback on Graph Data Modeling","text":"<p>You are welcome to connect with me on anytime on LinkedIn or submit any issues to GitHub Issue Log.  All pull-requests with fixes to errors or additions are always welcome.</p> <p>If you would like to fill out a short survey and give us ideas on how we can create better tools for intelligent textbooks in the future.</p>"},{"location":"glossary/","title":"Glossary of Terms","text":""},{"location":"glossary/#agent","title":"Agent","text":"<p>An autonomous computational entity that performs tasks on behalf of users or other programs with some degree of independence or autonomy, and in doing so, employs a precise knowledge representation of the user's goals or desires.</p> <p>Agents are similar to the use of complex rules engines but they have a higher degree of independence or autonomy.</p> <ul> <li>See also: Rules Engine</li> </ul>"},{"location":"glossary/#llm-agents","title":"LLM Agents","text":"<p>A specialized type of software agent where LLM outputs control the workflow.</p> <p>Note that \"agency\" evolves on a continuous spectrum, as you give more or less power to the LLM on your workflow.</p>"},{"location":"glossary/#deepseek-r1","title":"DeepSeek R1","text":""},{"location":"glossary/#iso-definition","title":"ISO Definition","text":"<p>A term definition is considered to be consistent with ISO metadata registry guideline 11179 if it meets the following criteria:</p> <ol> <li>Precise</li> <li>Concise</li> <li>Distinct</li> <li>Non-circular</li> <li>Unencumbered with business rules</li> </ol>"},{"location":"glossary/#knowledge-representation","title":"Knowledge Representation","text":"<p>The methods and structures used to encode information about the world.</p> <p>In this book, we focus on knowledge representation in a form that an agent can utilize for reasoning and decision-making. It provides the foundation for an agent's understanding of its environment, goals, and possible actions.</p> <p>Knowledge representation encompasses:</p> <ol> <li> <p>Symbolic Structures: Formalized frameworks such as ontologies, semantic networks, frames, rules, or logic-based systems that capture entities, relationships, and constraints in the agent's domain.</p> </li> <li> <p>Information Organization: Methods for structuring knowledge in ways that facilitate efficient access, inference, and updates as the agent interacts with its environment.</p> </li> <li> <p>Reasoning Mechanisms: Techniques that allow agents to derive new knowledge from existing representations, evaluate truth values, and make decisions based on incomplete information.</p> </li> <li> <p>Semantic Grounding: Connections between abstract symbols and their real-world referents, allowing the agent to map internal representations to external phenomena.</p> </li> </ol> <p>Effective knowledge representation systems in agents balance expressiveness (the ability to represent complex knowledge) with computational efficiency (allowing the agent to reason within reasonable time and resource constraints).</p>"},{"location":"glossary/#large-language-model","title":"Large Language Model","text":""},{"location":"glossary/#llm-agent-control-levels","title":"LLM Agent Control Levels","text":"<p>The amount you allow an LLM to control agentic execution. Her are some sample levels:</p> <p>Level 0 - LLMs have no control Level 1 - LLMs can impact an if/then/else decision Level 2 - LLMs determine what functions should be called Level 3 - LLMs control iteration, and program order and continuation Level 4 - LLMs can start another agentic workflow Level 5 - LLMs can generate new code and run it</p> <ul> <li>See also: HuggingFace Agent Definition</li> </ul>"},{"location":"glossary/#ollama","title":"Ollama","text":"<p>A software framework that enables deployment and execution of large language models (LLMs) locally on personal computing devices, designed to optimize resource utilization and provide standardized interfaces for model management, inference, and integration with agent-based systems.</p>"},{"location":"glossary/#react-framework","title":"ReAct framework","text":"<p>The use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner.  This interleaving provides better results than separated workflows.</p> <p>Reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. </p> <ul> <li>ReAct: Synergizing Reasoning and Acting in Language Models - March 2023</li> </ul>"},{"location":"glossary/#rules-engine","title":"Rules Engine","text":"<p>A software system that manages and executes a set of business rules in a runtime production environment. </p> <p>Rules engines separate business logic from application code by allowing rules to be defined, stored, and managed independently. Rules engines evaluate conditions against input data and execute corresponding actions when conditions are met, enabling non-technical users to modify business logic without changing the underlying application code.</p> <p>Key components typically include:</p> <ol> <li>A rule repository for storing and managing business rules</li> <li>A rule authoring interface for creating and modifying rules</li> <li>A rule execution engine that evaluates conditions and triggers actions</li> <li>Working memory that holds the data being evaluated against the rules</li> </ol> <p>Rules engines are commonly used in complex decision-making systems, regulatory compliance, fraud detection, pricing strategies, and other scenarios where business logic frequently changes or requires transparency.</p>"},{"location":"glossary/#smolagents","title":"SmolAgents","text":"<p>An agent framework released by HuggingFace in December of 2024 with the goal of creating a small efficient way to build agents</p> <ul> <li>HuggingFace SmolAgents Docs</li> </ul>"},{"location":"glossary/#tool-calling","title":"Tool Calling","text":"<p>Letting an LLM call a function.</p> <p>Example: Let an LLM call a function to get similar text to a question from a knowledge base.</p>"},{"location":"how-we-built-this-site/","title":"How We Built This Site","text":"<p>This page describes how we built this website and some of  the rationale behind why we made various design choices.</p>"},{"location":"how-we-built-this-site/#python","title":"Python","text":"<p>MicroSims are about how we use generative AI to create animations and simulations.  The language of AI is Python.  So we wanted to create a site that could be easily understood by Python developers.</p>"},{"location":"how-we-built-this-site/#mkdocs-vs-docusaurus","title":"Mkdocs vs. Docusaurus","text":"<p>There are two main tools used by Python developers to write documentation: Mkdocs and Docusaurus.  Mkdocs is easier to use and more popular than Docusaurus. Docusaurus is also optimized for single-page applications. Mkdocs also has an extensive library of themes and plugins. None of us are experts in JavaScript or React. Based on our ChatGPT Analysis of the Tradeoffs we chose mkdocs for this site management.</p>"},{"location":"how-we-built-this-site/#github-and-github-pages","title":"GitHub and GitHub Pages","text":"<p>GitHub is a logical choice to store our  site source code and documentation.  GitHub also has a Custom GitHub Action that does auto-deployment if any files on the site change. We don't currently have this action enabled, but other teams can use this feature if they don't have the ability to do a local build with mkdocs.</p> <p>GitHub also has Issues,  Projects and releases that we can use to manage our bugs and tasks.</p> <p>The best practice for low-cost websites that have public-only content is GitHub Pages. Mkdocs has a command (<code>mkdocs gh-deploy</code>) that does deployment directly to GitHub Pages.  This was an easy choice to make.</p>"},{"location":"how-we-built-this-site/#github-clone","title":"GitHub Clone","text":"<p>If you would like to clone this repository, here are the commands:</p> <pre><code>mkdir projects\ncd projects\ngit clone https://github.com/dmccreary/microsims\n</code></pre>"},{"location":"how-we-built-this-site/#after-changes","title":"After Changes","text":"<p>After you make local changes you must do the following:</p> <pre><code># add the new files to a a local commit transaction\ngit add FILES\n# Execute the a local commit with a message about what and why you are doing the commit\ngit commit -m \"comment\"\n# Update the central GitHub repository\ngit push\n</code></pre>"},{"location":"how-we-built-this-site/#material-theme","title":"Material Theme","text":"<p>We had several options when picking a mkdocs theme:</p> <ol> <li>Mkdocs default</li> <li>Readthedocs</li> <li>Third-Party Themes See Ranking</li> </ol> <p>The Material Theme had 16K stars.  No other theme had over a few hundred. This was also an easy design decision.</p> <p>One key criterial was the social Open Graph tags so that when our users post a link to a simulation, the image of the simulation is included in the link.  Since Material supported this, we used the Material theme. You can see our ChatGPT Design Decision Analysis if you want to check our decision process.</p>"},{"location":"how-we-built-this-site/#enable-edit-icon","title":"Enable Edit Icon","text":"<p>To enable the Edit icon on all pages, you must add the edit_uri and the content.action.edit under the theme features area.</p> <pre><code>edit_uri: edit/master/docs/\n</code></pre> <pre><code>    theme:\n        features:\n            - content.action.edit\n</code></pre>"},{"location":"how-we-built-this-site/#conda-vs-venv","title":"Conda vs VENV","text":"<p>There are two choices for virtual environments.  We can use the native Python venv or use Conda.  venv is simle but is only designed for pure Python projects.  We imagine that this site could use JavaScript and other langauges in the future, so we picked Conda. There is nothing on this microsite that prevents you from using one or the other.  See the ChatGPT Analysis Here.</p> <p>Here is the conda script that we ran to create a new mkdocs environment that also supports the material social imaging libraries.</p> <pre><code>conda deactivate\nconda create -n mkdocs python=3\nconda activate mkdocs\npip install mkdocs \"mkdocs-material[imaging]\"\n</code></pre>"},{"location":"how-we-built-this-site/#mkdocs-commands","title":"Mkdocs Commands","text":"<p>There are three simple mkdoc commands we use.</p>"},{"location":"how-we-built-this-site/#local-build","title":"Local Build","text":"<pre><code>mkdocs build\n</code></pre> <p>This builds your website in a folder called <code>site</code>.  Use this to test that the mkdocs.yml site is working and does not have any errors.</p>"},{"location":"how-we-built-this-site/#run-a-local-server","title":"Run a Local Server","text":"<pre><code>mkdocs serve\n</code></pre> <p>This runs a server on <code>http://localhost:8000</code>. Use this to test the display formatting locally before you push your code up to the GitHub repo.</p> <pre><code>mkdoc gh-deploy\n</code></pre> <p>This pushes everything up to the GitHub Pages site. Note that it does not commit your code to GitHub.</p>"},{"location":"how-we-built-this-site/#mkdocs-material-social-tags","title":"Mkdocs Material Social Tags","text":"<p>We are using the Material Social tags.  This is a work in progress!</p> <p>Here is what we have learned.</p> <ol> <li>There are extensive image processing libraries that can't be installed with just pip.  You will need to run a tool like brew on the Mac to get the libraries installed.</li> <li>Even after <code>brew</code> installs the libraries, you have to get your environment to find the libraries.  The only way I could get that to work was to set up a local UNIX environment variable.</li> </ol> <p>Here is the brew command that I ran:</p> <pre><code>brew install cairo freetype libffi libjpeg libpng zlib\n</code></pre> <p>I then had to add the following to my ~/.zshrc file:</p> <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib\n</code></pre> <p>Note that I am running on a Mac with Apple silicon.  This means that the image libraries that brew downloads must be specific to the Mac Arm instruction set.</p>"},{"location":"how-we-built-this-site/#image-generation-and-compression","title":"Image Generation and Compression","text":"<p>I have used ChatGPT to create most of my images.  However, they are too large for most websites.  To compress them down I used  https://tinypng.com/ which is a free tool  for compressing png images without significant loss of quality.  The files created with ChatGPT are typically around 1-2 MB.  After  using the TinyPNG site the size is typically around 200-300KB.</p> <ul> <li>Cover images for blog post #4364</li> <li>Discussion on overriding the Social Card Image</li> </ul>"},{"location":"license/","title":"Creative Commons License","text":"<p>All content in this repository is governed by the following license agreement:</p>"},{"location":"license/#license-type","title":"License Type","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED)</p>"},{"location":"license/#link-to-license-agreement","title":"Link to License Agreement","text":"<p>https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en</p>"},{"location":"license/#your-rights","title":"Your Rights","text":"<p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format</li> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#restrictions","title":"Restrictions","text":"<ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> <li>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</li> </ul> <p>Notices</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p> <p>This deed highlights only some of the key features and terms of the actual license. It is not a license and has no legal value. You should carefully review all of the terms and conditions of the actual license before using the licensed material.</p>"},{"location":"references/","title":"Site References","text":"<ol> <li>mkdocs - https://www.mkdocs.org/ - this is our tool for building the website.  It converts Markdown into HTML in the <code>site</code> directory.</li> <li>mkdocs material theme - https://squidfunk.github.io/mkdocs-material/ - this is the theme for our site.  The theme adds the user interface elements that give our site the look and feel.  It also has the features such as social cards.</li> <li>GitHub Pages - https://pages.github.com/ - this is the free tool for hosting public websites created by mkdocs</li> <li>Markdown - https://www.mkdocs.org/user-guide/writing-your-docs/#writing-with-markdown - this is the format we use for text.  It allows us to have headers, lists, tables, links and images without learning HTML.</li> <li>Deploy Mkdocs GitHub Action - https://github.com/marketplace/actions/deploy-mkdocs - this is the tool we use to automatically build our site after edits are checked in with Git.</li> <li>Git Book - https://git-scm.com/book/en/v2 - a useful book on Git.  Just read the first two chapters to learn how to check in new code.</li> <li>Conda - https://conda.io/ - this is a command line tool that keeps our Python libraries organized for each project.</li> <li>VS Code - https://code.visualstudio.com/ - this is the integrated development environment we use to mange the files on our website.</li> <li>Markdown Paste - https://marketplace.visualstudio.com/items?itemName=telesoho.vscode-markdown-paste-image - this is the VS code extension we use to make sure we keep the markdown format generated by ChatGPT.</li> </ol>"},{"location":"concepts/","title":"Key Concepts in Agentic AI","text":"<p>Code Agents</p>"},{"location":"concepts/agents/","title":"Software Agents","text":"<p>A software agent is an autonomous computational entity that performs tasks on behalf of users or other programs with some degree of independence or autonomy, and in doing so, employs a precise knowledge representation of the user's goals or desires.</p>"},{"location":"concepts/agents/#agents-are-not-procedural-code","title":"Agents are Not Procedural Code","text":"<p>Here are some characteristics differentiating agents from standard procedural code:</p> <ol> <li>Autonomy: Agents operate without direct human intervention, making decisions and taking actions based on their internal state and perception of the environment. Unlike procedural code, which executes a predetermined sequence of instructions, and can follow precise rules, agents can decide when and how to act.</li> <li>Reactivity and Adaptability: Agents can perceive their environment and respond to changes in a timely fashion, adjusting their behavior accordingly. Procedural code follows a fixed execution path regardless of environmental changes unless explicitly programmed to handle specific scenarios.</li> <li>Goal-Orientation: Agents act in pursuit of goals rather than simply executing instructions. They can prioritize actions, resolve conflicts between competing objectives, and find alternative paths to achieve their goals when faced with obstacles. Procedural code lacks this inherent purpose-driven behavior and instead follows explicit instructions without considering overarching objectives.</li> </ol>"},{"location":"concepts/agents/#agents-and-rules-engines","title":"Agents and Rules Engines","text":""},{"location":"concepts/code-agents/","title":"What are Code Agents?","text":"<p>The CodeAgent in the Hugging Face SmolAgents library is a specialized type of AI agent designed to perform tasks by directly generating and executing Python code snippets. Unlike traditional agents that rely on structured formats like JSON or plain text to specify which tools to call and with what parameters, the CodeAgent takes a code-centric approach, allowing it to write Python code that executes actions immediately.</p>"},{"location":"concepts/code-agents/#key-features-of-the-codeagent","title":"Key Features of the CodeAgent","text":"<ul> <li>Direct Code Generation: The CodeAgent produces executable Python code to carry out its tasks, such as calling functions, performing computations, or interacting with external tools.</li> <li>Flexibility: By leveraging Python\u2019s programming constructs\u2014like loops, conditionals, and variables\u2014it can handle complex operations more naturally than JSON-based instructions.</li> <li>Reduced Overhead: Since the agent skips intermediate steps (e.g., parsing JSON to determine tool calls), it can execute actions more efficiently.</li> </ul>"},{"location":"concepts/code-agents/#how-it-differs-from-other-agents","title":"How It Differs from Other Agents","text":"<p>Traditional agents, including some supported by the SmolAgents library, typically operate as follows: - JSON or Text-Based Tool Calls: They generate structured outputs (e.g., JSON objects or text blobs) that describe which tools to invoke and with what arguments. For example:</p> <pre><code>  {\n    \"tool\": \"search\",\n    \"query\": \"HuggingFace SmolAgents\"\n  }\n</code></pre> <p>This JSON would then be parsed and mapped to a function call by the system. - Multi-Step Process: These agents require an additional layer of interpretation to translate the structured output into executable actions, which can introduce complexity and latency.</p> <p>In contrast, the CodeAgent:</p> <ul> <li>Writes Code Directly: Instead of producing JSON, it might generate something like:</li> </ul> <pre><code>result = search_tool(\"HuggingFace SmolAgents\")\n</code></pre> <p>This code is then executed in a controlled environment, eliminating the need for parsing or mapping.</p> <ul> <li>Streamlined Execution: By bypassing structured intermediaries, the CodeAgent reduces the number of steps between decision-making and action, potentially improving performance and enabling more sophisticated workflows.</li> </ul>"},{"location":"concepts/code-agents/#advantages-of-the-codeagent-approach","title":"Advantages of the CodeAgent Approach","text":"<ul> <li>Efficiency: Fewer processing steps mean faster execution, which can be a significant advantage in performance-sensitive applications.</li> <li>Expressiveness: Python\u2019s full range of programming features allows the agent to tackle intricate tasks that might be cumbersome to encode in JSON or text.</li> <li>Innovation: This code-based method is a distinctive feature of the SmolAgents library, setting it apart from other frameworks that lean heavily on traditional tool-calling paradigms.</li> </ul>"},{"location":"concepts/code-agents/#safety-considerations","title":"Safety Considerations","text":"<p>Executing generated code poses security risks, such as unintended system access or malicious operations. The SmolAgents library addresses this by providing:</p> <ul> <li>Controlled Imports: Limiting the modules the CodeAgent can access.</li> <li>Sandboxed Environments: Running the code in isolated contexts to prevent harmful effects.</li> </ul>"},{"location":"concepts/code-agents/#conclusion","title":"Conclusion","text":"<p>The CodeAgent stands out in the Hugging Face SmolAgents library as an innovative agent that uses Python code snippets to perform actions, contrasting with traditional agents that rely on JSON or text-based tool calls. This approach offers greater flexibility, efficiency, and power, making it a compelling choice for developers building AI-driven solutions within a secure and streamlined framework.</p>"},{"location":"concepts/code-agents/#references","title":"References","text":"<ul> <li>HuggingFace: Secure code execution in SmolAgents</li> <li>HuggingFace Paper</li> <li>Arxiv.org: Executable Code Actions Elicit Better LLM Agents This work proposes to use executable Python code to consolidate LLM agents\u2019 actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions</li> </ul>"},{"location":"concepts/knowledge-representation/","title":"Knowledge Representation","text":"<p>The methods and structures used to encode information about the world.</p> <p>In this book, we focus on knowledge representation in a form that an agent can utilize for reasoning and decision-making. It provides the foundation for an agent's understanding of its environment, goals, and possible actions.</p> <p>Knowledge representation encompasses:</p> <ol> <li> <p>Symbolic Structures: Formalized frameworks such as ontologies, semantic networks, frames, rules, or logic-based systems that capture entities, relationships, and constraints in the agent's domain.</p> </li> <li> <p>Information Organization: Methods for structuring knowledge in ways that facilitate efficient access, inference, and updates as the agent interacts with its environment.</p> </li> <li> <p>Reasoning Mechanisms: Techniques that allow agents to derive new knowledge from existing representations, evaluate truth values, and make decisions based on incomplete information.</p> </li> <li> <p>Semantic Grounding: Connections between abstract symbols and their real-world referents, allowing the agent to map internal representations to external phenomena.</p> </li> </ol> <p>Effective knowledge representation systems in agents balance expressiveness (the ability to represent complex knowledge) with computational efficiency (allowing the agent to reason within reasonable time and resource constraints).</p>"},{"location":"concepts/react-framework/","title":"Using the ReAct Framework for Textbook Generation","text":""},{"location":"concepts/react-framework/#overview","title":"Overview","text":"<p>This document demonstrates how to use the ReAct (Reasoning + Action) framework to generate a comprehensive AI textbook on \"Introduction to Large Language Models.\" The ReAct approach interleaves reasoning traces with concrete actions, allowing for a more dynamic and informed content creation process.</p>"},{"location":"concepts/react-framework/#implementation-example","title":"Implementation Example","text":"<pre><code>from smolagents import CodeAgent, FinalAnswerTool\nfrom smolagents.models import Model\nimport research_tools\nimport content_evaluation\nimport citation_manager\n\n# Initialize the model and tools\nmodel = Model(name=\"deepseek-r1:7b\")  # Using deepseek model as shown in documents\nagent = CodeAgent(\n    model=model,\n    tools=[\n        research_tools.SearchTool(),  # For finding relevant information\n        research_tools.SummarizeTool(),  # For condensing research\n        content_evaluation.AccuracyCheckerTool(),  # Verifies factual accuracy\n        content_evaluation.ReadabilityTool(),  # Ensures appropriate complexity level\n        citation_manager.CitationTool(),  # Manages references\n        FinalAnswerTool()  # Finalizes sections\n    ]\n)\n\ndef generate_textbook_chapter(topic, audience_level=\"undergraduate\"):\n    \"\"\"Generate a textbook chapter using the ReAct approach\"\"\"\n    prompt = f\"\"\"\n    Generate a textbook chapter on '{topic}' for {audience_level} students.\n    Follow these steps:\n    1. Research the topic thoroughly\n    2. Create an outline\n    3. Draft each section\n    4. Review for accuracy and readability\n    5. Add appropriate citations\n    6. Finalize the chapter\n\n    Use reasoning to plan your approach and actions to gather information and validate content.\n    \"\"\"\n\n    return agent.run(prompt)\n</code></pre>"},{"location":"concepts/react-framework/#react-process-breakdown","title":"ReAct Process Breakdown","text":"<p>Below is how the process unfolds for generating a chapter on \"Attention Mechanisms in Transformers\":</p>"},{"location":"concepts/react-framework/#1-initial-reasoning-trace","title":"1. Initial Reasoning Trace","text":"<pre><code>I need to create a comprehensive textbook chapter on attention mechanisms in Transformers. \nThis is a complex topic that requires clear explanations and examples.\nFirst, I should gather the latest research and fundamental concepts.\n</code></pre>"},{"location":"concepts/react-framework/#2-action-research","title":"2. Action: Research","text":"<pre><code>search_results = search_tool(\"attention mechanisms transformers technical explanation\")\nfoundational_papers = search_tool(\"Attention Is All You Need paper key concepts\")\nrecent_developments = search_tool(\"recent advances attention mechanisms 2023\")\n\n# Summarize the findings\ncore_concepts = summarize_tool(search_results + foundational_papers, \n                              focus=\"fundamental concepts for undergraduate level\")\n</code></pre>"},{"location":"concepts/react-framework/#3-reasoning-trace-for-outline","title":"3. Reasoning Trace for Outline","text":"<pre><code>Based on my research, I should structure this chapter to build knowledge progressively:\n- Start with the intuition behind attention\n- Move to mathematical formulation\n- Then cover different types of attention\n- Include practical examples\n- Discuss limitations and recent improvements\n</code></pre>"},{"location":"concepts/react-framework/#4-action-create-outline","title":"4. Action: Create Outline","text":"<pre><code>outline = {\n    \"title\": \"Attention Mechanisms in Transformer Models\",\n    \"sections\": [\n        {\n            \"title\": \"1. Introduction to Attention\",\n            \"subsections\": [\n                \"1.1 The Problem of Sequence Context\",\n                \"1.2 Intuition Behind Attention\"\n            ]\n        },\n        {\n            \"title\": \"2. Mathematics of Attention\",\n            \"subsections\": [\n                \"2.1 Queries, Keys, and Values\",\n                \"2.2 Scaled Dot-Product Attention\",\n                \"2.3 Multi-Head Attention\"\n            ]\n        },\n        # Additional sections...\n    ]\n}\n</code></pre>"},{"location":"concepts/react-framework/#5-reasoning-trace-for-content-creation","title":"5. Reasoning Trace for Content Creation","text":"<pre><code>For the introduction section, I need to make the concept accessible.\nI should use an analogy that undergraduate students can relate to.\nThe spotlight analogy works well for explaining how attention focuses on different parts of input.\n</code></pre>"},{"location":"concepts/react-framework/#6-action-draft-introduction","title":"6. Action: Draft Introduction","text":"<pre><code>introduction = \"\"\"\n# 1. Introduction to Attention\n\nImagine you're in a crowded room trying to follow a conversation. Despite many people talking simultaneously, you can focus on specific voices by directing your attention. Neural networks face a similar challenge when processing sequences like sentences - they need a mechanism to focus on relevant parts of the input. This is where attention mechanisms come in.\n\n## 1.1 The Problem of Sequence Context\n\nBefore attention mechanisms, sequence models like RNNs struggled with long-range dependencies...\n\"\"\"\n\n# Check readability\nreadability_score = readability_tool.analyze(introduction)\nif readability_score &lt; 0.7:  # Below target\n    introduction = readability_tool.improve(introduction, target_level=\"undergraduate\")\n</code></pre>"},{"location":"concepts/react-framework/#7-reasoning-trace-for-technical-content","title":"7. Reasoning Trace for Technical Content","text":"<pre><code>The mathematical section needs to balance rigor with accessibility.\nI should include the formulas but break them down step by step.\nVisual representations would help here - I'll include pseudocode and a diagram.\n</code></pre>"},{"location":"concepts/react-framework/#8-action-create-technical-section-with-verification","title":"8. Action: Create Technical Section with Verification","text":"<pre><code>math_section = \"\"\"\n# 2. Mathematics of Attention\n\n## 2.1 Queries, Keys, and Values\n\nThe attention mechanism revolves around three key concepts: queries, keys, and values...\n\"\"\"\n\n# Verify accuracy\naccuracy_issues = accuracy_checker_tool.verify(math_section, domain=\"machine learning\")\nif accuracy_issues:\n    for issue in accuracy_issues:\n        math_section = accuracy_checker_tool.suggest_correction(math_section, issue)\n</code></pre>"},{"location":"concepts/react-framework/#9-reasoning-trace-for-citations","title":"9. Reasoning Trace for Citations","text":"<pre><code>I need to properly cite the original Transformer paper and recent work.\nThe citations should follow academic standards and include DOI where available.\n</code></pre>"},{"location":"concepts/react-framework/#10-action-add-citations","title":"10. Action: Add Citations","text":"<pre><code>citations = citation_tool.generate([\n    {\"title\": \"Attention Is All You Need\", \"authors\": \"Vaswani et al.\", \"year\": 2017},\n    {\"title\": \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \n     \"authors\": \"Devlin et al.\", \"year\": 2019},\n    # Additional citations...\n])\n\nchapter_with_citations = citation_tool.insert_citations(full_chapter, citations)\n</code></pre>"},{"location":"concepts/react-framework/#11-final-reasoning-trace","title":"11. Final Reasoning Trace","text":"<pre><code>The chapter is nearly complete. I should review it once more for coherence and ensure\nthe progression of ideas flows naturally. The examples should reinforce the concepts.\n</code></pre>"},{"location":"concepts/react-framework/#12-action-finalize-content","title":"12. Action: Finalize Content","text":"<pre><code>final_chapter = final_answer_tool(\n    content=chapter_with_citations,\n    metadata={\n        \"title\": \"Attention Mechanisms in Transformer Models\",\n        \"audience\": \"undergraduate\",\n        \"prerequisites\": [\"Basic Neural Networks\", \"Sequence Models\"],\n        \"learning_objectives\": [\n            \"Understand the intuition behind attention mechanisms\",\n            \"Grasp the mathematical formulation of self-attention\",\n            \"Identify different types of attention and their applications\",\n            \"Implement a simple attention mechanism\"\n        ]\n    }\n)\n</code></pre>"},{"location":"concepts/react-framework/#benefits-of-the-react-approach-for-textbook-generation","title":"Benefits of the ReAct Approach for Textbook Generation","text":"<ol> <li> <p>Dynamic Research Integration: The ability to search for and incorporate the latest research ensures the textbook remains current and accurate.</p> </li> <li> <p>Quality Control: Interleaved reasoning and verification steps help catch errors and improve content quality throughout the creation process.</p> </li> <li> <p>Adaptive Content Creation: The framework can adjust explanations based on readability metrics, ensuring the content matches the intended audience level.</p> </li> <li> <p>Structured Development: The reasoning traces help maintain a coherent narrative and logical progression of ideas.</p> </li> <li> <p>Comprehensive Citations: Action steps for citation management ensure proper attribution and academic rigor.</p> </li> </ol>"},{"location":"concepts/react-framework/#implementation-considerations","title":"Implementation Considerations","text":"<ul> <li>Tool Selection: The specific tools integrated with the CodeAgent should match the subject matter of the textbook.</li> <li>Model Capabilities: More complex textbooks benefit from more capable foundation models like the deepseek-r1 model.</li> <li>Feedback Integration: The process can incorporate human expert feedback at key checkpoints.</li> <li>Multimodal Content: For subjects requiring diagrams or visualizations, additional tools for generating figures can be integrated.</li> </ul>"},{"location":"concepts/react-framework/#conclusion","title":"Conclusion","text":"<p>The ReAct framework provides a powerful approach to textbook generation by combining the reasoning capabilities of language models with the ability to take concrete actions like research, verification, and citation management. This interleaved process produces more comprehensive, accurate, and well-structured educational content than approaches that separate reasoning from action.</p>"},{"location":"concepts/workflows/","title":"Agentic Workflows","text":""},{"location":"concepts/workflows/#overview","title":"Overview","text":""},{"location":"concepts/workflows/#prompt-iteration","title":"Prompt Iteration","text":"<ul> <li>The LLM is trained on public knowledge but does not know anything about your goals, your products or your customers </li> <li>A human decides if the response is OK</li> <li>The human is \u201ctrained\u201d to write better prompts</li> </ul>"},{"location":"concepts/workflows/#prompt-enrichment-from-knowledge-base-rag","title":"Prompt Enrichment from Knowledge Base (RAG)","text":""},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#picking-the-right-agent-library","title":"Picking the Right Agent Library","text":"<p>Picking the Right Library</p>"},{"location":"getting-started/#installing-gpu-drivers","title":"Installing GPU Drivers","text":"<p>GPU Driver Version</p>"},{"location":"getting-started/#conda-setup","title":"Conda Setup","text":"<p>Conda Setup</p>"},{"location":"getting-started/#testing-python-libraries","title":"Testing Python Libraries","text":"<p>Testing Libraries</p>"},{"location":"getting-started/#ollama-setup","title":"Ollama Setup","text":"<p>Ollama Setup</p>"},{"location":"getting-started/conda-setup/","title":"Conda Setup","text":""},{"location":"getting-started/conda-setup/#install-conda","title":"Install Conda","text":""},{"location":"getting-started/conda-setup/#creating-a-conda-python-environment","title":"Creating a Conda Python Environment","text":"<pre><code>conda create -n \"agents\" python=3\nconda activate agents\n# note we are using cuda 12.6\npip install --upgrade torch torchvision torchaudio \\\n    --index-url https://download.pytorch.org/whl/cu126\npip install --upgrade transformers accelerate bitsandbytes\n</code></pre> <p>Before you activate the conda environment your prompt will have the string 'base' in it like this:</p> <pre><code>(base) $\n</code></pre> <pre><code>conda activate agents\n(agents) $\n</code></pre> <p>You can get a full list of all the Python libraries by using the <code>pip freeze</code> command.</p> <pre><code>pip freeze\n</code></pre>"},{"location":"getting-started/gpu-drivers/","title":"Installing GPU Drivers","text":"<p>We need to find out the exact version of CUDA we have on our local GPU before we install Python libraries.  We can find this out by running the <code>nvcc</code> command at the UNIX shell.</p> <pre><code>nvcc --version\n</code></pre> <p>Response</p> <pre><code>$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2024 NVIDIA Corporation\nBuilt on Fri_Jun_14_16:34:21_PDT_2024\nCuda compilation tools, release 12.6, V12.6.20\nBuild cuda_12.6.r12.6/compiler.34431801_0\n</code></pre> <p>Note that the CUDA release is 12.6.  We will use this later in our installation process.</p>"},{"location":"getting-started/install-smolagents/","title":"Intalling SmolAgents","text":""},{"location":"getting-started/install-smolagents/#why-smolagents","title":"Why SmolAgents","text":"<p>AI agent frameworks are often criticized with two points:</p> <ol> <li>They build too many layers of abstraction, making them rigid and challenging to learn and use.</li> <li>They focus on \"workflows\" rather than building agents that can dynamically collaborate on their own using Python data structures.</li> </ol> <p>Smolagents has qualities that make ideal for simple agentic applications.</p> <ul> <li>The framework's abstractions are kept at a minimum.</li> <li>While most frameworks have the agents define their actions in JSON/text format, smolagents' main approach is <code>Code Agents</code> in which actions are written as Python code snippets (this is different from agents that write code).</li> <li>Being a Hugging Face framework, smolagents integrates well with the Hub and the Transformers library. You can use many models from the hub (some of them you can only use as a Pro user), and you can also work with proprietary models from OpenAI, Anthropic, etc.</li> <li>You can easily utilize the already-provided tools, or define your custom tools with minimum effort, almost as simple as writing a Python function.</li> </ul> <p>These qualities are what, on paper, make smolagents a plug-and-play with AI agents with little effort, so let's see if they hold in practice.</p>"},{"location":"getting-started/install-smolagents/#installing-smolagents","title":"Installing SmolAgents","text":"<p>Make sure you are in the correct environment.  In this class we used conda to create an environment called \"agents\"</p> <pre><code>conda activate agents\n</code></pre> <p>Next, use <code>pip</code> to install the smolagents Python library</p> <pre><code>pip install smolagents\npip install 'smolagents[litellm]'\n</code></pre> <p>The second line also includes the litellm module.</p>"},{"location":"getting-started/install-smolagents/#testing-smolagents","title":"Testing SmolAgents","text":""},{"location":"getting-started/install-smolagents/#basic-test","title":"Basic Test","text":"<pre><code>from smolagents import SmoLAgent\nprint(\"SmoLAgents installed successfully!\")\n</code></pre>"},{"location":"getting-started/install-smolagents/#test-with-version-and-dir","title":"Test with Version and Dir","text":"<p>We can also use the Python <code>dir</code> function on the smolagents module to get a detailed list of the items in the smolagents Python library.</p> <pre><code>import smolagents\n\n# Print version and available modules\nprint(f\"SmoLAgents version: {smolagents.__version__}\")\nprint(f\"Available in smolagents: {dir(smolagents)}\")\n\n# Try to create a \"TestAgent\"\ntry:\n    from smolagents.agents import Agent\n    agent = Agent(name=\"TestAgent\")\n    print(f\"Agent created successfully with name: {agent.name}\")\nexcept ImportError as e:\n    print(f\"Could not import Agent: {e}\")\n\nprint(\"SmoLAgents is installed!\")\n</code></pre> <pre><code>SmoLAgents version: 1.9.2\nAvailable in smolagents: ['AUTHORIZED_TYPES', 'ActionStep', 'AgentAudio', 'AgentError', 'AgentExecutionError', 'AgentGenerationError', 'AgentImage', 'AgentLogger', 'AgentMaxStepsError', 'AgentMemory', 'AgentParsingError', 'AgentText', 'AgentType', 'Any', 'AzureOpenAIServerModel', 'BASE_BUILTIN_MODULES', 'Callable', 'ChatMessage', 'CodeAgent', 'Dict', 'DuckDuckGoSearchTool', 'E2BExecutor', 'EMPTY_PROMPT_TEMPLATES', 'FinalAnswerPromptTemplate', 'FinalAnswerTool', 'Generator', 'GoogleSearchTool', 'GradioUI', 'Group', 'HfApiModel', 'List', 'LiteLLMModel', 'LocalPythonInterpreter', 'LogLevel', 'MLXModel', 'ManagedAgentPromptTemplate', 'MessageRole', 'Model', 'Monitor', 'MultiStepAgent', 'OpenAIServerModel', 'Optional', 'Panel', 'Path', 'PlanningPromptTemplate', 'PlanningStep', 'PromptTemplates', 'PythonInterpreterTool', 'Rule', 'Set', 'SpeechToTextTool', 'StrictUndefined', 'SystemPromptStep', 'TOOL_MAPPING', 'TaskStep', 'Template', 'Text', 'Tool', 'ToolCall', 'ToolCallingAgent', 'ToolCollection', 'TransformersModel', 'Tuple', 'TypedDict', 'Union', 'UserInputTool', 'VisitWebpageTool', 'YELLOW_HEX', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_function_type_hints_utils', 'agent_types', 'agents', 'argparse', 'cli', 'create_repo', 'default_tools', 'deque', 'e2b_executor', 'evaluate_python_code', 'fix_final_answer_code', 'getLogger', 'get_clean_message_list', 'get_variable_names', 'gradio_ui', 'handle_agent_output_types', 'importlib', 'inspect', 'jinja2', 'json', 'launch_gradio_demo', 'leopard_prompt', 'load_dotenv', 'load_model', 'load_tool', 'local_python_executor', 'logger', 'main', 'make_init_file', 'memory', 'metadata_update', 'models', 'monitoring', 'os', 'parse_arguments', 'parse_code_blobs', 'parse_json_tool_call', 'populate_template', 're', 'snapshot_download', 'stream_to_gradio', 'tempfile', 'textwrap', 'time', 'tool', 'tool_role_conversions', 'tool_validation', 'tools', 'truncate_content', 'upload_folder', 'utils', 'yaml']\n\nCould not import Agent: cannot import name 'Agent' from 'smolagents.agents' (/home/dan/miniconda3/envs/agents/lib/python3.13/site-packages/smolagents/agents.py)\n\nSmoLAgents is installed!\n</code></pre>"},{"location":"getting-started/install-smolagents/#creating-a-model","title":"Creating a Model","text":"<p>We can get a list of the agent types by looking for all method names with the string \"Agent\" it it.</p> <pre><code># test-smolagents-final.py\nimport smolagents\nfrom smolagents import FinalAnswerTool  # Import directly from smolagents\nfrom smolagents.models import Model\n\nprint(f\"SmoLAgents version: {smolagents.__version__}\")\n\ntry:\n    # Create a dummy model\n    dummy_model = Model(name=\"dummy-model\")\n\n    # Create a basic tool\n    final_answer_tool = FinalAnswerTool()\n\n    print(\"Successfully created tool and model!\")\n\n    # Print available agent types\n    print(\"Available agent types:\")\n    print([name for name in dir(smolagents) if 'Agent' in name])\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\nprint(\"SmoLAgents installation test complete!\")\n</code></pre> <pre><code>SmoLAgents version: 1.9.2\nSuccessfully created tool and model!\n\nAvailable agent types:\n['AgentAudio', 'AgentError', 'AgentExecutionError', 'AgentGenerationError', 'AgentImage', 'AgentLogger', 'AgentMaxStepsError', 'AgentMemory', 'AgentParsingError', 'AgentText', 'AgentType', 'CodeAgent', 'ManagedAgentPromptTemplate', 'MultiStepAgent', 'ToolCallingAgent']\n\nSmoLAgents installation test complete!\n</code></pre>"},{"location":"getting-started/install-smolagents/#references","title":"References","text":"<p>Build Fast with AI Video</p>"},{"location":"getting-started/ollama-python/","title":"Ollama python","text":""},{"location":"getting-started/ollama-python/#calling-deepseek-r1-via-ollama","title":"Calling DeepSeek R1 via Ollama","text":"<pre><code>#!/usr/bin/env python3\n# working-code-agent.py\nimport smolagents\nfrom smolagents import FinalAnswerTool, CodeAgent\nimport requests\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass OllamaModel:\n    \"\"\"Simple Ollama model that ignores unsupported parameters\"\"\"\n\n    def __init__(self, model_name, base_url=\"http://localhost:11434\"):\n        self.model_name = model_name\n        self.base_url = base_url\n        self.generate_endpoint = f\"{base_url}/api/generate\"\n        logger.info(f\"Initialized OllamaModel with model: {model_name}\")\n\n    def __call__(self, prompt, **kwargs):\n        \"\"\"Generate response from Ollama model, ignore any kwargs\"\"\"\n        # Handle various prompt formats\n        if isinstance(prompt, list):\n            # Convert list to text\n            if len(prompt) &gt; 0 and isinstance(prompt[0], dict):\n                # Extract from messages format\n                full_prompt = \"\\n\".join([msg.get(\"content\", \"\") for msg in prompt if isinstance(msg, dict) and \"content\" in msg])\n            else:\n                # Join list items\n                full_prompt = \"\\n\".join([str(item) for item in prompt if item])\n        else:\n            # Use as is\n            full_prompt = str(prompt)\n\n        payload = {\n            \"model\": self.model_name,\n            \"prompt\": full_prompt,\n            \"stream\": False\n        }\n\n        try:\n            logger.info(f\"Sending request to Ollama with prompt length: {len(full_prompt)} chars\")\n            response = requests.post(self.generate_endpoint, json=payload)\n\n            if response.status_code == 200:\n                result = response.json()\n                return result.get(\"response\", \"\")\n            else:\n                logger.error(f\"Ollama request failed: {response.status_code}\")\n                return \"Error: Failed to get response from Ollama\"\n        except Exception as e:\n            logger.error(f\"Error: {e}\")\n            return f\"Error: {str(e)}\"\n\ndef main():\n    try:\n        # Create the model\n        model = OllamaModel(\"deepseek-r1\")\n\n        # Test direct model use\n        response = model(\"What is 2 + 2?\")\n        logger.info(f\"Direct model response: {response[:100]}...\")\n\n        # Create a simple task for the agent\n        task = \"Calculate the sum of numbers from 1 to 5\"\n\n        # Create the agent with minimal setup - REMOVED verbose parameter\n        agent = CodeAgent(\n            model=model,\n            tools=[FinalAnswerTool()]\n        )\n\n        # Run the agent\n        logger.info(f\"Running agent with task: {task}\")\n        result = agent.run(task)\n\n        logger.info(f\"Agent result: {result}\")\n\n        # If that works, try a more complex task\n        complex_task = \"\"\"\n        I need to solve this problem:\n        1. Calculate the area of a rectangle with width 6 meters and length 9 meters\n        2. Then calculate what percentage of that area would be covered by a square with side length 4 meters\n        \"\"\"\n\n        logger.info(f\"\\nRunning agent with complex task...\")\n        complex_result = agent.run(complex_task)\n\n        logger.info(f\"Complex task result: {complex_result}\")\n\n    except Exception as e:\n        logger.error(f\"Error: {e}\")\n        import traceback\n        logger.error(traceback.format_exc())\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"getting-started/ollama-setup/","title":"Ollama Deepseek Setup","text":"<p>This is an example of installing Ollama and the DeepSeek R1 7b model on my local system that has a NVIDIA RTX 2070 GPU with 12GB RAM.</p>"},{"location":"getting-started/ollama-setup/#install-ollama","title":"Install Ollama","text":"<pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre>"},{"location":"getting-started/ollama-setup/#test-ollama","title":"Test Ollama","text":"<pre><code>ollama --version\n</code></pre> <p>Response:</p> <pre><code>ollama version is 0.3.6\n</code></pre>"},{"location":"getting-started/ollama-setup/#setup-ollama-as-a-unix-service","title":"Setup Ollama as a UNIX service","text":"<pre><code>sudo systemctl edit ollama.service\n</code></pre> <p>Add the following lines:</p> <pre><code>[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0\"\nEnvironment=\"OLLAMA_ORIGINS=*\"\n</code></pre>"},{"location":"getting-started/ollama-setup/#test-the-ollama-rest-service","title":"Test the Ollama REST Service","text":"<pre><code>curl http://localhost:11434/api/version\n</code></pre> <p>Result:</p> <pre><code>{\"version\":\"0.3.6\"}\n</code></pre>"},{"location":"getting-started/ollama-setup/#ollama-shell-commands","title":"Ollama Shell Commands","text":"<pre><code>ollama --help\n</code></pre> <pre><code>Large language model runner\n\nUsage:\n  ollama [flags]\n  ollama [command]\n\nAvailable Commands:\n  serve       Start ollama\n  create      Create a model from a Modelfile\n  show        Show information for a model\n  run         Run a model\n  pull        Pull a model from a registry\n  push        Push a model to a registry\n  list        List models\n  ps          List running models\n  cp          Copy a model\n  rm          Remove a model\n  help        Help about any command\n\nFlags:\n  -h, --help      help for ollama\n  -v, --version   Show version information\n\nUse \"ollama [command] --help\" for more information about a command.\n</code></pre>"},{"location":"getting-started/ollama-setup/#download-deepseek-r1","title":"Download DeepSeek-r1","text":"<p>You can download the full 7b model by adding the keyword <code>pull</code> and the model name to ollama.</p> <pre><code>ollama pull deepseek-r1:7b\n</code></pre> <p></p>"},{"location":"getting-started/ollama-setup/#test-that-deepseek-r1-7b-is-working","title":"Test that Deepseek-r1 7B is working","text":"<pre><code>ollama run deepseek-r1:7b\n&gt;&gt;&gt; /show info\n  Model                          \n    arch                qwen2            \n    parameters          7.6B             \n    quantization        Q4_K_M           \n    context length      131072           \n    embedding length    3584             \n\n  Parameters                     \n    stop    \"&lt;\uff5cbegin\u2581of\u2581sentence\uff5c&gt;\"   \n    stop    \"&lt;\uff5cend\u2581of\u2581sentence\uff5c&gt;\"     \n    stop    \"&lt;\uff5cUser\uff5c&gt;\"                \n    stop    \"&lt;\uff5cAssistant\uff5c&gt;\"           \n\n  License                        \n    MIT License                     \n    Copyright (c) 2023 DeepSeek     \n\n&gt;&gt;&gt; what are the 10 most difficult concepts in high-school geometry to understand?\n</code></pre>"},{"location":"getting-started/ollama-setup/#deepseek-response","title":"DeepSeek Response","text":""},{"location":"getting-started/ollama-setup/#thinking","title":"Thinking","text":"<p> Okay, so I need to figure out what the top 10 most difficult concepts in high school geometry are. Hmm, where do I start? I've taken geometry before, but it's been a while, so some of these might have faded from my  memory. Let me think step by step. <p>First off, when I was learning geometry, one topic that always confused me was proofs. There were so many theorems and properties to remember, and constructing a logical argument seemed like a whole other language. I can't even recall exactly what an auxiliary line is, but I know it's used in proofs somehow.</p> <p>Then there are circles. I remember they have circumference, area, radius, diameter, and all those formulas, but when you start talking about angles subtended by chords or tangents, things got really tangled for me. It was hard to keep track of the different theorems related to circles, like the inscribed angle theorem or  something.</p> <p>Triangles are another big topic in geometry. There are so many types\u2014scalene, isosceles, equilateral\u2014and each has its own properties. Then there's Pythagoras' theorem for right-angled triangles, which I sometimes mixed  up with other formulas because they all looked similar. Also, triangle congruence and similarity were confusing at times; remembering SAS and ASA rules was a struggle.</p> <p>Coordinate geometry also tripped me up. It combines algebra and geometry, so you have to visualize shapes on the coordinate plane and use equations to find distances, slopes, and intersections. I remember struggling with finding the distance between two points using the distance formula and calculating midpoints or perpendicular lines.</p> <p>Transformations like translations, rotations, reflections, and dilations were another area where I had trouble. Understanding how figures change position or size on the coordinate plane was confusing without a clear visual. Also, similarity transformations confused me because they involved scaling, which adds another  layer of complexity.</p> <p>Trigonometry in geometry felt separate from what I had learned before. Learning sine, cosine, and tangent functions to find side lengths and angles in right triangles was initially tough. Then, when we moved on to  the unit circle, everything shifted, and it became even more abstract. I had trouble understanding how trigonometric ratios extended beyond right triangles.</p> <p>Three-dimensional geometry confused me a lot. Moving from 2D shapes to 3D solids like prisms, pyramids, cylinders, cones, and spheres introduced new concepts and formulas. Calculating surface area and volume required knowing which formula to apply when, and I often mixed them up or used the wrong dimensions in  calculations.</p> <p>Parallelograms had their own set of properties that needed to be memorized for proofs. Proving whether a quadrilateral was a parallelogram based on given information was tricky because there were multiple ways to  show it, like using opposite sides equal or parallel lines and angles.</p> <p>Probability in geometry was another point where I struggled. It wasn't just basic probability; integrating geometric principles made the problems more complex. Calculating probabilities based on areas or lengths required a good understanding of both geometry and probability concepts.</p> <p>Lastly, constructions with a compass and straightedge were a nightmare. It involved precise steps without any measurements, which felt restrictive. Forgetting a step in a construction could lead to an incorrect figure, and figuring out the right sequence without trial and error was frustrating.</p> <p>Wait, did I miss anything? Let me think... Maybe locus problems where you have to find points satisfying certain conditions. Or maybe transformations again; they were mentioned before but perhaps not explicitly listed as top 10. Hmm, since it's about difficulty, maybe those are already covered.</p> <p>I should also consider if the order makes sense. Proofs seem foundational but can be really challenging because they require logical reasoning. Then coordinate geometry and constructions build on previous knowledge with algebra and precise drawing skills. Trigonometry and circles add layers of complexity with their formulas and applications. Three-dimensional geometry might come last as it's more abstract.</p> <p>Yeah, I think that covers the main areas where students usually struggle in high school geometry. Each topic has its own challenges, but some require a strong foundation in other areas like algebra or logical reasoning. </p>"},{"location":"getting-started/ollama-setup/#answer","title":"Answer","text":"<p>The top 10 most difficult concepts in high school geometry can be categorized based on their complexity and the skills they require:</p> <ol> <li> <p>Geometric Proofs: Constructing logical arguments using axioms and previously proven theorems is challenging due to its abstract nature.</p> </li> <li> <p>Circle Theorems: Understanding properties of circles, such as angles subtended by chords, tangents, and arcs requires memorization and application of multiple theorems.</p> </li> <li> <p>Triangle Properties and Congruence/Similarity: Distinguishing between different types of triangles and applying congruence/similarity rules (e.g., SAS, ASA) can be confusing.</p> </li> <li> <p>Coordinate Geometry: Combining algebra with geometry to find distances, slopes, and intersections on the coordinate plane is complex.</p> </li> <li> <p>Transformations: Grasping concepts like translations, rotations, reflections, and dilations, especially their effects on figures in the coordinate plane, is challenging.</p> </li> <li> <p>Trigonometry: Extending trigonometric ratios beyond right triangles using the unit circle abstracts the subject and complicates problem-solving.</p> </li> <li> <p>Three-Dimensional Geometry: Calculating surface areas and volumes of 3D solids like prisms and pyramids requires applying specific formulas with attention to detail.</p> </li> <li> <p>Parallelogram Properties: Proving properties of parallelograms based on given information involves multiple proof strategies.</p> </li> <li> <p>Geometric Probability: Integrating probability concepts with geometric principles, such as area-based calculations, adds complexity.</p> </li> <li> <p>Compass and Straightedge Constructions: Performing precise constructions without measurement tools demands careful step-by-step execution.</p> </li> </ol> <p>Each concept builds upon previous knowledge, requiring a blend of logical reasoning, algebraic skills, and spatial understanding. Mastery often necessitates practice and a solid grasp of foundational topics.</p>"},{"location":"getting-started/ollama-setup/#other-ollama-commands","title":"Other Ollama Commands","text":"<p>/help Available Commands:   /set            Set session variables   /show           Show model information   /load    Load a session or model   /save    Save your current session   /clear          Clear session context   /bye            Exit   /?, /help       Help for a command   /? shortcuts    Help for keyboard shortcuts <p>Use \"\"\" to begin a multi-line message.</p>"},{"location":"getting-started/picking-agent-library/","title":"Selecting Agent Libraries","text":"<p>As of February of 2025, there are three AI Agent Libraries that are popular: </p> <ol> <li>LangChain - the first popular agent library</li> <li>LlamaIndex - a more recent library that fixed many problems with LangChain</li> <li>SmoLAgents - a library created by HuggingFace to work with small models on your local GPU</li> </ol> <p>Let's compare these three AI agent libraries based on their features, strengths, and limitations:</p>"},{"location":"getting-started/picking-agent-library/#langchain","title":"LangChain","text":""},{"location":"getting-started/picking-agent-library/#pros","title":"Pros","text":"<ul> <li>Comprehensive framework for building LLM-powered applications</li> <li>Strong focus on chains and composability of components</li> <li>Extensive tool and service integrations (databases, APIs, etc.)</li> <li>Robust memory and state management capabilities</li> <li>Large community with abundant documentation and examples</li> <li>Flexible architecture allowing for customization</li> <li>Well-established ecosystem with many extensions</li> </ul>"},{"location":"getting-started/picking-agent-library/#cons","title":"Cons","text":"<ul> <li>Can be complex for beginners due to its extensive feature set</li> <li>Sometimes considered overly modular, leading to complexity</li> <li>Rapid development pace can lead to breaking changes</li> <li>Higher computational requirements for some components</li> <li>Can be overkill for simpler applications</li> </ul>"},{"location":"getting-started/picking-agent-library/#llamaindex","title":"LlamaIndex","text":""},{"location":"getting-started/picking-agent-library/#pros_1","title":"Pros","text":"<ul> <li>Specialized in data indexing and retrieval for LLMs</li> <li>Excellent for RAG (Retrieval-Augmented Generation) applications</li> <li>Strong document processing capabilities</li> <li>Efficient handling of various data sources and formats</li> <li>Good performance with large document collections</li> <li>Focused architecture making it easier to learn for specific use cases</li> <li>Growing integration ecosystem</li> </ul>"},{"location":"getting-started/picking-agent-library/#cons_1","title":"Cons","text":"<ul> <li>More focused on data retrieval than end-to-end agent capabilities</li> <li>Less comprehensive agent framework compared to LangChain</li> <li>May require additional libraries for complete agent implementations</li> <li>Fewer built-in tools for complex reasoning chains</li> <li>Still evolving, with some features in experimental stages</li> </ul>"},{"location":"getting-started/picking-agent-library/#smolagents","title":"SmoLAgents","text":""},{"location":"getting-started/picking-agent-library/#pros_2","title":"Pros","text":"<ul> <li>Focuses on small language model agents, making it more efficient for certain tasks</li> <li>Designed for resource-constrained environments such as a local GPU</li> <li>Lower computational requirements compared to frameworks requiring larger models</li> <li>Good for edge devices or applications with limited processing power</li> <li>Emphasizes agent simplicity and minimalism</li> </ul>"},{"location":"getting-started/picking-agent-library/#cons_2","title":"Cons","text":"<ul> <li>Less comprehensive feature set than more established frameworks</li> <li>Smaller community and ecosystem</li> <li>More limited documentation and tutorials</li> <li>Fewer integrations with external tools and services</li> <li>May not handle extremely complex reasoning tasks as effectively as libraries using larger models</li> </ul>"},{"location":"getting-started/picking-agent-library/#summary","title":"Summary","text":"<ul> <li>SmoLAgents is best for lightweight applications with limited resources where efficiency is critical.</li> <li>LangChain excels as a comprehensive framework for complex, feature-rich AI applications with extensive integration needs.</li> <li>LlamaIndex shines for data-intensive applications where efficient document retrieval and processing are the primary concerns.</li> </ul> <p>Your choice should depend on your specific use case, resource constraints, and whether you need a complete agent framework or specialized functionality for particular aspects of AI application development.</p>"},{"location":"getting-started/smolagents-ollama/","title":"Testing SmolAgents with Ollama","text":"<pre><code># minimal-smolagents-ollama.py\nimport smolagents\nfrom smolagents import FinalAnswerTool\nimport requests\nimport logging\nimport time\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass SimplerOllamaModel:\n    \"\"\"Simplified Ollama model adapter for smolagents\"\"\"\n\n    def __init__(self, model_name, base_url=\"http://localhost:11434\"):\n        self.model_name = model_name\n        self.base_url = base_url\n        self.generate_endpoint = f\"{base_url}/api/generate\"\n        logger.info(f\"Initialized SimplerOllamaModel with model: {model_name}\")\n\n    def __call__(self, prompt, **kwargs):\n        \"\"\"Handle both direct prompts and formatted messages\"\"\"\n        # Process the prompt to handle different formats\n        processed_prompt = self._process_prompt(prompt)\n\n        # Prepare the request payload\n        payload = {\n            \"model\": self.model_name,\n            \"prompt\": processed_prompt,\n            \"stream\": False\n        }\n\n        # Send request to Ollama\n        try:\n            logger.info(f\"Sending request to Ollama, prompt length: {len(processed_prompt)} chars\")\n            response = requests.post(self.generate_endpoint, json=payload)\n\n            if response.status_code == 200:\n                result = response.json()\n                return result.get(\"response\", \"\")\n            else:\n                logger.error(f\"Ollama request failed: {response.status_code}\")\n                return f\"Error: Ollama request failed with status {response.status_code}\"\n        except Exception as e:\n            logger.error(f\"Error: {e}\")\n            return f\"Error: {str(e)}\"\n\n    def _process_prompt(self, prompt):\n        \"\"\"Convert various prompt formats to a string\"\"\"\n        if prompt is None:\n            return \"\"\n\n        if isinstance(prompt, str):\n            return prompt\n\n        if isinstance(prompt, list):\n            # If it's a list of messages (dicts with role and content)\n            if len(prompt) &gt; 0 and isinstance(prompt[0], dict):\n                result = \"\"\n                for msg in prompt:\n                    if isinstance(msg, dict) and \"content\" in msg:\n                        role = msg.get(\"role\", \"\")\n                        content = msg.get(\"content\", \"\")\n                        if role and content:\n                            result += f\"{role.upper()}: {content}\\n\\n\"\n                        else:\n                            result += f\"{content}\\n\\n\"\n                return result.strip()\n\n            # If it's a list of strings\n            return \"\\n\".join([str(item) for item in prompt if item])\n\n        # Default case - convert to string\n        return str(prompt)\n\ndef use_direct_ollama():\n    \"\"\"Use Ollama directly without smolagents\"\"\"\n    try:\n        # Create model\n        model = SimplerOllamaModel(\"deepseek-r1\")\n\n        # Test with simple prompt\n        result = model(\"What is 2 + 2? Explain step by step.\")\n        logger.info(f\"Direct Ollama result: {result[:200]}...\")\n\n        # Test with ChatML format\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"What is the sum of numbers from 1 to 5?\"}\n        ]\n\n        result2 = model(messages)\n        logger.info(f\"Messages format result: {result2[:200]}...\")\n\n        return True\n    except Exception as e:\n        logger.error(f\"Direct Ollama test failed: {e}\")\n        return False\n\ndef create_custom_agent():\n    \"\"\"Create a custom agent that uses Ollama without relying on smolagents classes\"\"\"\n    try:\n        # Create our Ollama model\n        model = SimplerOllamaModel(\"deepseek-r1\")\n\n        # Define a simple agent function\n        def simple_agent(prompt):\n            # Format the prompt to encourage step-by-step thinking\n            enhanced_prompt = f\"\"\"\nPlease solve this problem step by step:\n\n{prompt}\n\nThink through this problem carefully. First, understand what is being asked.\nThen break it down into steps and solve each step.\nFinally, provide your answer.\n\"\"\"\n            # Get response from model\n            response = model(enhanced_prompt)\n\n            # Return the result\n            return response\n\n        # Test the simple agent\n        task = \"Calculate the sum of numbers from 1 to 5.\"\n        logger.info(f\"Running simple agent with task: {task}\")\n\n        result = simple_agent(task)\n        logger.info(f\"Simple agent result: {result[:200]}...\")\n\n        # Test with complex task\n        complex_task = \"\"\"\nI need to solve this problem:\n1. Calculate the area of a rectangle with width 6 meters and length 9 meters\n2. Then calculate what percentage of that area would be covered by a square with side length 4 meters\n\"\"\"\n        logger.info(f\"Running simple agent with complex task\")\n        complex_result = simple_agent(complex_task)\n        logger.info(f\"Complex task result: {complex_result[:200]}...\")\n\n        return True\n    except Exception as e:\n        logger.error(f\"Custom agent test failed: {e}\")\n        return False\n\ndef main():\n    logger.info(\"Starting Ollama integration test...\")\n\n    # First, test direct Ollama usage\n    if use_direct_ollama():\n        logger.info(\"Direct Ollama test successful!\")\n\n        # Then, test our custom agent\n        if create_custom_agent():\n            logger.info(\"Custom agent test successful!\")\n        else:\n            logger.warning(\"Custom agent test failed.\")\n    else:\n        logger.error(\"Direct Ollama test failed. Cannot proceed.\")\n\n    logger.info(\"All tests completed.\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"getting-started/testing-libraries/","title":"Testing Supporting Libraries","text":""},{"location":"getting-started/testing-libraries/#python-library-version-test","title":"Python Library Version Test","text":"<pre><code>import torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\n</code></pre>"},{"location":"getting-started/testing-libraries/#sample-response","title":"Sample Response","text":"<pre><code>PyTorch version: 2.6.0+cu126\nCUDA available: True\nCUDA version: 12.6\n</code></pre>"}]}