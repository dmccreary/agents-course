{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Building Intelligent Agents with AI","text":"<p>Welcome to our course on building intelligent agents with AI.  This course is unique in that it teaches you how to use generative AI to create intelligent agents that run on your local computer.</p> <p>This website is based on our intelligent textbook</p> <p>Please contact me on LinkedIn if you have any questions about this textbook.</p> <p>Thanks! - Dan</p>"},{"location":"about/","title":"About this Course","text":"<p>This course is designed for individuals that would like to learn the concepts involved in using and building intelligent software agents.</p>"},{"location":"about/#prerequisites","title":"Prerequisites","text":"<ul> <li>Be curious about what software agents are and how organizations are using them</li> <li>Have some knowledge of using generative AI tools such as ChatGPT, Anthropic Clause or similar tools.</li> <li>Have some experience with creating precise prompts and correcting prompts to get the desired outcome</li> <li>Be somewhat familiar with basic Python programming including the steps to setup a Python</li> <li>Have access to the terminal or shell on your local computer or a remote host</li> </ul>"},{"location":"about/#related-course-for-beginners","title":"Related Course for Beginners","text":"<p>If you don't have this background we recommend our Prompt Engineering course.</p> <p>If you are new to Python, we have a fun interactive Introduction to Python course.</p> <p>If you want to take a deep dive into deep learning is our Deep Learning Course.</p>"},{"location":"contact/","title":"Contact","text":"<p>Please contact me on LinkedIn</p> <p>Thanks! - Dan</p>"},{"location":"course-description/","title":"Building Intelligent Software Agents with LLMs and Knowledge Graphs - Course Description","text":""},{"location":"course-description/#course-description","title":"Course Description","text":"<p>This 12-week course introduces students to the design, development, and deployment of intelligent software agents powered by Large Language Models (LLMs) and knowledge representation systems. Students will learn to build autonomous computational entities that perform tasks with independence while effectively representing user goals and domain knowledge.</p> <p>Through hands-on projects and practical exercises, students will explore the spectrum of agent capabilities\u2014from simple rule-based systems to sophisticated code-generating agents that can write and execute Python. The course emphasizes both theoretical foundations and practical implementation, with special focus on the emerging SmolAgents framework, local LLM deployment with Ollama, and effective knowledge representation techniques.</p>"},{"location":"course-description/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this course, students will be able to:</p> <ul> <li>Implement various levels of LLM agent control (from basic decision-making to code generation)</li> <li>Deploy and run local LLMs using Ollama and similar frameworks</li> <li>Design effective tool catalogs with well-documented functions for agent use</li> <li>Create agents that combine public knowledge (from LLMs) with private knowledge sources</li> <li>Implement the ReAct framework to interleave reasoning and action in agent systems</li> <li>Develop autonomous agents that can understand, generate, and execute Python code</li> <li>Evaluate agent performance through structured peer review processes</li> </ul>"},{"location":"course-description/#prerequisites","title":"Prerequisites","text":"<ul> <li>Intermediate Python programming skills</li> <li>Basic understanding of data types and structures</li> <li>Familiarity with GitHub or similar version control systems (helpful but not required)</li> <li>Experience with IDEs such as Visual Studio (helpful but not required)</li> <li>Basic understanding of command-line interfaces (helpful but not required)</li> </ul>"},{"location":"course-description/#course-structure","title":"Course Structure","text":""},{"location":"course-description/#weeks-1-2-foundations-of-agent-systems","title":"Weeks 1-2: Foundations of Agent Systems","text":"<ul> <li>Introduction to software agents and agency levels</li> <li>Knowledge representation fundamentals</li> <li>Tools, tool calling, and tool catalogs</li> <li>Setting up local development environments with Ollama</li> </ul>"},{"location":"course-description/#weeks-3-4-llm-integration-and-local-deployment","title":"Weeks 3-4: LLM Integration and Local Deployment","text":"<ul> <li>Working with DeepSeek R1 and other lightweight LLMs</li> <li>Prompt engineering for agent development</li> <li>Building basic tool-calling agents</li> <li>GPU optimization for local deployment</li> </ul>"},{"location":"course-description/#weeks-5-6-agent-frameworks","title":"Weeks 5-6: Agent Frameworks","text":"<ul> <li>Introduction to SmolAgents library</li> <li>Comparative analysis of agent frameworks (LangChain, PydanticAI)</li> <li>The ReAct framework implementation</li> <li>Creating custom tools with proper documentation</li> </ul>"},{"location":"course-description/#weeks-7-8-advanced-agent-capabilities","title":"Weeks 7-8: Advanced Agent Capabilities","text":"<ul> <li>Code Agents: direct code generation and execution</li> <li>Combining public and private knowledge sources</li> <li>Building multi-agent systems</li> <li>Error handling and safety considerations</li> </ul>"},{"location":"course-description/#weeks-9-12-capstone-project","title":"Weeks 9-12: Capstone Project","text":"<ul> <li>Team formation and project planning</li> <li>Milestone reviews and peer feedback sessions</li> <li>Project implementation and iteration</li> <li>Final presentations and demonstrations</li> </ul>"},{"location":"course-description/#assessment-methods","title":"Assessment Methods","text":"<ul> <li>Hands-on programming assignments (30%)</li> <li>Midterm project: Basic agent implementation (15%)</li> <li>Peer review participation and quality (20%)</li> <li>Capstone project development and presentation (35%)</li> </ul>"},{"location":"course-description/#required-tools","title":"Required Tools","text":"<ul> <li>GitHub account (required)</li> <li>Python 3.8+ with relevant libraries</li> <li>GPU-compatible system for local LLM deployment (recommended)</li> <li>Development environment (VS Code or similar IDE recommended)</li> </ul>"},{"location":"course-description/#special-course-features","title":"Special Course Features","text":"<ul> <li>Weekly agent showcase sessions where students demonstrate their latest implementations</li> <li>Industry guest lectures from professionals working with agent technologies</li> <li>Collaborative problem-solving sessions using the intelligent textbook approach</li> <li>Optional advanced tracks for students interested in specialized applications (educational agents, coding assistants, etc.)</li> </ul>"},{"location":"course-description/#note-to-prospective-students","title":"Note to Prospective Students","text":"<p>This course operates at the intersection of artificial intelligence, software engineering, and knowledge representation. While we provide substantial guidance, students should be prepared for a highly experimental learning environment, as agent technologies are rapidly evolving. Creative problem-solving and adaptation to emerging frameworks will be essential for success in this course.</p>"},{"location":"course-description/#questions","title":"Questions","text":""},{"location":"course-description/#do-i-have-to-have-a-computer-with-a-gpu","title":"Do I have to have a computer with a GPU?","text":"<p>No, but it is strongly recommended.</p>"},{"location":"course-description/#what-is-a-good-enough-local-gpu","title":"What is a good enough local GPU?","text":"<p>The ideal system is one that has a minimum of 12GB of RAM such as a NVIDIA GTX series of Consumer/Gaming GPUs.  Here are some examples:</p> <ul> <li>GeForce RTX 4090 (24 GB)</li> <li>GeForce RTX 4080 (16 GB)</li> <li>GeForce RTX 3090 Ti (24 GB)</li> <li>GeForce RTX 3090 (24 GB)</li> <li>GeForce RTX 3080 Ti (12 GB)</li> <li>GeForce RTX 3080 (12 GB version)</li> </ul> <p>These systems all can generate over 50 tokens per second when running Ollama and the DeepSeek-R1 LLM.  Using a local LLM means you don't need to pay high fees to cloud LLM service providers.</p> <p>Note that the four year old GeForce RTX 3080 sells for approximately \\(350-\\)550 on marketplaces like eBay and Craigslist.  You can often ask your friends of they have an old GPU that they are no longer using.</p>"},{"location":"feedback/","title":"Feedback on Graph Data Modeling","text":"<p>You are welcome to connect with me on anytime on LinkedIn or submit any issues to GitHub Issue Log.  All pull-requests with fixes to errors or additions are always welcome.</p> <p>If you would like to fill out a short survey and give us ideas on how we can create better tools for intelligent textbooks in the future.</p>"},{"location":"glossary/","title":"Glossary","text":""},{"location":"glossary/#action","title":"Action","text":"<p>The process of calling a specific tool directly from within an agent.</p> <p>In a multi-step agent, at each step, the LLM can write an action, in the form of some calls to external tools.  Tools much be carefully described so that agents can match their capabilities with the needs of a the user.</p> <p>A common format (used by Anthropic, OpenAI, and many others) for writing these actions is generally different variations of \"writing actions as a JSON of tools names, description and arguments to use, which you then parse to know which tool to execute and with which arguments\".</p>"},{"location":"glossary/#agency-level","title":"Agency Level","text":"<p>A classification system that defines the degree of autonomy and decision-making authority granted to an intelligent software agent.</p> <p>In the context of intelligent software agents, agency levels provide a framework for understanding and designing agents with appropriate capabilities for specific use cases. These levels range from simple decision support (Level 0-1) to complex autonomous behaviors including code generation and execution (Level 5). In this course, students progressively implement agents with increasing agency levels as they develop more sophisticated understanding and implementation skills.</p> <p>Example: A student project might begin with a Level 2 agent that can select and call predefined tools based on user input, then evolve to a Level 5 agent capable of writing, testing, and executing its own Python code to solve novel problems.</p> <p>Here are additional terms that are not currently in the glossary but would be valuable additions for the course on intelligent software agents:</p>"},{"location":"glossary/#agent","title":"Agent","text":"<p>An autonomous computational entity that performs tasks on behalf of users or other programs with some degree of independence or autonomy.</p> <p>In this course we focus on agents that use and add precise knowledge representation of the user's goals or desires.</p> <p>Our agents use graphs to give precise context and provide a rules engines tool for agents to use.</p> <ul> <li>See also: Rules Engine</li> </ul>"},{"location":"glossary/#agentic-workflow","title":"Agentic Workflow","text":"<p>A sequence of operations where large language models control the execution flow, decision points, and tool selection to accomplish complex tasks autonomously.</p> <p>In the context of intelligent software agents, agentic workflows represent the orchestration of planning, reasoning, and action that enables an agent to pursue goals with minimal human intervention. These workflows may involve loops, conditionals, and dynamic adjustments based on intermediate results. In this course, students implement increasingly sophisticated agentic workflows as they progress from basic tool-calling to advanced code generation and execution.</p> <p>Example: A student might develop an agentic workflow for data analysis that begins with data exploration, automatically determines which cleaning operations are needed, selects appropriate visualization techniques based on data characteristics, and finally generates insights without requiring step-by-step human guidance.#### Agntcy</p> <p>A set of proposed standards for Agent interoperability.</p> <ul> <li>See Agntcy on Perplexity</li> </ul>"},{"location":"glossary/#agntcy","title":"Agntcy","text":"<p>A open-source agent collaboration standard to allow agents to communicate and work together.</p> <p>Agntcy was launched in early 2025 to establish standards and infrastructure for AI agent collaboration. This initiative, spearheaded by tech leaders Cisco, LangChain, and Galileo as core maintainers, with contributions from Glean and LlamaIndex, envisions creating an \"Internet of Agents\" \u2013 an open, interoperable foundation enabling AI agents from different platforms to communicate and work together seamlessly.</p> <p>AGNTCY is (pronounced \"agency\").</p> <ul> <li>See Perplexity</li> <li>See also: Tool</li> <li>See also: Model Content Protocol</li> </ul>"},{"location":"glossary/#business-process-modeling-notation","title":"Business Process Modeling Notation","text":"<p>BPMN References Guide</p>"},{"location":"glossary/#code-agent","title":"Code Agent","text":"<p>A specialized type of artificial intelligence agent that directly generates and executes programming code to perform tasks autonomously.</p> <p>In the context of intelligent software agents, code agents represent an advanced form of agency where the agent produces executable code rather than structured formats like JSON to perform actions. This approach allows for greater expressiveness, efficiency, and flexibility when solving complex problems, as the agent can leverage the full capabilities of a programming language. Code agents in this course are implemented using the SmolAgents framework and typically generate Python code.</p> <p>Example: A student might implement a code agent that analyzes a dataset by writing Python code to load the CSV file, perform statistical calculations, and visualize the results, all without requiring predefined functions for each specific operation.</p>"},{"location":"glossary/#controlled-imports","title":"Controlled Imports","text":"<p>A security mechanism that restricts which external libraries or modules an agent can access when executing generated code.</p> <p>In the context of intelligent software agents, controlled imports are a critical safety feature, particularly for code agents that generate and execute Python code. By limiting the modules an agent can import and use, developers can prevent potentially harmful operations while still allowing the agent to perform its intended functions. In this course, controlled imports would be implemented as part of the sandboxed environment when working with code-generating agents.</p> <p>Example: When configuring a code agent for a data analysis task, a student would implement controlled imports to allow access to pandas and matplotlib for data processing and visualization, while restricting access to system-level modules that could pose security risks.</p>"},{"location":"glossary/#decorator","title":"Decorator","text":"<p>A decorator is a design pattern implemented as a special syntax that allows a function, method, or class to be modified or extended without changing its source code. </p> <p>In this course, we use Python decorators with the @ symbol followed by a decorator name such as <code>@tool</code>  placed above the definition of the function to be decorated. </p> <p>They effectively wrap the target function, enabling pre-processing of arguments, post-processing of return values, modification of behavior, or registration within a larger system.</p>"},{"location":"glossary/#deepseek-r1","title":"DeepSeek R1","text":"<p>A large language model developed in China that combines mixture of experts with reinforcement learning to create efficient, smaller-scale models.</p> <p>In the context of intelligent software agents, DeepSeek R1 represents an accessible yet powerful foundation model that can run on consumer-grade hardware while maintaining strong reasoning and code generation capabilities. The model's architecture enables it to perform complex tasks with relatively modest computational requirements. In this course, the 7 billion parameter variant of DeepSeek R1 is deployed through Ollama to serve as the reasoning engine for various agent implementations.</p> <p>Example: A student might observe that their code agent running on DeepSeek R1 7B can generate effective Python code for data analysis while maintaining an inference speed of over 50 tokens per second on their local GPU.</p>"},{"location":"glossary/#docstring","title":"Docstring","text":"<p>A docstring in Python is a string literal that appears as the first statement in a module, function, class, or method definition.</p> <p>Python docstrings are enclosed by triple quotes (<code>\"\"\"</code> or <code>'''</code>) and can span multiple lines.</p> <p>Docstrings combined with the <code>@tool</code> decorator are used to document the purpose, behavior, parameters, and return values of the code object. </p> <p>This approach allows agents to understand not just what functions are available, but when and how to use them appropriately based on their documented purpose and parameters.</p>"},{"location":"glossary/#finalanswertool","title":"FinalAnswerTool","text":"<p>A component within agent frameworks that enables an agent to provide definitive responses to user queries.</p> <p>In the context of intelligent software agents, the FinalAnswerTool represents the simplest form of agent output mechanism, allowing the agent to formulate and deliver conclusive responses. It serves as a foundation for more complex tool interactions and is often used as a fallback when specialized tools are not required for a particular query. In the SmolAgents framework used in this course, the FinalAnswerTool is a basic building block for agent implementation.</p> <p>Example: When implementing their first agent using SmolAgents, a student would start by incorporating the FinalAnswerTool to allow their agent to respond directly to informational queries before adding more specialized tools for data manipulation or code execution.</p>"},{"location":"glossary/#goal-driven-process-gdp","title":"Goal-Driven Process (GDP)","text":"<p>In the context of the Software Development Life Cycle (SDLC), GDP typically refers to \"Goal-Driven Process\" or \"Goal-Directed Planning.\" This is a methodological approach within software development that emphasizes:</p> <p>Defining clear goals and objectives for the software project Aligning development processes with these established goals Making decisions throughout the development lifecycle based on how they contribute to achieving the defined goals</p> <p>The GDP approach within SDLC helps teams:</p> <ul> <li>Maintain focus on delivering value that aligns with business or user objectives</li> <li>Prioritize features and development tasks based on goal contribution</li> <li>Establish measurable criteria for evaluating progress and success</li> <li>Create a framework for making consistent decisions when trade-offs are necessary</li> </ul> <p>This approach can be implemented across various SDLC methodologies, including Agile, Waterfall, or hybrid approaches, as it's more about the strategic alignment of development efforts rather than the specific technical implementation process.</p>"},{"location":"glossary/#graphics-processing-unit","title":"Graphics Processing Unit","text":"<p>A specialized device designed to accelerate matrix operations using parallel processing.</p> <p>In the context of intelligent software agents, GPUs are relevant as they provide parallel processing capabilities that significantly accelerate machine learning model training and inference operations, enabling more complex and performant AI systems. The parallel architecture of GPUs allows for efficient matrix operations that form the computational backbone of many neural network implementations.</p> <p>Example: When deploying a code agent for real-time coding tasks, a developer would typically configure the environment to utilize a GPU to ensure the agent can perform inference operations at sufficient speed, rather than relying solely on CPU processing.</p>"},{"location":"glossary/#iso-definition","title":"ISO Definition","text":"<p>A term definition is considered to be consistent with ISO metadata registry guideline 11179 if it meets the following criteria:</p> <ul> <li>Precise: Using words with unambiguous meanings that clearly delineate the exact scope and boundaries of the concept being defined.</li> <li>Concise: Employing the minimum number of words necessary to communicate the complete meaning without unnecessary elaboration.</li> <li>Distinct: Clearly differentiating the term from other related concepts to avoid confusion or overlap with existing terminology.</li> <li>Non-circular: Avoiding the use of the term being defined within its own definition.</li> <li>Unencumbered with business rules: Focusing solely on what the term means rather than including operational constraints, implementation details, or contextual usage rules.</li> </ul> <p>Our agents use these definitions to enrich prompts that use these terms.</p>"},{"location":"glossary/#integrated-development-environment","title":"Integrated Development Environment","text":"<p>A software application that provides comprehensive facilities to programmers for software development, combining multiple development tools into a single graphical user interface.</p> <p>In the context of intelligent software agents, IDEs are relevant as they provide environments where code agents can be integrated to assist developers with tasks such as code completion, error detection, refactoring suggestions, and automated documentation. Modern IDEs often incorporate AI-powered features that enhance developer productivity by leveraging language models to understand code context and provide intelligent assistance.</p> <p>Example: A developer working with a code agent might configure their IDE to seamlessly interact with the agent, allowing it to analyze code as it's being written, suggest improvements based on best practices, and even generate test cases automatically based on the function implementations.</p> <ul> <li>Codium Windsurf</li> <li>Lovable</li> <li>Cursor</li> </ul>"},{"location":"glossary/#intelligent-textbook","title":"Intelligent Textbook","text":"<p>An interactive digital learning resource that employs artificial intelligence to generate personalized educational content and adaptive learning experiences in response to individual student needs and progress.</p> <p>In the context of intelligent software agents, intelligent textbooks represent an advanced application domain where multiple specialized agents collaborate to deliver customized educational experiences. These agents work together to analyze student performance, identify knowledge gaps, generate appropriate lesson content, create interactive simulations, and adapt teaching methodologies in real-time. The intelligent textbook serves as both a platform for agent integration and an interface through which students interact with these educational AI systems.</p> <p>Example: A computer science student using an intelligent textbook to learn about data structures might receive dynamically generated lesson content on binary trees based on their previous interaction patterns, followed by an interactive coding simulation that adjusts its complexity based on the student's demonstrated proficiency, all orchestrated by specialized AI agents working in concert to optimize the learning experience.</p>"},{"location":"glossary/#knowledge-graph","title":"Knowledge Graph","text":"<p>A structured representation of knowledge consisting of entities, their semantic types, properties, and the relationships between them.</p> <p>In the context of intelligent software agents, knowledge graphs provide a mechanism for organizing and interconnecting domain-specific information in a way that facilitates reasoning, inference, and contextual understanding. Knowledge graphs extend beyond simple data storage by explicitly modeling the relationships between concepts, allowing agents to navigate complex information spaces more effectively. In this course, knowledge graphs may be used to augment agent capabilities by providing structured knowledge beyond what is encoded in the base language model.</p> <p>Example: A student might enhance their agent by creating a knowledge graph of programming concepts and their relationships, enabling the agent to understand connections between different libraries, functions, and programming paradigms when generating code solutions.</p>"},{"location":"glossary/#knowledge-representation","title":"Knowledge Representation","text":"<p>The methods and structures used to encode information about the world.</p> <p>In this book, we focus on knowledge representation in a form that an agent can utilize for reasoning and decision-making. It provides the foundation for an agent's understanding of its environment, goals, and possible actions.</p> <p>Knowledge representation encompasses:</p> <ol> <li> <p>Symbolic Structures: Formalized frameworks such as ontologies, semantic networks, frames, rules, or logic-based systems that capture entities, relationships, and constraints in the agent's domain.</p> </li> <li> <p>Information Organization: Methods for structuring knowledge in ways that facilitate efficient access, inference, and updates as the agent interacts with its environment.</p> </li> <li> <p>Reasoning Mechanisms: Techniques that allow agents to derive new knowledge from existing representations, evaluate truth values, and make decisions based on incomplete information.</p> </li> <li> <p>Semantic Grounding: Connections between abstract symbols and their real-world referents, allowing the agent to map internal representations to external phenomena.</p> </li> </ol> <p>Effective knowledge representation systems in agents balance expressiveness (the ability to represent complex knowledge) with computational efficiency (allowing the agent to reason within reasonable time and resource constraints).</p>"},{"location":"glossary/#llm-agent-control-levels","title":"LLM Agent Control Levels","text":"<p>The amount you allow an LLM to control agentic execution. Her are some sample levels:</p> <ol> <li>Level 0 - LLMs have no control over workflows</li> <li>Level 1 - LLMs can impact an if/then/else decision within workflows</li> <li>Level 2 - LLMs determine what functions should be called</li> <li>Level 3 - LLMs control iteration, and program order and continuation</li> <li>Level 4 - LLMs can start another agentic workflow</li> <li>Level 5 - LLMs can generate new code and run it</li> </ol> <p>See also: HuggingFace Agent Definition</p>"},{"location":"glossary/#llm-agents","title":"LLM Agents","text":"<p>A specialized type of software agent where LLM outputs control the workflow.</p> <p>Note that \"agency\" evolves on a continuous spectrum, as you give more or less power to the LLM on your workflow.</p>"},{"location":"glossary/#llm-reasoning","title":"LLM Reasoning","text":"<p>The process by which a large language model analyzes information, makes inferences, and arrives at conclusions through patterns learned during training.</p> <p>In the context of intelligent software agents, LLM reasoning forms the cognitive foundation that enables agents to understand requests, plan approaches, and generate appropriate responses or actions. While traditional software relies on explicit logic, LLM reasoning emerges from statistical patterns learned across vast text corpora. In this course, students learn to leverage and guide LLM reasoning through effective prompting, context management, and task decomposition.</p> <p>Example: When designing an agent to solve complex programming problems, a student would structure their prompts to encourage step-by-step LLM reasoning, allowing the model to break down the problem, consider alternative approaches, and identify potential edge cases before generating code.</p>"},{"location":"glossary/#langchain","title":"LangChain","text":"<p>An open-source framework designed to simplify the development of applications that leverages large language models (LLMs). </p> <p>LangChain provides a standardized interface for chaining together different components needed for LLM applications, such as prompt templates, language models, memory systems, and external tools or data sources.</p> <p>Key features of LangChain include:</p> <ol> <li>Chains: Sequences of operations that combine prompts, models, and other components to perform complex tasks.</li> <li>Agents: Systems that use LLMs as reasoning engines to determine which actions to take based on user input and tool feedback.</li> <li>Memory: Components that allow applications to maintain conversational state and context over interactions.</li> <li>Retrieval Augmented Generation (RAG): Tools for connecting LLMs to external knowledge sources and databases to enhance responses with specific information.</li> <li>Tool integration: Standardized methods for connecting LLMs to external APIs, databases, and computational resources.</li> </ol> <p>LangChain helps developers build sophisticated applications like chatbots, question-answering systems, summarization tools, and other AI assistants by providing reusable components and patterns that work across different LLM providers.</p> <p>The main disadvantage of LangChain is a long learning curve.  To use LangChain you must master several abstractions.</p>"},{"location":"glossary/#large-language-model","title":"Large Language Model","text":"<p>A deep learning model that is trained on natural language and used to predict the next token.</p> <p>Examples of LLMs include BERT, DeepSeek, GPTs and Llama.</p>"},{"location":"glossary/#model-context-protocol","title":"Model Context Protocol","text":"<p>An open protocol and standardized set of rules and data structures that govern the exchange of contextual information between an application programming interface (API) and a large language model (LLM).</p> <p>The Model Context Protocol (MCP) defines how prompts, system instructions, session states, user interactions, and relevant contextual metadata are structured, transmitted, and maintained to ensure consistent and coherent model behavior across interactions.</p> <p>Example: In an AI chatbot API, a Model Context Protocol may define how user messages, past conversation history, user preferences, and domain-specific knowledge are encoded and sent to the LLM, ensuring that responses remain relevant and contextually aware.</p> <ul> <li>Model Context Protocol</li> <li>Cursor Model Content Protocol</li> </ul>"},{"location":"glossary/#multi-agent-system","title":"Multi-Agent System","text":"<p>A computational framework composed of multiple interacting intelligent agents working together to solve complex problems or achieve shared goals.</p> <p>In the context of intelligent software agents, multi-agent systems represent an advanced architectural pattern where specialized agents collaborate, communicate, and coordinate their activities. Each agent may have distinct capabilities, knowledge bases, or roles, but collectively they address problems that would be difficult for a single agent to solve effectively. In this course, students explore how to design and implement multi-agent systems where agents with complementary capabilities can interact.</p> <p>Example: For their capstone project, a team might develop a multi-agent system for software development assistance, with specialized agents handling requirements analysis, code generation, testing, and documentation, all coordinating through a shared communication protocol.</p>"},{"location":"glossary/#organizational-knowledge-graph","title":"Organizational Knowledge Graph","text":"<p>A structured representation of enterprise-wide knowledge, processes, and information organized as interconnected entities and relationships that reflect an organization's collective understanding, operational contexts, and institutional memory.</p> <p>In the context of intelligent software agents, organizational knowledge graphs are relevant as they provide a comprehensive framework for agents to access, interpret, and reason with proprietary knowledge distributed across different levels and units of an organization. These graphs enable agents to navigate complex organizational structures, understand departmental interdependencies, and apply appropriate business rules while respecting established hierarchies. Organizational knowledge graphs serve as centralized repositories that standardize terminology, document relationships between business entities, and preserve institutional expertise that can be leveraged by intelligent systems to provide consistent and contextually appropriate assistance across the organization.</p> <p>Example: A financial services company might implement an organizational knowledge graph that maps relationships between products, compliance requirements, customer segments, and internal processes, with role-based access control (RBAC) ensuring that when executives interact with intelligence agents, they receive strategic insights across all domains, while customer service representatives' agents can only access relevant product information and approved customer interaction protocols appropriate to their authorization level.</p> <ul> <li>See also: Personal Knowledge Graph</li> <li>See also: Project Knowledge Graph</li> </ul>"},{"location":"glossary/#ollama","title":"Ollama","text":"<p>A software framework that enables deployment and execution of large language models locally on personal computing devices.</p> <p>In the context of intelligent software agents, Ollama provides a crucial capability for running LLMs without cloud dependencies, allowing for development and testing of agents with reduced latency and cost. The framework optimizes resource utilization and provides standardized interfaces for model management, inference, and integration with agent frameworks like SmolAgents. In this course, Ollama is used to run the DeepSeek R1 model on local GPUs.</p> <p>Example: Before implementing their code agent, a student would first install Ollama on their system and download the DeepSeek R1 7B model to enable local execution of their agent without relying on external API services.</p>"},{"location":"glossary/#personal-knowledge-graph","title":"Personal Knowledge Graph","text":"<p>A structured representation of an individual's knowledge, experiences, and information organized as interconnected typed entities and typed relationships that reflect personal understanding and context.</p> <p>In the context of intelligent software agents, personal knowledge graphs are relevant as they provide a mechanism for agents to access, interpret, and reason with individualized knowledge that complements the general knowledge contained in large language models. These graphs enable agents to maintain persistent and personalized information contexts that evolve through interaction, allowing for more accurate and contextually appropriate responses to user queries based on their unique knowledge domain.</p> <p>Example: When a student uses a code agent for programming assistance throughout a semester, the agent might maintain a personal knowledge graph tracking which programming concepts the student has mastered, their preferred coding style, and frequently used libraries, enabling increasingly personalized recommendations and explanations tailored to the student's progressive understanding.</p> <ul> <li>See also: Project Knowledge Graph</li> <li>See also: Organizational Knowledge Graph</li> </ul>"},{"location":"glossary/#private-knowledge","title":"Private Knowledge","text":"<p>Knowledge that is not publicly available.</p> <p>For organizations, private knowledge usually is stored on an intranet or private company databases such as ERP and CRM systems.</p> <p>Software Agents must combine both public knowledge (usually within a LLM) with private knowledge.</p>"},{"location":"glossary/#project-knowledge-graph","title":"Project Knowledge Graph","text":"<p>A structured representation of collective knowledge, resources, and information organized as interconnected entities and relationships that reflect a project team's shared understanding and context.</p> <p>In the context of intelligent software agents, project knowledge graphs are relevant as they provide a centralized mechanism for agents to access, interpret, and reason with domain-specific knowledge shared across a project team. These graphs enable consistent information exchange between team members and agents, capturing project-specific terminology, relationships, dependencies, and constraints that may not exist in general knowledge sources. Project knowledge graphs allow intelligent agents to provide contextually appropriate assistance that aligns with established project conventions and accumulated team wisdom.</p> <p>Example: A development team building an intelligent agent system might maintain a project knowledge graph documenting the relationships between various components, API specifications, design decisions, and implementation constraints, enabling their code agents to generate suggestions that adhere to project-specific patterns and integrate correctly with existing systems created by different team members.</p> <ul> <li>See also: Personal Knowledge Graph</li> <li>See also: Organizational Knowledge Graph</li> </ul>"},{"location":"glossary/#procedural-code","title":"Procedural Code","text":"<p>Procedural code is a programming paradigm that focuses on defining a sequence of operations for a computer to execute.</p> <p>It emphasizes how a program should accomplish tasks through explicit step-by-step instructions, control flow structures, and state changes. Procedural code typically organizes functionality into procedures or functions that are called in a predetermined order.</p> <p>Procedural code is contracted with declarative code and agentic code.</p>"},{"location":"glossary/#public-knowledge","title":"Public Knowledge","text":"<p>Knowledge that is openly available on public data sources such as the Internet and Wikipedia.</p> <p>Public knowledge is contrasted to private knowledge that has limited access.  For organizations, private knowledge usually is stored on an intranet or private company databases such as ERP and CRM systems.</p> <p>Software Agents must combine both public knowledge (usually within a LLM) with private knowledge.</p>"},{"location":"glossary/#pydanticai","title":"PydanticAI","text":"<p>A Python agent framework designed to make it easy to build production grade applications with Generative AI.</p> <p>PydanticAI is a reaction to overly abstract frameworks such as LangChain that have long learning curves to manage the abstractions.</p> <ul> <li>See also PydanticAI Web Site</li> </ul>"},{"location":"glossary/#react-framework","title":"ReAct framework","text":"<p>The use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner.  This interleaving provides better results than separated workflows.</p> <p>Reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. </p> <ul> <li>ReAct: Synergizing Reasoning and Acting in Language Models - March 2023</li> </ul>"},{"location":"glossary/#reasoning-trace","title":"Reasoning Trace","text":"<p>A documented sequence of logical steps generated by an agent that explains its thought process in arriving at a decision or conclusion.</p> <p>In the context of intelligent software agents, reasoning traces provide transparency into an agent's decision-making, allowing users and developers to understand how and why particular actions were chosen. Reasoning traces are a key component of the ReAct framework, where they are interleaved with actions to improve performance and explainability. In this course, students implement agents that generate reasoning traces to make their behavior more interpretable and trustworthy.</p> <p>Example: When analyzing a complex dataset, a student's agent might produce a reasoning trace that outlines its approach: \"First, I'll check for missing values in the dataset. Then, I'll examine the distribution of each variable. Based on those distributions, I'll select appropriate statistical tests for the hypothesis.\"</p>"},{"location":"glossary/#retrieval-augmented-generation-rag","title":"Retrieval Augmented Generation (RAG)","text":"<p>A hybrid approach that enhances language model outputs by retrieving relevant information from external knowledge sources before generating responses.</p> <p>In the context of intelligent software agents, RAG systems address the limitation of fixed knowledge in pre-trained models by dynamically incorporating domain-specific or up-to-date information during inference. This approach allows agents to leverage both the reasoning capabilities of language models and accurate, specific knowledge from external sources. In this course, RAG techniques are used to combine public knowledge (encoded in LLMs) with private knowledge sources specific to particular domains or organizations.</p> <p>Example: A student implementing an agent for academic research assistance might use RAG to retrieve information from course materials and research papers before generating responses, ensuring the agent provides accurate and relevant guidance based on the latest scholarship.</p>"},{"location":"glossary/#rules-engine","title":"Rules Engine","text":"<p>A software system that manages and executes a set of business rules in a runtime production environment. </p> <p>Rules engines separate business logic from application code by allowing rules to be defined, stored, and managed independently. Rules engines evaluate conditions against input data and execute corresponding actions when conditions are met, enabling non-technical users to modify business logic without changing the underlying application code.</p> <p>Key components typically include:</p> <ol> <li>A rule repository for storing and managing business rules</li> <li>A rule authoring interface for creating and modifying rules</li> <li>A rule execution engine that evaluates conditions and triggers actions</li> <li>Working memory that holds the data being evaluated against the rules</li> </ol> <p>Rules engines are commonly used in complex decision-making systems, regulatory compliance, fraud detection, pricing strategies, and other scenarios where business logic frequently changes or requires transparency.</p>"},{"location":"glossary/#sandboxed-environment","title":"Sandboxed Environment","text":"<p>A secured, isolated execution context that constrains the resources, capabilities, and potential impact of code generated by an artificial intelligence system.</p> <p>In the context of intelligent software agents, sandboxed environments provide essential protection when working with code agents or any system that executes dynamically generated code. They establish boundaries around what operations can be performed, what files can be accessed, and what system resources can be utilized, mitigating the risk of unintended or malicious behaviors. In this course, sandboxed environments are used when implementing code agents to ensure safe execution of agent-generated Python code.</p> <p>Example: To safely deploy their code agent in a production setting, a student would configure a sandboxed environment that allows the agent to process data and generate insights without having access to sensitive system files or network capabilities.</p>"},{"location":"glossary/#smolagents","title":"SmolAgents","text":"<p>A lightweight agent framework developed by HuggingFace designed for efficient creation and deployment of language model-powered agents.</p> <p>In the context of intelligent software agents, SmolAgents provides a streamlined approach to agent development that reduces unnecessary abstraction layers while maintaining flexibility for implementing various agency levels. The framework supports multiple agent types, including specialized code agents that can generate and execute Python code directly. In this course, SmolAgents serves as the primary framework for implementing and experimenting with different agent architectures.</p> <p>Example: Using the SmolAgents library, a student might implement a basic agent that uses the <code>FinalAnswerTool</code> to provide responses, then progressively enhance it to a code agent capable of writing and executing Python functions to solve more complex problems.</p>"},{"location":"glossary/#token-generation-rate","title":"Token Generation Rate","text":"<p>The speed at which a language model produces new text elements, measured in tokens per second during the inference process.</p> <p>In the context of intelligent software agents, token generation rate is a critical performance metric that directly impacts the responsiveness and usability of agent systems. Higher generation rates result in more fluid interactions and shorter response times, particularly important for interactive applications. In this course, token generation rate is a consideration when deploying models locally using Ollama, with adequate GPU hardware enabling rates of 50+ tokens per second for models like DeepSeek R1.</p> <p>Example: A student might observe that after optimizing their local environment, their agent built on DeepSeek R1 achieves a token generation rate of 55 tokens per second, making it suitable for real-time coding assistance tasks that require prompt responses.</p>"},{"location":"glossary/#tool","title":"Tool","text":"<p>An atomic function used by an agent. </p> <p>To be used by an LLM, tools needs a few attributes that constitute its API and will be used to describe to the LLM how to call this tool.  Here  are the four key attributes a tool needs:</p> <ol> <li>A tool name - this is usually the name of a python function</li> <li>A description - this comes from the Python Docstring</li> <li>Input parameter names, types and descriptions which include descriptions of enumerated values</li> <li> <p>An output format including types and how errors are returned</p> </li> <li> <p>See also: Docstring</p> </li> <li>See also: Decorator</li> </ol>"},{"location":"glossary/#tool-calling","title":"Tool Calling","text":"<p>Letting an LLM call a specific function usually with specific patterns.</p> <p>Example: Let an LLM call a function to get similar text to a question from a knowledge base.</p> <ul> <li>See also: Tool</li> <li>See also: Docstring</li> <li>See also: Decorator</li> </ul>"},{"location":"glossary/#tool-catalog","title":"Tool Catalog","text":"<p>A comprehensive inventory of functions, APIs, or services available for an agent to utilize when performing tasks.</p> <p>In the context of intelligent software agents, tool catalogs serve as the interface between an agent's reasoning capabilities and the external world. A well-designed tool catalog provides the agent with clearly defined operations it can perform, complete with detailed metadata about each tool's purpose, required parameters, and expected outputs. In this course, tool catalogs are typically implemented using Python functions decorated with <code>@tool</code> and accompanied by descriptive docstrings.</p> <p>Example: When building an agent to assist with data analysis tasks, a student would first create a tool catalog containing functions for data loading, transformation, statistical analysis, and visualization, each documented with proper parameters and return types.</p>"},{"location":"how-we-built-this-site/","title":"How We Built This Site","text":"<p>This page describes how we built this website and some of  the rationale behind why we made various design choices.</p>"},{"location":"how-we-built-this-site/#python","title":"Python","text":"<p>MicroSims are about how we use generative AI to create animations and simulations.  The language of AI is Python.  So we wanted to create a site that could be easily understood by Python developers.</p>"},{"location":"how-we-built-this-site/#mkdocs-vs-docusaurus","title":"Mkdocs vs. Docusaurus","text":"<p>There are two main tools used by Python developers to write documentation: Mkdocs and Docusaurus.  Mkdocs is easier to use and more popular than Docusaurus. Docusaurus is also optimized for single-page applications. Mkdocs also has an extensive library of themes and plugins. None of us are experts in JavaScript or React. Based on our ChatGPT Analysis of the Tradeoffs we chose mkdocs for this site management.</p>"},{"location":"how-we-built-this-site/#github-and-github-pages","title":"GitHub and GitHub Pages","text":"<p>GitHub is a logical choice to store our  site source code and documentation.  GitHub also has a Custom GitHub Action that does auto-deployment if any files on the site change. We don't currently have this action enabled, but other teams can use this feature if they don't have the ability to do a local build with mkdocs.</p> <p>GitHub also has Issues,  Projects and releases that we can use to manage our bugs and tasks.</p> <p>The best practice for low-cost websites that have public-only content is GitHub Pages. Mkdocs has a command (<code>mkdocs gh-deploy</code>) that does deployment directly to GitHub Pages.  This was an easy choice to make.</p>"},{"location":"how-we-built-this-site/#github-clone","title":"GitHub Clone","text":"<p>If you would like to clone this repository, here are the commands:</p> <pre><code>mkdir projects\ncd projects\ngit clone https://github.com/dmccreary/microsims\n</code></pre>"},{"location":"how-we-built-this-site/#after-changes","title":"After Changes","text":"<p>After you make local changes you must do the following:</p> <pre><code># add the new files to a a local commit transaction\ngit add FILES\n# Execute the a local commit with a message about what and why you are doing the commit\ngit commit -m \"comment\"\n# Update the central GitHub repository\ngit push\n</code></pre>"},{"location":"how-we-built-this-site/#material-theme","title":"Material Theme","text":"<p>We had several options when picking a mkdocs theme:</p> <ol> <li>Mkdocs default</li> <li>Readthedocs</li> <li>Third-Party Themes See Ranking</li> </ol> <p>The Material Theme had 16K stars.  No other theme had over a few hundred. This was also an easy design decision.</p> <p>One key criterial was the social Open Graph tags so that when our users post a link to a simulation, the image of the simulation is included in the link.  Since Material supported this, we used the Material theme. You can see our ChatGPT Design Decision Analysis if you want to check our decision process.</p>"},{"location":"how-we-built-this-site/#enable-edit-icon","title":"Enable Edit Icon","text":"<p>To enable the Edit icon on all pages, you must add the edit_uri and the content.action.edit under the theme features area.</p> <pre><code>edit_uri: edit/master/docs/\n</code></pre> <pre><code>    theme:\n        features:\n            - content.action.edit\n</code></pre>"},{"location":"how-we-built-this-site/#conda-vs-venv","title":"Conda vs VENV","text":"<p>There are two choices for virtual environments.  We can use the native Python venv or use Conda.  venv is simle but is only designed for pure Python projects.  We imagine that this site could use JavaScript and other langauges in the future, so we picked Conda. There is nothing on this microsite that prevents you from using one or the other.  See the ChatGPT Analysis Here.</p> <p>Here is the conda script that we ran to create a new mkdocs environment that also supports the material social imaging libraries.</p> <pre><code>conda deactivate\nconda create -n mkdocs python=3\nconda activate mkdocs\npip install mkdocs \"mkdocs-material[imaging]\"\n</code></pre>"},{"location":"how-we-built-this-site/#mkdocs-commands","title":"Mkdocs Commands","text":"<p>There are three simple mkdoc commands we use.</p>"},{"location":"how-we-built-this-site/#local-build","title":"Local Build","text":"<pre><code>mkdocs build\n</code></pre> <p>This builds your website in a folder called <code>site</code>.  Use this to test that the mkdocs.yml site is working and does not have any errors.</p>"},{"location":"how-we-built-this-site/#run-a-local-server","title":"Run a Local Server","text":"<pre><code>mkdocs serve\n</code></pre> <p>This runs a server on <code>http://localhost:8000</code>. Use this to test the display formatting locally before you push your code up to the GitHub repo.</p> <pre><code>mkdoc gh-deploy\n</code></pre> <p>This pushes everything up to the GitHub Pages site. Note that it does not commit your code to GitHub.</p>"},{"location":"how-we-built-this-site/#mkdocs-material-social-tags","title":"Mkdocs Material Social Tags","text":"<p>We are using the Material Social tags.  This is a work in progress!</p> <p>Here is what we have learned.</p> <ol> <li>There are extensive image processing libraries that can't be installed with just pip.  You will need to run a tool like brew on the Mac to get the libraries installed.</li> <li>Even after <code>brew</code> installs the libraries, you have to get your environment to find the libraries.  The only way I could get that to work was to set up a local UNIX environment variable.</li> </ol> <p>Here is the brew command that I ran:</p> <pre><code>brew install cairo freetype libffi libjpeg libpng zlib\n</code></pre> <p>I then had to add the following to my ~/.zshrc file:</p> <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib\n</code></pre> <p>Note that I am running on a Mac with Apple silicon.  This means that the image libraries that brew downloads must be specific to the Mac Arm instruction set.</p>"},{"location":"how-we-built-this-site/#image-generation-and-compression","title":"Image Generation and Compression","text":"<p>I have used ChatGPT to create most of my images.  However, they are too large for most websites.  To compress them down I used  https://tinypng.com/ which is a free tool  for compressing png images without significant loss of quality.  The files created with ChatGPT are typically around 1-2 MB.  After  using the TinyPNG site the size is typically around 200-300KB.</p> <ul> <li>Cover images for blog post #4364</li> <li>Discussion on overriding the Social Card Image</li> </ul>"},{"location":"license/","title":"Creative Commons License","text":"<p>All content in this repository is governed by the following license agreement:</p>"},{"location":"license/#license-type","title":"License Type","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED)</p>"},{"location":"license/#link-to-license-agreement","title":"Link to License Agreement","text":"<p>https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en</p>"},{"location":"license/#your-rights","title":"Your Rights","text":"<p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format</li> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#restrictions","title":"Restrictions","text":"<ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> <li>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</li> </ul> <p>Notices</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p> <p>This deed highlights only some of the key features and terms of the actual license. It is not a license and has no legal value. You should carefully review all of the terms and conditions of the actual license before using the licensed material.</p>"},{"location":"references/","title":"Site References","text":""},{"location":"references/#agentic-interoperability","title":"Agentic Interoperability","text":"<ul> <li>Agntcy on Perplexity</li> </ul>"},{"location":"references/#knowledge","title":"Knowledge","text":"<p>Deloitte Whitepaper: Knowledge Enriched Agentic Workflows</p>"},{"location":"references/#key-tutorials","title":"Key Tutorials","text":""},{"location":"references/#_1","title":"References","text":""},{"location":"references/#references-for-agents-that-do-web-search","title":"References for Agents that Do Web Search","text":"<ol> <li> <p>OpenAI Deep Research</p> </li> <li> <p>mkdocs - https://www.mkdocs.org/ - this is our tool for building the website.  It converts Markdown into HTML in the <code>site</code> directory.</p> </li> <li>mkdocs material theme - https://squidfunk.github.io/mkdocs-material/ - this is the theme for our site.  The theme adds the user interface elements that give our site the look and feel.  It also has the features such as social cards.</li> <li>GitHub Pages - https://pages.github.com/ - this is the free tool for hosting public websites created by mkdocs</li> <li>Markdown - https://www.mkdocs.org/user-guide/writing-your-docs/#writing-with-markdown - this is the format we use for text.  It allows us to have headers, lists, tables, links and images without learning HTML.</li> <li>Deploy Mkdocs GitHub Action - https://github.com/marketplace/actions/deploy-mkdocs - this is the tool we use to automatically build our site after edits are checked in with Git.</li> <li>Git Book - https://git-scm.com/book/en/v2 - a useful book on Git.  Just read the first two chapters to learn how to check in new code.</li> <li>Conda - https://conda.io/ - this is a command line tool that keeps our Python libraries organized for each project.</li> <li>VS Code - https://code.visualstudio.com/ - this is the integrated development environment we use to mange the files on our website.</li> <li>Markdown Paste - https://marketplace.visualstudio.com/items?itemName=telesoho.vscode-markdown-paste-image - this is the VS code extension we use to make sure we keep the markdown format generated by ChatGPT.</li> </ol>"},{"location":"concepts/","title":"Key Concepts in Agentic AI","text":"<p>Code Agents</p>"},{"location":"concepts/agents/","title":"Software Agents","text":"<p>A software agent is an autonomous computational entity that performs tasks on behalf of users or other programs with some degree of independence or autonomy, and in doing so, employs a precise knowledge representation of the user's goals or desires.</p>"},{"location":"concepts/agents/#agents-are-not-procedural-code","title":"Agents are Not Procedural Code","text":"<p>Here are some characteristics differentiating agents from standard procedural code:</p> <ol> <li>Autonomy: Agents operate without direct human intervention, making decisions and taking actions based on their internal state and perception of the environment. Unlike procedural code, which executes a predetermined sequence of instructions, and can follow precise rules, agents can decide when and how to act.</li> <li>Reactivity and Adaptability: Agents can perceive their environment and respond to changes in a timely fashion, adjusting their behavior accordingly. Procedural code follows a fixed execution path regardless of environmental changes unless explicitly programmed to handle specific scenarios.</li> <li>Goal-Orientation: Agents act in pursuit of goals rather than simply executing instructions. They can prioritize actions, resolve conflicts between competing objectives, and find alternative paths to achieve their goals when faced with obstacles. Procedural code lacks this inherent purpose-driven behavior and instead follows explicit instructions without considering overarching objectives.</li> </ol>"},{"location":"concepts/agents/#agents-and-rules-engines","title":"Agents and Rules Engines","text":""},{"location":"concepts/background/","title":"Useful Background Concepts","text":"<p>Here are some useful concepts that we will not cover too much in this course. This document is provided for you to review if you don't have a strong background in Python, LLMs, or knowledge representation.</p>"},{"location":"concepts/background/#what-is-a-llm","title":"What is a LLM?","text":""},{"location":"concepts/background/#how-do-you-write-a-good-prompt","title":"How do you write a good prompt?","text":""},{"location":"concepts/background/#what-is-a-knowledge-graph","title":"What is a knowledge graph?","text":""},{"location":"concepts/background/#how-do-we-represent-concepts","title":"How do we represent concepts?","text":""},{"location":"concepts/background/#how-do-we-measure-llm-quality","title":"How do we measure LLM quality?","text":""},{"location":"concepts/code-agents/","title":"What are Code Agents?","text":"<p>The CodeAgent in the Hugging Face SmolAgents library is a specialized type of AI agent designed to perform tasks by directly generating and executing Python code snippets. Unlike traditional agents that rely on structured formats like JSON or plain text to specify which tools to call and with what parameters, the CodeAgent takes a code-centric approach, allowing it to write Python code that executes actions immediately.</p>"},{"location":"concepts/code-agents/#key-features-of-the-codeagent","title":"Key Features of the CodeAgent","text":"<ul> <li>Direct Code Generation: The CodeAgent produces executable Python code to carry out its tasks, such as calling functions, performing computations, or interacting with external tools.</li> <li>Flexibility: By leveraging Python\u2019s programming constructs\u2014like loops, conditionals, and variables\u2014it can handle complex operations more naturally than JSON-based instructions.</li> <li>Reduced Overhead: Since the agent skips intermediate steps (e.g., parsing JSON to determine tool calls), it can execute actions more efficiently.</li> </ul>"},{"location":"concepts/code-agents/#how-it-differs-from-other-agents","title":"How It Differs from Other Agents","text":"<p>Traditional agents, including some supported by the SmolAgents library, typically operate as follows: - JSON or Text-Based Tool Calls: They generate structured outputs (e.g., JSON objects or text blobs) that describe which tools to invoke and with what arguments. For example:</p> <pre><code>  {\n    \"tool\": \"search\",\n    \"query\": \"HuggingFace SmolAgents\"\n  }\n</code></pre> <p>This JSON would then be parsed and mapped to a function call by the system. - Multi-Step Process: These agents require an additional layer of interpretation to translate the structured output into executable actions, which can introduce complexity and latency.</p> <p>In contrast, the CodeAgent:</p> <ul> <li>Writes Code Directly: Instead of producing JSON, it might generate something like:</li> </ul> <pre><code>result = search_tool(\"HuggingFace SmolAgents\")\n</code></pre> <p>This code is then executed in a controlled environment, eliminating the need for parsing or mapping.</p> <ul> <li>Streamlined Execution: By bypassing structured intermediaries, the CodeAgent reduces the number of steps between decision-making and action, potentially improving performance and enabling more sophisticated workflows.</li> </ul>"},{"location":"concepts/code-agents/#advantages-of-the-codeagent-approach","title":"Advantages of the CodeAgent Approach","text":"<ul> <li>Efficiency: Fewer processing steps mean faster execution, which can be a significant advantage in performance-sensitive applications.</li> <li>Expressiveness: Python\u2019s full range of programming features allows the agent to tackle intricate tasks that might be cumbersome to encode in JSON or text.</li> <li>Innovation: This code-based method is a distinctive feature of the SmolAgents library, setting it apart from other frameworks that lean heavily on traditional tool-calling paradigms.</li> </ul>"},{"location":"concepts/code-agents/#safety-considerations","title":"Safety Considerations","text":"<p>Executing generated code poses security risks, such as unintended system access or malicious operations. The SmolAgents library addresses this by providing:</p> <ul> <li>Controlled Imports: Limiting the modules the CodeAgent can access.</li> <li>Sandboxed Environments: Running the code in isolated contexts to prevent harmful effects.</li> </ul>"},{"location":"concepts/code-agents/#conclusion","title":"Conclusion","text":"<p>The CodeAgent stands out in the Hugging Face SmolAgents library as an innovative agent that uses Python code snippets to perform actions, contrasting with traditional agents that rely on JSON or text-based tool calls. This approach offers greater flexibility, efficiency, and power, making it a compelling choice for developers building AI-driven solutions within a secure and streamlined framework.</p>"},{"location":"concepts/code-agents/#references","title":"References","text":"<ul> <li>HuggingFace: Secure code execution in SmolAgents</li> <li>HuggingFace Paper</li> <li>Arxiv.org: Executable Code Actions Elicit Better LLM Agents This work proposes to use executable Python code to consolidate LLM agents\u2019 actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions</li> </ul>"},{"location":"concepts/deepseek-r1/","title":"Deepseek R1","text":""},{"location":"concepts/deepseek-r1/#technical-breakdown","title":"Technical Breakdown","text":"<p>Architecture and Model Variants: DeepSeek R1 models are built on a transformer-based mixture-of-experts (MoE) architecture, with an enormous 671 billion total parameters, of which about 37 billion are active per inference forward pass (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat) (deepseek-ai/DeepSeek-R1 - Hugging Face). This design allows the model to leverage many expert subnetworks while keeping the inference budget similar to a ~37B dense model. The context window is extended to 128,000 tokens, supporting very long inputs and outputs (comparable to GPT-4's 128K context support) (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat) (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). The DeepSeek-R1 family includes the full MoE model (671B) and several distilled dense models (1.5B, 7B, 8B, 14B, 32B, 70B) that capture R1's reasoning patterns in smaller architectures (deepseek-ai/DeepSeek-R1 - Hugging Face) (deepseek-ai/DeepSeek-R1 - Hugging Face). The distilled models are based on popular backbones like Qwen (Alibaba) and Llama and use the same tokenizer modifications as their bases for compatibility (deepseek-ai/DeepSeek-R1 - Hugging Face).</p> <p>Training Methodology: The DeepSeek R1 training pipeline is novel in that it heavily uses reinforcement learning (RL) to instill reasoning skills. The initial \"DeepSeek-R1-Zero\" model was obtained by applying large-scale RL directly on a pre-trained base (DeepSeek-V3-Base) without any supervised fine-tuning (SFT) seed (deepseek-ai/DeepSeek-R1 - Hugging Face) (deepseek-ai/DeepSeek-R1 - Hugging Face). This demonstrated that complex reasoning behaviors (like chain-of-thought, self-reflection, and self-verification) can emerge solely from RL rewards, a first in open research (deepseek-ai/DeepSeek-R1 - Hugging Face). However, R1-Zero suffered from issues like repetitive outputs and mixed-language responses (deepseek-ai/DeepSeek-R1 - Hugging Face). To address this, the final DeepSeek R1 model incorporated a more balanced training pipeline with two SFT stages and two RL stages (deepseek-ai/DeepSeek-R1 - Hugging Face). In practice, a small \"cold-start\" SFT on curated data was used to prime the base model (improving coherence and following instructions), then an RL phase encouraged the model to discover better reasoning strategies (e.g. using longer chain-of-thought for complex problems). This was followed by another SFT (to integrate the high-quality RL-generated solutions and improve general readability), and a final RL fine-tuning focused on aligning with human preferences and instructions (deepseek-ai/DeepSeek-R1 - Hugging Face). The RL training signal uses both learned reward models and rule-based verifiers -- for example, math answers are checked by an internal solver and code solutions by execution, similar to techniques used by OpenAI and Alibaba's Qwen team (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). Notably, DeepSeek R1's RL-first approach proved that even without an initial human-supervised phase, an LLM can learn to reason through trial-and-error reinforcement -- a key architectural innovation of this model (deepseek-ai/DeepSeek-R1 - Hugging Face).</p> <p>Key Optimizations: Several optimizations in DeepSeek R1's architecture and training improved its efficiency and performance. In the underlying DeepSeek-V3 base (the foundation for R1), the team introduced an auxiliary-loss-free load balancing strategy for the MoE layers, ensuring that the many experts are utilized effectively without needing extra loss terms (GitHub - deepseek-ai/DeepSeek-V3). This addresses a common challenge in MoE models where some experts can become under-trained; DeepSeek's solution keeps expert usage balanced purely through careful gating architecture. The model also uses multi-token prediction objectives during pre-training (predicting multiple tokens per step) to boost training efficiency and downstream performance (GitHub - deepseek-ai/DeepSeek-V3). Standard enhancements like RoPE positional encodings for long context, SwiGLU activation, and RMSNorm are employed similar to other modern LLMs (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). Compared to dense models of similar scale, DeepSeek R1's MoE approach trades higher memory footprint for faster inference per token -- e.g. it has ~671B total weights (requiring ~1.5 TB VRAM to load fully), but only a fraction (~37B) are activated for each input, meaning inference compute is closer to a 37B model's cost (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). In practice, running the full DeepSeek-R1 requires a multi-GPU setup (e.g. 16\u00d7A100 GPUs) (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat), but the distilled R1 models provide much more accessible alternatives. For instance, the 32B distilled model (DeepSeek-R1-Distill-Qwen-32B) can achieve comparable results with just ~24GB GPU memory needed (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat), using a dense architecture distilled from R1's knowledge. This strategy of training one very large \"teacher\" model and then compressing its knowledge into smaller models is a key optimization that DeepSeek uses to make deployment practical. All DeepSeek R1 models are released under a permissive MIT license, allowing commercial use and further modification (deepseek-r1), which contrasts with the more restrictive licenses of some contemporary open models. In summary, DeepSeek R1's design innovates by combining extreme scale (hundreds of billions of parameters), advanced MoE engineering, and an RL-driven training regime focused on reasoning -- setting it apart from conventional LLM training pipelines (which usually rely heavily on supervised fine-tuning and human feedback alignment before any RL).</p> <p>Parameters and Efficiency vs. Other LLMs: In terms of size, DeepSeek R1 (671B) is one of the largest language models ever publicized. It far exceeds the dense parameter count of models like GPT-4 (estimated on the order of ~170B to 1T, though exact numbers are unpublished) and Meta's Llama 3 (which introduced a 405B dense variant as its largest model) (What Is Meta's Llama 3.1 405B? How It Works, Use Cases &amp; More) (GitHub - deepseek-ai/DeepSeek-V3). Even Google's PaLM 2 and Gemini prototypes are smaller in parameter count than R1's total. However, thanks to its MoE architecture, R1's effective inference footprint (37B) is lower than GPT-4's if GPT-4 is purely dense -- making R1 surprisingly efficient for its scale. For example, GPT-4's 32k-context version is known to be very resource-intensive, whereas DeepSeek R1 can handle 128k context with its MoE sharding of work across experts (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat) (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). Against Llama 3, the largest Llama 3.1 (405B dense) achieves strong performance but still slightly trails DeepSeek R1 on challenging benchmarks (e.g. MMLU) (GitHub - deepseek-ai/DeepSeek-V3), likely because R1's RL-enhanced reasoning gives it an edge beyond what scaling alone provided. Smaller open models like Mistral (7B) and Falcon (40B) are dramatically smaller than R1 and emphasize efficiency and speed on limited hardware, but they cannot match DeepSeek R1's state-of-the-art results in complex tasks due to the vast gap in training scale and techniques. For instance, Falcon-40B (one of the best pre-R1 open models) scored around 54% on MMLU (MMLU: Better Benchmarking for LLM Language Understanding), whereas R1 approaches ~91% on the same benchmark (see next section). In summary, DeepSeek R1 pushes the limits of model size and training innovation, going beyond the architectures of GPT-4 and others by using MoE at scale and RL-driven training. Its design balances the frontier of maximum capability with strategies (like distillation and MoE gating) to make such capability usable in practice.</p>"},{"location":"concepts/deepseek-r1/#benchmarks-and-performance","title":"Benchmarks and Performance","text":"<p>DeepSeek R1 has been evaluated on a wide range of standard NLP and reasoning benchmarks, where it achieves cutting-edge performance comparable to or surpassing other leading LLMs of 2024-2025. Below is a summary of how R1 stacks up on key tasks:</p> <ul> <li> <p>Knowledge and Reasoning (MMLU): On the Massive Multitask Language Understanding benchmark (MMLU, which tests knowledge across 57 subjects), DeepSeek R1 scores around 90.8% (accuracy) (deepseek-ai/DeepSeek-R1 - Hugging Face). This is essentially at GPT-4 level -- for comparison, an OpenAI model (\"o1-1217\", presumably GPT-4 or similar) scored 91.8% on the same test (deepseek-ai/DeepSeek-R1 - Hugging Face). R1's score also exceeds Anthropic's Claude 3.5 (which was ~88--90% (deepseek-ai/DeepSeek-R1 - Hugging Face)) and vastly outperforms open-source predecessors like Llama 2 (which was ~68% on MMLU) and Falcon-40B (~54% (MMLU: Better Benchmarking for LLM Language Understanding)). In a variant \"MMLU-Redux\" evaluation, R1 reached 92.9%, the highest among the compared models (deepseek-ai/DeepSeek-R1 - Hugging Face). These results demonstrate that R1 has acquired a broad range of world knowledge and can reason across domains nearly as well as the best proprietary models.</p> </li> <li> <p>Mathematical Reasoning (GSM8K, MATH, AIME): DeepSeek R1 particularly shines at math and quantitative reasoning tasks, likely due to its RL-based training that emphasized step-by-step solution verification. On GSM8K (Grade School Math problems), R1 achieves high accuracy -- sources indicate 85%+ range (DeepSeek R1 vs Qwen 2.5 Max: A Detailed Comparison of Features and Performance), which is on par with or slightly above GPT-4's performance on this dataset. For more advanced math, R1 reached 79.8% pass@1 on the AIME 2024 math competition problems (deepseek-ai/DeepSeek-R1 - Hugging Face), essentially tying OpenAI's model (which scored 79.2%). On the challenging MATH dataset (MATH-500) of high school math problems, R1's chain-of-thought solving ability yields an impressive 97.3% pass@1 (deepseek-ai/DeepSeek-R1 - Hugging Face) -- outperforming OpenAI's reference (96.4%). These numbers are remarkable: they suggest DeepSeek R1 can correctly solve nearly all math questions in that benchmark when allowed to generate and evaluate multiple solution attempts. In summary, R1 has state-of-the-art math reasoning skills, leveraging its self-reflection and verifier-enhanced training to check its work. This is a significant improvement over models like GPT-3.5 or Llama, which struggled with complex multi-step math. Even smaller distilled versions of R1 do well -- e.g. the 32B distilled model achieves over 72% on GSM8K and ~94% on MATH, rivaling much larger dense models (DeepSeek R1 vs Qwen 2.5 Max: A Detailed Comparison of Features and Performance) (deepseek-ai/DeepSeek-R1 - Hugging Face).</p> </li> <li> <p>Coding and Code Reasoning: DeepSeek R1 is highly proficient at coding tasks. On HumanEval (a benchmark of Python coding problems), R1 achieves about 71% pass@1 (solving rate) (DeepSeek R1 vs Qwen 2.5 Max: A Detailed Comparison of Features and Performance), slightly edging out Alibaba's Qwen-2.5 Max (69.3%) and comparable to GPT-4's level on this test. This indicates that given a programming prompt, R1 can write correct solutions the majority of the time without needing multiple attempts. Moreover, R1 was tested on more difficult coding benchmarks that involve reasoning and multi-step planning. For instance, on LiveCodeBench (which requires generating code with intermediate reasoning steps), R1 attained 65.9% pass@1 (with chain-of-thought), which is higher than OpenAI's smaller model (o1-mini 53.8%) and even slightly above OpenAI's full model (~63.4%) (deepseek-ai/DeepSeek-R1 - Hugging Face). Additionally, R1 was evaluated on Codeforces programming challenges, where it achieved a competitive Elo rating around 2030, very close to OpenAI's result (~2061) (deepseek-ai/DeepSeek-R1 - Hugging Face). These results suggest R1 not only writes correct code, but can handle complex programming puzzles with reasoning -- a testament to the effectiveness of its RL training on coding tasks (possibly using code execution feedback as a reward). In practical terms, DeepSeek R1's coding ability is among the best in class, making it useful for code generation, debugging, and assisting in software tasks at a level comparable to top closed models. Even the distilled R1-32B model shows strong coding performance (~57% on LiveCodeBench, Codeforces ~1690 rating) outperforming other open models of similar size (deepseek-ai/DeepSeek-R1 - Hugging Face).</p> </li> <li> <p>General NLP and Text Generation: Beyond specialized tasks, R1 was evaluated on general instruction-following and reasoning benchmarks. In AlpacaEval 2.0, a benchmark that compares models on helpfulness and clarity of responses to instructions, DeepSeek R1 had an 87.6% win rate in pairwise comparisons (beating a strong reference in the vast majority of test prompts) (deepseek-ai/DeepSeek-R1 - Hugging Face). This is notably higher than Claude 3.5 (~52%) or GPT-4 (~51%) in that evaluation, indicating R1 produces very high-quality, helpful responses in a chat/instruction setting (deepseek-ai/DeepSeek-R1 - Hugging Face). Similarly, on Big Bench Hard (BBH) -- a collection of challenging reasoning tasks -- R1 scored about 80.2%, slightly above Qwen-2.5 Max's 78.5% (DeepSeek R1 vs Qwen 2.5 Max: A Detailed Comparison of Features and Performance). These scores underline R1's strength in complex reasoning and following instructions: thanks to its reinforcement learning fine-tuning, it can perform lengthy chain-of-thought reasoning and self-check its answers, leading to very thorough and correct responses on difficult queries. However, some trade-offs were observed: in a Prompt Strictness test (following instructions to the letter), R1 scored a bit lower (83.3) than Claude 3.5 (86.5) (deepseek-ai/DeepSeek-R1 - Hugging Face), suggesting that R1's focus on deep reasoning sometimes comes at the cost of strictly obeying the prompt format or brevity. Overall, for text generation tasks requiring reasoning, depth, and accuracy, R1 is among the top performers, whereas for simple prompt adherence or safe-chat behavior, models like Claude (with heavy alignment training) might have an edge.</p> </li> <li> <p>Multilingual Understanding: DeepSeek R1 was trained on a large multilingual corpus (notably both English and Chinese data), and it exhibits excellent multilingual capabilities. On C-Eval, a comprehensive Chinese evaluation benchmark, DeepSeek R1 achieved 91.8% (exact match) (deepseek-ai/DeepSeek-R1 - Hugging Face), dramatically surpassing GPT-4's performance (~76.0% on the same test) (deepseek-ai/DeepSeek-R1 - Hugging Face). This indicates R1 has a superior grasp of Chinese academic and common knowledge tasks, likely due to targeted training data and perhaps specific rewards in RL for Chinese reasoning. On a Chinese WSC (winograd style coreference) test, R1 also tops the charts at 92.8% (deepseek-ai/DeepSeek-R1 - Hugging Face). In a Chinese simple QA benchmark, R1 is slightly ahead of GPT-4 (63.7% vs 58.7% accuracy) (deepseek-ai/DeepSeek-R1 - Hugging Face). These results demonstrate that R1 is not just an English-centric model but a bilingual powerhouse, especially strong in Chinese -- likely on par with or better than native Chinese models. For other languages, R1's multilingual training (and possibly larger context) help it perform robustly, though detailed scores weren't listed in the source, we can infer from related benchmarks (XGLUE, XTREME) that it performs competitively but might be edged out by models like Qwen 2.5 Max in some multilingual categories (DeepSeek R1 vs Qwen 2.5 Max: A Detailed Comparison of Features and Performance) (since Qwen was specifically optimized for broad multilingual support). Still, the availability of R1's model for multiple languages has been a boon for non-English NLP. Its performance in multilingual QA and cross-lingual tasks is state-of-the-art among open models, reducing the gap with specialized models built for those languages.</p> </li> </ul> <p>In summary, DeepSeek R1's benchmark performance is on par with the very best LLMs (GPT-4 class) in many areas. It excels at complex reasoning, coding, and math -- often matching or slightly surpassing GPT-4 and Claude on those tasks (deepseek-ai/DeepSeek-R1 - Hugging Face) (deepseek-ai/DeepSeek-R1 - Hugging Face). It also holds its own in knowledge and general understanding tasks (MMLU ~90+%). There are a few areas where OpenAI's or others maintain a lead, such as certain open-ended knowledge queries (for example, OpenAI's model scored higher on a broad GPQA benchmark: 75.7 vs R1's 71.5 (deepseek-ai/DeepSeek-R1 - Hugging Face)) -- suggesting GPT-4 might have an edge in extensive world knowledge or factual recall. But the gaps are small. Importantly, R1's distilled smaller models also achieve remarkable performance for their size. The 32B R1-Distill, for instance, outperforms OpenAI's smaller \"o1-mini\" model across many benchmarks (deepseek-ai/DeepSeek-R1 - Hugging Face), setting new state-of-art results among models in the 30B-70B range. This means R1's impact isn't limited to those with giant compute clusters -- its knowledge has effectively trickled down to models one can run on a single GPU, without losing too much accuracy. The upshot is that DeepSeek R1 established itself as one of the top-performing LLMs across the board, validating the team's emphasis on reasoning-centric training through strong benchmark results in reasoning, coding, text generation quality, and multilingual understanding.</p>"},{"location":"concepts/deepseek-r1/#comparison-with-other-llms","title":"Comparison with Other LLMs","text":"<p>DeepSeek R1 arrives in a landscape alongside both proprietary giants (like OpenAI's GPT-4 series and Anthropic's Claude) and open-source models (like Meta's Llama 3 and newcomers such as Mistral and Falcon). Below we highlight how DeepSeek R1 differs from and compares to these models in terms of capabilities, efficiency, and deployment:</p> <ul> <li> <p>GPT-4 (OpenAI \"o\" series): GPT-4 is the most prominent closed-source model and R1's performance peer. In head-to-head comparisons, DeepSeek R1 is roughly on par with GPT-4 on many academic and coding benchmarks -- for example, their MMLU scores differ by only ~1% (deepseek-ai/DeepSeek-R1 - Hugging Face), and they trade blows on coding tasks (GPT-4 slightly behind on some, slightly ahead on others) (deepseek-ai/DeepSeek-R1 - Hugging Face). Architecturally, GPT-4 is believed to be a dense transformer (possibly with some mixture-of-expert components internally, but not confirmed) with an estimated hundreds of billions of parameters. DeepSeek R1, by contrast, is explicitly a Mixture-of-Experts model with 671B parameters total (and 37B active) (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). This gives R1 an advantage in specialization (many experts can handle different aspects of a problem) but makes it heavy to deploy in full. GPT-4's training relied on a large supervised fine-tuning and human-feedback loop, making it very aligned and reliable in following instructions and staying within guardrails. R1's training put more emphasis on autonomous reasoning via RL, which yielded superior problem-solving but at times slightly less compliance (as seen in prompt strictness tests) (deepseek-ai/DeepSeek-R1 - Hugging Face). In practical use, GPT-4 might produce more concise or safe answers by default, whereas R1 might produce more detailed, step-by-step explanations (since it was rewarded for showing its reasoning). Capabilities: Both models handle a wide spectrum of tasks (coding, writing, reasoning, etc.). GPT-4 remains a bit better in open-domain knowledge QA (likely due to broader training data or retrieval strategies in deployment) (deepseek-ai/DeepSeek-R1 - Hugging Face), whereas R1 has a slight edge in systematic reasoning (e.g. long math proofs, complex code with self-checking). GPT-4 is multi-modal (can accept images) in some versions; DeepSeek R1 is text-only (no image input capability reported in R1). Efficiency and Deployment: GPT-4 is accessible only via API (cloud) and not runnable locally, with OpenAI tightly controlling its model. DeepSeek R1 is fully open-source -- anyone can download the weights (deepseek-ai/DeepSeek-R1 - Hugging Face) -- but running the full 671B model requires significant hardware (multi-GPU server). However, R1's distilled models (e.g. 32B, 70B) can be run on a single high-end GPU or modest GPU cluster, giving practitioners a local alternative that approximates GPT-4-level performance. In terms of context length, both are in the top tier: GPT-4 supports up to 128k tokens (with the 2024 updates), and R1 similarly supports 128k context out of the box (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). This makes them both suitable for lengthy documents or transcripts, unlike many smaller models. In sum, DeepSeek R1 has essentially brought GPT-4-like capabilities to the open source world, with slight differences in style and accessibility: GPT-4 is still the leader in fine alignment and perhaps breadth of training data, while R1 demonstrates the power of an RL-trained, openly available model that organizations can host themselves.</p> </li> <li> <p>Claude (Anthropic): Claude 2 and 3 (e.g. Claude 3.5 \"Sonnet\") are Anthropic's large models focusing on safety and large context. Claude 3.5 was one of the first to offer 100k token context windows and is known for being very conversationally aligned (trying to be helpful, harmless, honest). In evaluations, Claude 3.5 Sonnet reaches about 90.5% on MMLU, roughly equal to R1 and GPT-4. However, in coding and some reasoning tasks, Claude tends to lag slightly behind both GPT-4 and R1. For example, Claude 3.5 scored ~52% win-rate on AlpacaEval vs R1's 87% (deepseek-ai/DeepSeek-R1 - Hugging Face), indicating that on complex instruction-following, R1 can provide more robust answers. Claude's strength is its harmlessness and compliance -- it's less likely to produce problematic content and often follows user instructions very literally. R1, while aligned via RLHF in its final stage, may not have undergone as intensive a safety training as Claude, so it might be more prone to generating unfiltered content if prompted maliciously (though no specific issues have been reported publicly). Efficiency: Claude is closed-source and accessible through API (Anthropic's), with no known parameter count (estimated similar scale to GPT-4). R1's open availability is a contrast -- anyone worrying about data privacy or needing offline use can opt for R1's models instead of Claude. Context: Both offer huge context (Claude 100k vs R1 128k), making them suitable for long documents and conversations. Use cases: If an enterprise's priority is a pre-aligned, safety-first model with massive context (and doesn't mind cloud usage), Claude is attractive. If the priority is maximal reasoning ability, code proficiency, and self-hosting, DeepSeek R1 is a compelling alternative, delivering stronger problem-solving performance in many cases. It's worth noting that R1's RL-based reasoning is somewhat in line with Anthropic's philosophy of constitutional AI (models improving via feedback); both signify a move towards models that can reflect on their answers. But R1 took a more direct approach by training with explicit problem-solving rewards, whereas Claude was trained on a large mix of dialogue with a safety-focused objective.</p> </li> <li> <p>Llama 3 (Meta): Meta's Llama series (Llama 2 was 2023, Llama 3 in 2024) represents the cutting edge of open foundational models. Llama 3 introduced models at 8B, 70B, and a gargantuan 405B \"Llama 3.1\" model (What Is Meta's Llama 3.1 405B? How It Works, Use Cases &amp; More). The 405B Llama is a dense model that Meta made available to researchers and on platforms like Azure, touting it as the largest open model at the time. Despite its size and quality (it outperforms Llama 2 by a large margin and is competitive with many proprietary models (What Is Meta's Llama 3.1 405B? How It Works, Use Cases &amp; More)), Llama 3.1--405B slightly underperforms DeepSeek R1 on reasoning benchmarks -- for instance, Llama3.1 achieved ~88.6% on MMLU (GitHub - deepseek-ai/DeepSeek-V3), whereas R1 reached ~90.8%. This suggests that beyond a certain scale, training methodology becomes crucial: R1's specialized reinforcement learning approach likely extracted more reasoning ability than Meta's standard next-token prediction training plus instruction tuning. On coding, Llama 70B or 405B are strong (Llama 70B chat was around ~67% HumanEval), but R1 still has the edge (71%+ HumanEval) (DeepSeek R1 vs Qwen 2.5 Max: A Detailed Comparison of Features and Performance). Where Llama 3 excels is being a versatile base -- many fine-tuned variants exist for various tasks, and it is fully open (though with a Meta-specific license). DeepSeek R1 actually leverages Llama 3 in its distilled models -- for example, \"DeepSeek-R1-Distill-Llama-70B\" is R1's knowledge distilled into a Llama 3.3 70B instruct model (deepseek-r1) (deepseek-r1). This cross-pollination shows that R1 complements Llama: one can use R1 as an oracle teacher to improve Llama-family models. Efficiency: Llama 3's 70B and smaller models can run on a single GPU (with 70B needing ~2\u00d748GB GPUs or so), making them easier to deploy than R1's full model. The 405B Llama3 obviously is large (likely requiring 8+ GPUs), but still it's a dense model -- some simpler infrastructure than MoE. R1's MoE might be trickier to serve (needing a custom MoE runtime or partitioning across experts). Deployment: Both R1 and Llama 3 are open; however, Meta's license for Llama might restrict commercial use (as was the case with Llama 2's license for certain users). DeepSeek R1 is MIT licensed (very permissive) (deepseek-r1), which encourages broader adoption, even in commercial products, without legal worry. Another difference is training data focus: Llama was trained on a balanced multilingual corpus but not especially targeting any single domain. DeepSeek R1's base (DeepSeek-V3) was trained on a massive 14.8T token corpus including a lot of English/Chinese and technical data (GitHub - deepseek-ai/DeepSeek-V3), and then R1 was further tuned on reasoning heavy data. Thus R1 might have more baked-in \"problem-solving knowledge\" (e.g. math proofs, code debugging logs, etc.) than a vanilla Llama. For a developer deciding between them: if one wants a solid general-purpose model to fine-tune, Llama 70B is great; if one wants the best reasoning out-of-the-box and possibly to study novel training techniques, DeepSeek R1 is the exemplar.</p> </li> <li> <p>Mistral (Open-Source Efficient LLMs): Mistral 7B (2023) and any subsequent models from Mistral AI highlight efficiency: the 7B model was trained on a high-quality dataset and tuned to outperform larger models like Llama2-13B in many tasks, despite its small size. However, there is a clear gap between what can be achieved at single-digit billions of parameters and what R1 (with hundreds of billions + specialized training) achieves. For example, Mistral 7B's MMLU score is around mid-60s% (Mistral 7B vs DeepSeek R1 Performance: Which LLM is the Better Choice?), whereas R1 is ~90%. On coding, Mistral 7B might score ~30-40% on HumanEval, versus R1's 71%. These differences are huge -- indicating that while techniques like data quality, longer training, and smart initialization can boost a small model, they cannot fully compensate for scale and advanced training on the hardest tasks. That said, Mistral and similar models have an ultra-low footprint: a 7B model can run on a consumer laptop or even a phone in some cases. DeepSeek R1's distilled 7B (there is an R1-distill Qwen-7B model) actually leverages R1's strengths to also outperform standard 7B models (deepseek-ai/DeepSeek-R1 - Hugging Face). In fact, DeepSeek-R1-Distill-7B (Qwen base) reportedly achieves ~49% on HumanEval and ~83% on GSM8K (deepseek-ai/DeepSeek-R1 - Hugging Face), which is much higher than a naive 7B model could do -- showing the value of R1's knowledge transfer. So, comparing R1 to Mistral: R1 is a research-grade, maximum performance model, whereas Mistral 7B (and hypothetical 13B or 20B follow-ups) are deployment-grade, compact models. One might use R1 to generate data or insights to then fine-tune a Mistral model, for instance. Another difference: R1 uses 128k context; most small models have 4k to 8k context (Mistral 7B extended to 8k with RoPE scaling). So R1 also wins in context length by a large margin over typical small models. In sum, R1 isn't directly \"competing\" with a 7B model -- they occupy different ends of the spectrum -- but R1 has redefined the upper bound of open model capability, whereas Mistral et al. define how far one can go with minimal parameter budgets. It's worth noting that R1's advent may inspire more efficient techniques to approximate its performance: e.g., Alibaba's QwQ-32B showed that clever multi-stage RL on a 32B model could reach R1-like reasoning quality (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat) (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). So smaller, efficient LLMs are quickly learning from R1's example to narrow the gap.</p> </li> <li> <p>Falcon and Other Open Models: Falcon-40B (from TII UAE) was a leading open model prior to Llama 2, with good performance on English tasks but relatively weaker alignment and limited context (2048 tokens). DeepSeek R1 clearly outperforms Falcon-40B by a wide margin on virtually every benchmark (knowledge, reasoning, etc.), as indicated by Falcon's ~63% average on some evaluations vs R1's ~90% in the same (MMLU: Better Benchmarking for LLM Language Understanding). R1's release (and that of Llama 2/3) essentially eclipsed Falcon, though Falcon demonstrated the viability of large open models (and had an Apache license). Other models like PaLM 2 (Google) and Gemini (Google DeepMind) are closed-source but were expected to be strong; the VentureBeat article notes that by late 2024, OpenAI's \"o3\" series and Google's Gemini were also focusing on reasoning and extended context, partly influenced by the success of models like DeepSeek R1 (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). We see that R1 pushed the envelope, causing even the big players to iterate (e.g., OpenAI's hypothetical \"o3\" might be an even more reasoning-focused successor to GPT-4). In deployment terms, R1 being open means organizations that prefer self-hosting had, for the first time, an alternative to relying on API access to GPT-4/Claude for top-tier performance. This is a significant strategic difference from proprietary models -- R1 can be deployed on Azure AI Foundry, on local servers, or via community projects like Hugging Face and Ollama, giving users more control (DeepSeek R1 is now available on Azure AI Foundry and GitHub | Microsoft Azure Blog). Models like GPT-4/Claude require trusting a third-party service. On the flip side, proprietary models often integrate up-to-date knowledge (live browsing or tools) in their offerings, something R1 by itself doesn't do (though one can combine R1 with retrieval systems in a RAG pipeline to mitigate its knowledge cutoff).</p> </li> </ul>"},{"location":"concepts/deepseek-r1/#summary","title":"Summary","text":"<p>In summary, DeepSeek R1 stands out for its unprecedented combination of openness and high performance. Against closed models (GPT-4, Claude), it competes neck-and-neck in ability while offering the transparency and modifiability of open source. Against other open models (Llama 3, Falcon, Mistral), R1 defines the high end of capability, introducing training innovations that others are now adopting or distilling. Each model has its niche: GPT-4 remains the general-purpose gold standard with strong factuality and multi-modality, Claude is the aligned long-context specialist, Llama is the adaptable open base model, and DeepSeek R1 is the reasoning and problem-solving expert that bridges the gap between research and open deployment. The existence of R1 has effectively raised the bar for all LLMs, ensuring that new models (whether open or closed) must contend with a state-of-the-art performer that is freely available for anyone to use or build upon.</p>"},{"location":"concepts/deepseek-r1/#code-example-using-deepseek-r1-with-ollama","title":"Code Example (Using DeepSeek R1 with Ollama)","text":"<p>One convenient way to run DeepSeek R1 models locally is via Ollama, a tool for serving and using LLMs on local GPUs. The DeepSeek R1 family is available in Ollama's model library, including the distilled versions that are feasible to run on a single GPU. Below is a code snippet demonstrating how to download and run the DeepSeek R1 32B distilled model (which offers excellent performance) using Ollama on a local machine:</p> <pre><code># Install Ollama (if not already installed) and ensure you have a GPU with sufficient VRAM (e.g. 24GB for the 32B model).\n\n# Pull the DeepSeek-R1 32B model from Ollama's library:\nollama pull deepseek-r1:32b\n\n# Once the model is downloaded, you can run queries against it.\n# For example, ask a question or prompt:\nollama run deepseek-r1:32b --prompt \"Explain the architecture of DeepSeek R1 in a few sentences.\"\n</code></pre> <p>The above commands will load the DeepSeek-R1-Distill-Qwen-32B model and execute the given prompt on your local GPU. You should see the model's generated answer in the console. The <code>ollama pull</code> step is only needed the first time to download the weights; afterward, <code>ollama run deepseek-r1:32b \"&lt;your prompt&gt;\"</code> is sufficient. You can replace <code>32b</code> with other available model sizes (such as <code>70b</code> for the 70B Llama-derived model, or <code>7b</code> for a smaller model) depending on your hardware capacity (deepseek-r1) (deepseek-r1). For instance, <code>ollama run deepseek-r1:70b</code> would run the 70B distilled model (which may require 2\u00d7GPU for smooth operation), and <code>ollama run deepseek-r1:7b</code> would run a much smaller 7B model. All DeepSeek R1 models in Ollama are under the MIT license, so they can be used commercially. This local deployment option illustrates the practicality of R1's open-source approach -- even though the flagship 671B model is huge, its distilled offspring allow developers to experiment with R1's capabilities on everyday hardware.</p>"},{"location":"concepts/deepseek-r1/#strategic-consequences","title":"Strategic Consequences","text":"<p>The release of DeepSeek R1 has had significant strategic implications for the AI development landscape, especially in the open-source community and global AI competition:</p> <ul> <li> <p>Democratizing High-End AI: Perhaps the most immediate impact of DeepSeek R1 is that it bridged the performance gap between open-source and closed-source models. Prior to R1, the best open models (like Llama 2 or Falcon) were notably behind models like GPT-4 in capability. R1's emergence as an open model comparable to GPT-4 (deepseek-ai/DeepSeek-R1 - Hugging Face) proved that top-tier AI is not exclusive to tech giants. This has empowered researchers, startups, and even hobbyists worldwide -- they now have access to a GPT-4-class model that can be studied and integrated without needing permission or hefty API fees. Microsoft's inclusion of DeepSeek R1 in Azure AI Foundry (DeepSeek R1 is now available on Azure AI Foundry and GitHub | Microsoft Azure Blog) underscores this democratization: a major cloud provider saw value in offering R1 alongside proprietary models, giving enterprise customers more choice. The strategic shift here is that open models are now part of the \"frontier models\" conversation, not just supporting players. We see organizations leveraging R1 to build advanced applications entirely on open infrastructure, which pressures closed model providers to justify their cost and policies. In essence, DeepSeek R1 fueled a greater openness in AI, forcing a reevaluation of the balance between proprietary advantage and community collaboration.</p> </li> <li> <p>Influence on Research Directions: DeepSeek R1's successful use of reinforcement learning for reasoning has spurred new research into LLM training techniques. The fact that R1-Zero showed emergent reasoning purely from RL rewards (no initial human examples) was a groundbreaking result (deepseek-ai/DeepSeek-R1 - Hugging Face). This has encouraged other AI labs to experiment with reinforcement learning, self-play, and other beyond-supervised methods to push reasoning capabilities. For example, OpenAI's rumored \"o3\" series and projects like Anthropic's next Claude are likely incorporating lessons from R1, focusing on inference-time reasoning loops, self-reflection, and multi-stage training (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). We also see academic interest in the idea of Large Reasoning Models (LRMs) as a category, which R1 exemplifies -- models that explicitly perform internal reasoning steps to improve answer quality (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). Additionally, R1's distillation of reasoning into smaller models validated a path to efficiency: rather than training a small model to do reasoning from scratch, use a big RL-tuned teacher to generate high-quality reasoning traces and fine-tune the small model on that. This approach achieved better results for the 32B model than training it with RL directly (deepseek-ai/DeepSeek-R1 - Hugging Face). As a consequence, we're seeing a strategic shift in how new models are developed -- many teams are now adopting a two-stage approach (train a very large model with new techniques, then compress it). The open release of R1 (and its data) has given researchers a wealth of material to study, leading to a flurry of papers analyzing its chain-of-thought outputs, its reward model design, and even its failures, which ultimately advances the field's understanding of LLM cognition.</p> </li> <li> <p>Acceleration of Open-Source AI: R1's impact on the open-source AI movement cannot be overstated. It demonstrated that an relatively small lab (DeepSeek-AI, a spin-off from a finance firm (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat)) could not only build a model competitive with those from tech giants, but also open-source it under a permissive license. This has encouraged other companies and organizations to open-source their strong models as well. Within months of R1's release, we saw Alibaba announce Qwen 2.5 Max (72B) and QwQ-32B, open-sourced under Apache 2.0, claiming performance on par with R1 (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). Likewise, Meta pushed forward with Llama 3.1 405B and made it available to the public (What Is Meta's Llama 3.1 405B? How It Works, Use Cases &amp; More). Each of these moves is a strategic response to R1 -- essentially a realization that open models can drive rapid adoption. DeepSeek R1 quickly became one of the most visited AI model websites globally (second only to OpenAI's), indicating enormous interest and community uptake (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). This user base in turn contributes back improvements, extensions (such as integrations into applications, or fine-tuned domain-specific versions), creating a positive feedback loop for the model's evolution. We also see governments and NGOs taking note: for instance, R1 being open means countries concerned about access to GPT-4 have an alternative they can control. Strategically, this could reduce reliance on a handful of AI providers and distribute AI capabilities more evenly worldwide.</p> </li> <li> <p>Competitive Pressure on Proprietary Models: Strategically, OpenAI, Google, and Anthropic now face a more level playing field. While they still hold some advantages (compute resources, multi-modal capabilities, proprietary data), the margin of AI superiority they enjoyed shrank because of models like R1. This has likely spurred them to invest in next-generation technologies -- e.g., focusing on multimodality (images, video, tools) where open models haven't yet caught up, or massively increasing context length (as Google did with a 2 million token context experiment) (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). It may also influence pricing and openness: OpenAI, for example, might eventually consider offering on-premise versions or weight licenses of their models to compete with the free availability of R1 (something that was previously unthinkable). We have already seen a \"model proliferation\" effect: many new startups releasing specialized LLMs (for reasoning, for dialogue, for coding) because R1 proved that with innovation, newcomers can beat established models in niches. Strategically, Big Tech can no longer assume their lead in LLMs will translate to usage monopoly -- the community can and will catch up. This dynamic could lead to faster progress overall, but also raises questions of safety: powerful models are now widely available, so ensuring responsible use becomes a distributed challenge. In response, initiatives for open model evaluation and alignment (some sponsored by governments) have ramped up, treating models like R1 as benchmarks for what open models can do.</p> </li> <li> <p>Ecosystem and Applications: DeepSeek R1 has enriched the AI ecosystem by being integrated into various platforms and pipelines. Its strong performance in retrieval-augmented generation (RAG) scenarios has been noted -- one guide calls it a way to \"supercharge RAG projects with DeepSeek R1\", thanks to its precise reasoning over retrieved documents (Supercharge RAG Projects with DeepSeek R1 AI Reasoning Model). This is steering application developers to consider using R1 for tasks like enterprise Q&amp;A, analytics, and decision support, often in place of closed models. The strategic consequence is that organizations can build sophisticated AI features without dependency on a single vendor. We also see R1's influence in education and research: universities use R1 as a teaching tool for AI courses (since it's inspectable), and researchers build on top of R1 for things like agent frameworks (knowing that R1 can handle long-horizon reasoning). Open-source community projects, from chatbots to coding assistants, now frequently include DeepSeek R1 or its distilled variants as a backbone. This broad adoption is shaping AI development priorities -- for example, there is increased interest in scaling laws for reinforcement learning (thanks to R1's approach) and in safety for open models (making sure models like R1 are as rigorously tested as closed ones).</p> </li> </ul> <p>In conclusion, DeepSeek R1's launch and open-source release have acted as a catalyst in the AI world: accelerating innovation, encouraging openness, and driving competitive and collaborative responses in equal measure. It has proven that cutting-edge AI need not be locked behind corporate doors, and in doing so, it has shifted strategic priorities -- from how models are trained (more focus on reasoning and RL) to how they are shared (greater openness) to how they are deployed (embracing on-premise and hybrid solutions). The \"DeepSeek effect\" -- a small team achieving big AI breakthroughs and sharing them -- is inspiring a new wave of AI development globally, ensuring that the race for ever more intelligent systems benefits from the collective efforts of the whole research community, not just a few large players (deepseek-ai/DeepSeek-R1 - Hugging Face). The end result is a more vibrant, accessible, and fast-evolving AI landscape, with DeepSeek R1 having carved its name as one of the pivotal contributions to this shift.</p>"},{"location":"concepts/deepseek-r1/#sources","title":"Sources","text":"<ul> <li>DeepSeek-AI (2025). DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning (arXiv preprint) (deepseek-ai/DeepSeek-R1 - Hugging Face) (deepseek-ai/DeepSeek-R1 - Hugging Face).</li> <li>DeepSeek R1 HuggingFace Model Card (deepseek-ai/DeepSeek-R1 - Hugging Face) (deepseek-ai/DeepSeek-R1 - Hugging Face) (deepseek-ai/DeepSeek-R1 - Hugging Face).</li> <li>VentureBeat (2025). Alibaba's QwQ-32B matches DeepSeek-R1 with way smaller compute (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat) (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat).</li> <li>SimilarWeb via VentureBeat -- R1 web traffic and model popularity (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat).</li> <li>DeepSeek R1 GitHub &amp; Ollama Library -- usage and licensing info (deepseek-r1) (deepseek-r1).</li> <li>OneClick AI Solutions (2025). DeepSeek R1 vs Qwen 2.5 Max -- benchmark comparison (DeepSeek R1 vs Qwen 2.5 Max: A Detailed Comparison of Features and Performance) (DeepSeek R1 vs Qwen 2.5 Max: A Detailed Comparison of Features and Performance).</li> <li>Azure AI Foundry Blog (2025). DeepSeek R1 on Azure -- enterprise adoption (DeepSeek R1 is now available on Azure AI Foundry and GitHub | Microsoft Azure Blog).</li> <li>LifeArchitect.ai Model Leaderboard (2025) -- MMLU/GPQA rankings.</li> <li>DeepSeek-V3 GitHub -- technical details on MoE architecture (GitHub - deepseek-ai/DeepSeek-V3).</li> <li>Meta AI (2024). Llama 3.1 405B announcement -- scale of open models (What Is Meta's Llama 3.1 405B? How It Works, Use Cases &amp; More).</li> </ul>"},{"location":"concepts/docstring/","title":"Python Docstring and Decorators","text":"<p>A docstring in Python is a string literal that appears as the first statement in a module, function, class, or method definition. It is used to document the purpose, behavior, parameters, and return values of the code object. Python docstrings are enclosed by triple quotes (<code>\"\"\"</code> or <code>'''</code>) and can span multiple lines.</p> <p>When working with AI agents (particularly in frameworks like LangChain or SmoLAgents), the <code>@tool</code> decorator or annotation near a docstring is a special decorator or marker that designates a function as being callable by an agent. This process typically works as follows:</p> <ol> <li> <p>Function Definition: Create a Python function with clearly defined inputs and outputs.</p> </li> <li> <p>Docstring Documentation: Write a comprehensive docstring that describes:</p> </li> <li>What the function does</li> <li>The parameters it accepts</li> <li>The return value and type</li> <li> <p>Any exceptions it might raise</p> </li> <li> <p>@tool Annotation: Include the <code>@tool</code> marker within the docstring or as a decorator above the function definition.</p> </li> <li> <p>Registration: The function is automatically registered in the agent's available toolset.</p> </li> <li> <p>Discovery and Usage: When the agent needs to perform a task related to the function's capability, it can discover and call this function based on the description in the docstring.</p> </li> </ol> <p>Example: <pre><code>def search_database(query: str) -&gt; list:\n    \"\"\"\n    @tool\n    Search the database for information matching the query string.\n\n    Args:\n        query: The search term to look for in the database\n\n    Returns:\n        A list of matching records from the database\n    \"\"\"\n    # Implementation details\n    results = db.execute_search(query)\n    return results\n</code></pre></p> <p>This approach allows agents to understand not just what functions are available, but when and how to use them appropriately based on their documented purpose and parameters.</p>"},{"location":"concepts/docstring/#decorators","title":"Decorators","text":"<p>A decorator is a design pattern in Python implemented as a special syntax that allows a function, method, or class to be modified or extended without changing its source code. </p> <p>Decorators use the <code>@</code> symbol followed by a decorator name placed above the definition of the function to be decorated. They effectively wrap the target function, enabling pre-processing of arguments, post-processing of return values, modification of behavior, or registration within a larger system.</p>"},{"location":"concepts/docstring/#decorators-in-smolagents-for-tool-identification","title":"Decorators in SmoLAgents for Tool Identification","text":"<p>In the SmoLAgents library, decorators are commonly used to identify and register functions as tools that can be discovered and used by agents. Here are examples of how decorators are used in this context:</p> <pre><code>from smolagents import tool\n\n@tool\ndef search_web(query: str) -&gt; list:\n    \"\"\"\n    Search the web for information related to the query.\n\n    Args:\n        query: The search terms to look for\n\n    Returns:\n        A list of search results\n    \"\"\"\n    # Implementation details\n    return web_search_implementation(query)\n</code></pre> <p>More specific decorator examples in SmoLAgents might include:</p> <pre><code>from smolagents import final_answer_tool\n\n@final_answer_tool\ndef provide_solution(answer: str) -&gt; str:\n    \"\"\"\n    Provide the final answer to the user's question.\n\n    Args:\n        answer: The complete solution to return to the user\n\n    Returns:\n        Confirmation message\n    \"\"\"\n    # Implementation details\n    return f\"Final answer recorded: {answer}\"\n</code></pre> <p>The SmoLAgents library might also support specialized tool decorators for different capabilities:</p> <pre><code>from smolagents import data_tool, code_tool, io_tool\n\n@data_tool\ndef analyze_dataset(file_path: str) -&gt; dict:\n    \"\"\"Process and analyze the dataset at the given path.\"\"\"\n    # Implementation\n\n@code_tool\ndef execute_python(code_snippet: str) -&gt; dict:\n    \"\"\"Execute the provided Python code snippet.\"\"\"\n    # Implementation\n\n@io_tool\ndef save_result(filename: str, content: str) -&gt; bool:\n    \"\"\"Save content to a file with the given filename.\"\"\"\n    # Implementation\n</code></pre> <p>These decorators serve multiple purposes: 1. Registering the function in the agent's available toolset 2. Providing metadata about the tool's purpose and usage patterns 3. Potentially applying input validation or output formatting 4. Enabling the agent to reason about which tool to use based on its current task</p>"},{"location":"concepts/knowledge-representation/","title":"Knowledge Representation","text":"<p>Knowledge Representation are the methods and structures used to encode information about the world.</p> <p>In this book, we focus on knowledge representation in a form that an agent can utilize for reasoning and decision-making. It provides the foundation for an agent's understanding of its environment, goals, and possible actions.</p> <p>Knowledge representation encompasses:</p> <ol> <li> <p>Symbolic Structures: Formalized frameworks such as ontologies, semantic networks, frames, rules, or logic-based systems that capture entities, relationships, and constraints in the agent's domain.</p> </li> <li> <p>Information Organization: Methods for structuring knowledge in ways that facilitate efficient access, inference, and updates as the agent interacts with its environment.</p> </li> <li> <p>Reasoning Mechanisms: Techniques that allow agents to derive new knowledge from existing representations, evaluate truth values, and make decisions based on incomplete information.</p> </li> <li> <p>Semantic Grounding: Connections between abstract symbols and their real-world referents, allowing the agent to map internal representations to external phenomena.</p> </li> </ol> <p>Effective knowledge representation systems in agents balance expressiveness (the ability to represent complex knowledge) with computational efficiency (allowing the agent to reason within reasonable time and resource constraints).</p>"},{"location":"concepts/knowledge-representation/#public-vs-private-knowledge","title":"Public vs Private Knowledge","text":""},{"location":"concepts/llm-control-levels/","title":"LLM Control Levels","text":"<p>Level 0 - No Control: LLMs function purely as information processors, generating outputs with no ability to influence program execution. All workflows and decisions are predetermined by human-written code.</p> <p>Level 1 - Decision Support: LLMs can influence conditional branches in existing workflows, essentially acting as a sophisticated decision-making component that determines which predefined path to take based on analysis.</p> <p>Level 2 - Function Selection: LLMs can select and call specific functions from a predefined tool catalog based on their understanding of the task. The agent chooses which tools to use but operates within a fixed set of capabilities.</p> <p>Level 3 - Flow Control: LLMs determine not just which functions to call but also the order, frequency, and conditions for termination. They control program flow, deciding when to loop, continue, or conclude a process.</p> <p>Level 4 - Workflow Initiation: LLMs can spawn additional agent workflows, effectively creating sub-agents to handle specialized tasks. This enables complex hierarchical problem-solving with task delegation.</p> <p>Level 5 - Code Generation: LLMs can write, execute, and evaluate original code in real-time, essentially programming themselves to solve novel problems beyond their predefined toolset, as seen in current IDEs like Cursor and Windsurf and SmolAgents' CodeAgent.</p>"},{"location":"concepts/project-rules/","title":"Project Rules for Intelligent Agents Course","text":"<p>These rules should be loaded into the LLM Projects area to provide context when we are generating content or code.</p>"},{"location":"concepts/project-rules/#definition-rule","title":"Definition Rule","text":"<p>When we ask for an ISO definition of a term or concept, always give use a term that follows the ISO 11179 metadata registry guidelines for terms.  These definitions should be:</p> <ol> <li>Precise - use words in the definition that have unambiguous meaning</li> <li>Concise - use the fewest words that will still provide a clear definition</li> <li>Distinct - check with the glossary.md file in the project to make sure the term is distinct from other terms</li> <li>Non-circular - do not place the term in the definition</li> <li>Unencumbered with business rules - do not places business rules within the definition</li> <li>Singular form - do not use plurals for terms</li> </ol> <p>After you define the term in 1 or 2 sentences, create a new paragraph and give some context of why this term is relevant to a course on intelligent software agents.  Then create a new paragraph that begins with **Example: ** and provide and example of usage within this context.</p> <p>Place the term in a level 4 markdown header.</p> <p>Example:</p>"},{"location":"concepts/project-rules/#graphics-processing-unit","title":"Graphics Processing Unit","text":"<p>A specialized device designed to accelerate matrix operations using parallel processing.</p> <p>In the context of intelligent software agents, GPUs are relevant as they provide parallel processing capabilities that significantly accelerate machine learning model training and inference operations, enabling more complex and performant AI systems. The parallel architecture of GPUs allows for efficient matrix operations that form the computational backbone of many neural network implementations.</p> <p>Example: When deploying a code agent for real-time coding tasks, a developer would typically configure the environment to utilize a GPU to ensure the agent can perform inference operations at sufficient speed, rather than relying solely on CPU processing.</p>"},{"location":"concepts/project-rules/#python-code-generation-rules","title":"Python Code Generation Rules","text":"<p>When generating Python code, always provide a brief comment at the beginning of the code that describes what the code should do.</p> <p>When creating a python function that could be used by an agent, make sure there is a docstring in the function that describes the name, description, input parameters and output type.</p> <p>Add the decorator <code>@tool</code> when generating a function that can be used by an agent.</p>"},{"location":"concepts/pydanticai/","title":"PydanticAI","text":"<p>PydanticAI is a Python agent framework designed to make it easy to build production grade applications with Generative AI.</p> <p>PydanticAI is a reaction to overly abstract frameworks such as LangChain that have long learning curves to manage the abstractions.</p>"},{"location":"concepts/pydanticai/#key-benefits","title":"Key Benefits","text":"<ol> <li> <p>Simplicity - designed to be easy to learn and easy to use with few abstractions</p> </li> <li> <p>Model Agnostic - you can use any LLM including local DeepSeek running on Ollama</p> </li> </ol>"},{"location":"concepts/pydanticai/#code-sample","title":"Code Sample","text":"<pre><code>from pydantic_ai import Agent\n\nagent = Agent(  \n    'google-gla:gemini-1.5-flash',\n    system_prompt='Be concise, reply with one sentence.',  \n)\n\nresult = agent.run_sync('Where does \"hello world\" come from?')  \nprint(result.data)\n\"\"\"\nThe first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\n\"\"\"\n</code></pre>"},{"location":"concepts/pydanticai/#references","title":"References","text":"<ul> <li>PydanticAI Web Site</li> </ul>"},{"location":"concepts/react-framework/","title":"Using the ReAct Framework for Textbook Generation","text":""},{"location":"concepts/react-framework/#overview","title":"Overview","text":"<p>This document demonstrates how to use the ReAct (Reasoning + Action) framework to generate a comprehensive AI textbook on \"Introduction to Large Language Models.\" The ReAct approach interleaves reasoning traces with concrete actions, allowing for a more dynamic and informed content creation process.</p>"},{"location":"concepts/react-framework/#implementation-example","title":"Implementation Example","text":"<pre><code>from smolagents import CodeAgent, FinalAnswerTool\nfrom smolagents.models import Model\nimport research_tools\nimport content_evaluation\nimport citation_manager\n\n# Initialize the model and tools\nmodel = Model(name=\"deepseek-r1:7b\")  # Using deepseek model as shown in documents\nagent = CodeAgent(\n    model=model,\n    tools=[\n        research_tools.SearchTool(),  # For finding relevant information\n        research_tools.SummarizeTool(),  # For condensing research\n        content_evaluation.AccuracyCheckerTool(),  # Verifies factual accuracy\n        content_evaluation.ReadabilityTool(),  # Ensures appropriate complexity level\n        citation_manager.CitationTool(),  # Manages references\n        FinalAnswerTool()  # Finalizes sections\n    ]\n)\n\ndef generate_textbook_chapter(topic, audience_level=\"undergraduate\"):\n    \"\"\"Generate a textbook chapter using the ReAct approach\"\"\"\n    prompt = f\"\"\"\n    Generate a textbook chapter on '{topic}' for {audience_level} students.\n    Follow these steps:\n    1. Research the topic thoroughly\n    2. Create an outline\n    3. Draft each section\n    4. Review for accuracy and readability\n    5. Add appropriate citations\n    6. Finalize the chapter\n\n    Use reasoning to plan your approach and actions to gather information and validate content.\n    \"\"\"\n\n    return agent.run(prompt)\n</code></pre>"},{"location":"concepts/react-framework/#react-process-breakdown","title":"ReAct Process Breakdown","text":"<p>Below is how the process unfolds for generating a chapter on \"Attention Mechanisms in Transformers\":</p>"},{"location":"concepts/react-framework/#1-initial-reasoning-trace","title":"1. Initial Reasoning Trace","text":"<pre><code>I need to create a comprehensive textbook chapter on attention mechanisms in Transformers. \nThis is a complex topic that requires clear explanations and examples.\nFirst, I should gather the latest research and fundamental concepts.\n</code></pre>"},{"location":"concepts/react-framework/#2-action-research","title":"2. Action: Research","text":"<pre><code>search_results = search_tool(\"attention mechanisms transformers technical explanation\")\nfoundational_papers = search_tool(\"Attention Is All You Need paper key concepts\")\nrecent_developments = search_tool(\"recent advances attention mechanisms 2023\")\n\n# Summarize the findings\ncore_concepts = summarize_tool(search_results + foundational_papers, \n                              focus=\"fundamental concepts for undergraduate level\")\n</code></pre>"},{"location":"concepts/react-framework/#3-reasoning-trace-for-outline","title":"3. Reasoning Trace for Outline","text":"<pre><code>Based on my research, I should structure this chapter to build knowledge progressively:\n- Start with the intuition behind attention\n- Move to mathematical formulation\n- Then cover different types of attention\n- Include practical examples\n- Discuss limitations and recent improvements\n</code></pre>"},{"location":"concepts/react-framework/#4-action-create-outline","title":"4. Action: Create Outline","text":"<pre><code>outline = {\n    \"title\": \"Attention Mechanisms in Transformer Models\",\n    \"sections\": [\n        {\n            \"title\": \"1. Introduction to Attention\",\n            \"subsections\": [\n                \"1.1 The Problem of Sequence Context\",\n                \"1.2 Intuition Behind Attention\"\n            ]\n        },\n        {\n            \"title\": \"2. Mathematics of Attention\",\n            \"subsections\": [\n                \"2.1 Queries, Keys, and Values\",\n                \"2.2 Scaled Dot-Product Attention\",\n                \"2.3 Multi-Head Attention\"\n            ]\n        },\n        # Additional sections...\n    ]\n}\n</code></pre>"},{"location":"concepts/react-framework/#5-reasoning-trace-for-content-creation","title":"5. Reasoning Trace for Content Creation","text":"<pre><code>For the introduction section, I need to make the concept accessible.\nI should use an analogy that undergraduate students can relate to.\nThe spotlight analogy works well for explaining how attention focuses on different parts of input.\n</code></pre>"},{"location":"concepts/react-framework/#6-action-draft-introduction","title":"6. Action: Draft Introduction","text":"<pre><code>introduction = \"\"\"\n# 1. Introduction to Attention\n\nImagine you're in a crowded room trying to follow a conversation. Despite many people talking simultaneously, you can focus on specific voices by directing your attention. Neural networks face a similar challenge when processing sequences like sentences - they need a mechanism to focus on relevant parts of the input. This is where attention mechanisms come in.\n\n## 1.1 The Problem of Sequence Context\n\nBefore attention mechanisms, sequence models like RNNs struggled with long-range dependencies...\n\"\"\"\n\n# Check readability\nreadability_score = readability_tool.analyze(introduction)\nif readability_score &lt; 0.7:  # Below target\n    introduction = readability_tool.improve(introduction, target_level=\"undergraduate\")\n</code></pre>"},{"location":"concepts/react-framework/#7-reasoning-trace-for-technical-content","title":"7. Reasoning Trace for Technical Content","text":"<pre><code>The mathematical section needs to balance rigor with accessibility.\nI should include the formulas but break them down step by step.\nVisual representations would help here - I'll include pseudocode and a diagram.\n</code></pre>"},{"location":"concepts/react-framework/#8-action-create-technical-section-with-verification","title":"8. Action: Create Technical Section with Verification","text":"<pre><code>math_section = \"\"\"\n# 2. Mathematics of Attention\n\n## 2.1 Queries, Keys, and Values\n\nThe attention mechanism revolves around three key concepts: queries, keys, and values...\n\"\"\"\n\n# Verify accuracy\naccuracy_issues = accuracy_checker_tool.verify(math_section, domain=\"machine learning\")\nif accuracy_issues:\n    for issue in accuracy_issues:\n        math_section = accuracy_checker_tool.suggest_correction(math_section, issue)\n</code></pre>"},{"location":"concepts/react-framework/#9-reasoning-trace-for-citations","title":"9. Reasoning Trace for Citations","text":"<pre><code>I need to properly cite the original Transformer paper and recent work.\nThe citations should follow academic standards and include DOI where available.\n</code></pre>"},{"location":"concepts/react-framework/#10-action-add-citations","title":"10. Action: Add Citations","text":"<pre><code>citations = citation_tool.generate([\n    {\"title\": \"Attention Is All You Need\", \"authors\": \"Vaswani et al.\", \"year\": 2017},\n    {\"title\": \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \n     \"authors\": \"Devlin et al.\", \"year\": 2019},\n    # Additional citations...\n])\n\nchapter_with_citations = citation_tool.insert_citations(full_chapter, citations)\n</code></pre>"},{"location":"concepts/react-framework/#11-final-reasoning-trace","title":"11. Final Reasoning Trace","text":"<pre><code>The chapter is nearly complete. I should review it once more for coherence and ensure\nthe progression of ideas flows naturally. The examples should reinforce the concepts.\n</code></pre>"},{"location":"concepts/react-framework/#12-action-finalize-content","title":"12. Action: Finalize Content","text":"<pre><code>final_chapter = final_answer_tool(\n    content=chapter_with_citations,\n    metadata={\n        \"title\": \"Attention Mechanisms in Transformer Models\",\n        \"audience\": \"undergraduate\",\n        \"prerequisites\": [\"Basic Neural Networks\", \"Sequence Models\"],\n        \"learning_objectives\": [\n            \"Understand the intuition behind attention mechanisms\",\n            \"Grasp the mathematical formulation of self-attention\",\n            \"Identify different types of attention and their applications\",\n            \"Implement a simple attention mechanism\"\n        ]\n    }\n)\n</code></pre>"},{"location":"concepts/react-framework/#benefits-of-the-react-approach-for-textbook-generation","title":"Benefits of the ReAct Approach for Textbook Generation","text":"<ol> <li> <p>Dynamic Research Integration: The ability to search for and incorporate the latest research ensures the textbook remains current and accurate.</p> </li> <li> <p>Quality Control: Interleaved reasoning and verification steps help catch errors and improve content quality throughout the creation process.</p> </li> <li> <p>Adaptive Content Creation: The framework can adjust explanations based on readability metrics, ensuring the content matches the intended audience level.</p> </li> <li> <p>Structured Development: The reasoning traces help maintain a coherent narrative and logical progression of ideas.</p> </li> <li> <p>Comprehensive Citations: Action steps for citation management ensure proper attribution and academic rigor.</p> </li> </ol>"},{"location":"concepts/react-framework/#implementation-considerations","title":"Implementation Considerations","text":"<ul> <li>Tool Selection: The specific tools integrated with the CodeAgent should match the subject matter of the textbook.</li> <li>Model Capabilities: More complex textbooks benefit from more capable foundation models like the deepseek-r1 model.</li> <li>Feedback Integration: The process can incorporate human expert feedback at key checkpoints.</li> <li>Multimodal Content: For subjects requiring diagrams or visualizations, additional tools for generating figures can be integrated.</li> </ul>"},{"location":"concepts/react-framework/#conclusion","title":"Conclusion","text":"<p>The ReAct framework provides a powerful approach to textbook generation by combining the reasoning capabilities of language models with the ability to take concrete actions like research, verification, and citation management. This interleaved process produces more comprehensive, accurate, and well-structured educational content than approaches that separate reasoning from action.</p>"},{"location":"concepts/similarity/","title":"Similarity: Connecting Concepts Through Shared Attributes","text":""},{"location":"concepts/similarity/#understanding-similarity","title":"Understanding Similarity","text":"<p>Similarity is a fundamental concept in AI and agents that describes the degree to which entities share attributes, properties, or characteristics. When we say two things are similar, we're recognizing patterns of commonality between them while acknowledging that they remain distinct entities. Similarity serves as a cognitive framework that helps us organize, categorize, and make sense of the world around us.</p> <p>At its core, similarity involves measuring the overlap of features or attributes between objects. These attributes can be physical properties (color, shape, size), functional capabilities, conceptual meanings, or structural patterns. The more attributes two entities share\u2014and the more significant those shared attributes are\u2014the greater their similarity.</p>"},{"location":"concepts/similarity/#lexical-similarity-words-and-synonyms","title":"Lexical Similarity: Words and Synonyms","text":"<p>The most straightforward example of similarity appears in language with words that share meanings. Synonyms are words that possess nearly identical meanings, though they often carry subtle differences in connotation, formality, or usage contexts.</p> <p>Consider these synonym sets: - happy, joyful, delighted, pleased, content - big, large, enormous, gigantic, massive - smart, intelligent, clever, bright, astute</p> <p>Each set contains words with similar core meanings, yet each word has its own nuances. \"Happy\" might suggest a general state of well-being, while \"delighted\" implies a more intense, momentary feeling. \"Massive\" suggests imposing size, while \"enormous\" emphasizes exceptional scale relative to normal expectations.</p> <p>Word similarity exists on a spectrum rather than in binary terms. Some words are near-perfect substitutes for each other in most contexts, while others share only partial semantic overlap.</p>"},{"location":"concepts/similarity/#sentence-and-paragraph-similarity","title":"Sentence and Paragraph Similarity","text":"<p>Similarity extends beyond individual words to larger linguistic structures like sentences and paragraphs. When comparing texts, similarity operates at multiple levels:</p> <ol> <li>Lexical similarity: The overlap of identical words</li> <li>Semantic similarity: The overlap of meaning, even with different words</li> <li>Structural similarity: Parallel grammatical or organizational patterns</li> <li>Functional similarity: Texts that serve similar purposes</li> </ol> <p>Consider these sentences: - \"The cat sat on the mat.\" - \"A feline rested on the floor covering.\"</p> <p>These sentences have low lexical similarity (they share few identical words) but high semantic similarity (they convey essentially the same meaning). Meanwhile, consider:</p> <ul> <li>\"The researchers discovered a new vaccine for the virus.\"</li> <li>\"The scientists invented a novel preventative treatment for the pathogen.\"</li> </ul> <p>These sentences demonstrate both semantic similarity and structural similarity, following parallel subject-verb-object patterns while using different words to express similar concepts.</p> <p>Paragraph similarity follows similar principles but encompasses broader patterns of organization, thematic development, and rhetorical strategies.</p>"},{"location":"concepts/similarity/#embeddings-and-semantic-distance","title":"Embeddings and Semantic Distance","text":"<p>To quantify and operationalize similarity, modern computational approaches often rely on embeddings\u2014dense vector representations that capture semantic properties of words, sentences, or documents in multi-dimensional space.</p>"},{"location":"concepts/similarity/#word-embeddings","title":"Word Embeddings","text":"<p>Word embeddings map words to vectors of real numbers, positioning semantically related words close together in vector space. Models like Word2Vec, GloVe, and FastText create these representations by analyzing patterns of word co-occurrence in large text corpora.</p> <p>In embedding space, similarity becomes a mathematical operation\u2014typically measured by cosine similarity or Euclidean distance between vectors. The smaller the distance between two word vectors, the more semantically similar they are.</p> <p>For example, in a well-trained embedding space: - The vector for \"king\" - \"man\" + \"woman\" produces a vector close to \"queen\" - The vector for \"Paris\" - \"France\" + \"Italy\" produces a vector close to \"Rome\"</p> <p>These operations demonstrate how embeddings capture meaningful semantic relationships.</p>"},{"location":"concepts/similarity/#sentence-and-document-embeddings","title":"Sentence and Document Embeddings","text":"<p>Advanced models like BERT, Sentence-BERT, and embeddings from large language models extend this approach to entire sentences and documents. These models create contextual embeddings that account for word order, syntactic structure, and complex semantic relationships.</p> <p>Document similarity then becomes a matter of measuring the distance between these high-dimensional vectors, enabling applications like semantic search, document clustering, and recommendation systems.</p>"},{"location":"concepts/similarity/#visualizing-semantic-spaces","title":"Visualizing Semantic Spaces","text":"<p>Figure 1: Word Embedding Visualization This figure shows a two-dimensional projection of word embeddings where semantically related words cluster together. The visualization would show distinct clusters for concepts like animals, foods, and countries, with similar concepts positioned closer to one another. Lines connect words with strong semantic relationships, revealing patterns like analogies and associations.</p> <p>Figure 2: Semantic Distance Map This figure illustrates the concept of semantic distance by displaying a heat map of similarity scores between pairs of sentences from a sample text. Darker colors indicate higher similarity. The diagonal shows perfect self-similarity (1.0), while off-diagonal elements reveal cross-sentence relationships, highlighting thematic patterns within the text.</p>"},{"location":"concepts/similarity/#graph-embeddings","title":"Graph Embeddings","text":"<p>While word and document embeddings capture semantic relationships in language, graph embeddings extend similar principles to structured relational data represented as networks.</p> <p>Graph embeddings map nodes (entities) and sometimes edges (relationships) in a graph to vector representations that preserve the graph's structural properties. The goal is to position nodes with similar connectivity patterns close together in the embedding space.</p> <p>Several approaches exist for creating graph embeddings:</p> <ol> <li>Random walk-based methods (like Node2Vec and DeepWalk) sample paths through the graph and apply word embedding techniques to the resulting sequences</li> <li>Matrix factorization methods decompose adjacency or Laplacian matrices of the graph</li> <li>Graph neural networks (GNNs) learn representations by aggregating and transforming neighborhood information</li> </ol>"},{"location":"concepts/similarity/#applications-in-knowledge-graphs","title":"Applications in Knowledge Graphs","text":"<p>Graph embeddings are particularly valuable for knowledge graphs\u2014structured representations of entities and their relationships that form the backbone of many intelligent systems.</p> <p>In a knowledge graph embedding, entities (nodes) and relations (edges) are represented as vectors in a shared space. This allows for:</p> <ol> <li>Link prediction: Identifying missing connections between entities</li> <li>Entity resolution: Recognizing when different references point to the same entity</li> <li>Similarity search: Finding entities with similar attributes or relationships</li> </ol> <p>Figure 3: Knowledge Graph Embedding This figure demonstrates how entities in a knowledge graph are mapped to a vector space. The left side shows a fragment of a knowledge graph with entities (people, places, organizations) connected by typed relationships. The right side displays how these entities are positioned in a reduced-dimension embedding space, where semantically related entities cluster together regardless of direct connections in the original graph.</p>"},{"location":"concepts/similarity/#vector-indices-for-knowledge-graph-properties","title":"Vector Indices for Knowledge Graph Properties","text":"<p>Building on the concept of embeddings, vector indices provide efficient data structures for organizing and retrieving vectors based on similarity. These indices are crucial for implementing practical applications that leverage similarity in large-scale systems.</p>"},{"location":"concepts/similarity/#vector-stores-and-access-control","title":"Vector Stores and Access Control","text":"<p>Modern vector stores must address not just similarity search but also security concerns. Role-based access control (RBAC) becomes essential when storing embeddings derived from sensitive documents or proprietary knowledge graphs.</p> <p>In practice, this means: 1. Associating access permissions with each vector in the store 2. Filtering search results based on the querying user's roles and permissions 3. Maintaining security boundaries even when computing similarity across document collections</p> <p>For example, a company's internal knowledge base might contain embeddings from various departments, with sensitive financial projections only accessible to executives, while general product documentation is available company-wide. The vector store must respect these boundaries during similarity search operations.</p>"},{"location":"concepts/similarity/#vector-indices-for-search-applications","title":"Vector Indices for Search Applications","text":"<p>Vector indices transform semantic search from a conceptual possibility to a practical tool. Modern search systems increasingly combine:</p> <ol> <li>Traditional keyword matching: Finding exact text patterns</li> <li>Vector similarity: Finding conceptually related content</li> <li>Structural awareness: Incorporating knowledge of document or graph structure</li> </ol> <p>This hybrid approach enables more intelligent search experiences that understand user intent beyond literal query terms.</p> <p>Figure 4: Hybrid Search Architecture This figure illustrates a modern search architecture that combines multiple similarity measures. The diagram shows how a user query is processed through parallel paths: keyword matching against an inverted index, semantic matching against a vector index, and structure-aware matching against a knowledge graph. The results are then combined using a ranking algorithm that weighs evidence from all three sources to present the most relevant results to the user.</p>"},{"location":"concepts/similarity/#applications-and-future-directions","title":"Applications and Future Directions","text":"<p>The concept of similarity, operationalized through embeddings and vector indices, enables numerous applications:</p> <ol> <li>Recommendation systems that suggest items similar to those a user has engaged with</li> <li>Question answering that finds text passages semantically similar to a query</li> <li>Clustering and classification that group similar items based on learned representations</li> <li>Anomaly detection that identifies items dissimilar from normal patterns</li> <li>Knowledge discovery that reveals non-obvious connections between entities</li> </ol> <p>As embedding techniques continue to advance, we can expect even more nuanced measures of similarity that account for contextual factors, domain-specific knowledge, and multimodal information spanning text, images, and structured data.</p> <p>The future of similarity-based systems will likely involve more sophisticated hybrid approaches that combine the strengths of different similarity models while addressing challenges related to computational efficiency, interpretability, and privacy-preserving similarity computation.</p>"},{"location":"concepts/workflows/","title":"Agentic Workflows","text":"<p>Here is a brief overview of the key workflows we use when building intelligent agents.</p>"},{"location":"concepts/workflows/#overview","title":"Overview","text":""},{"location":"concepts/workflows/#prompt-iteration","title":"Prompt Iteration","text":"<ul> <li>The LLM is trained on public knowledge but does not know anything about your goals, your products or your customers </li> <li>A human decides if the response is OK</li> <li>The human is \u201ctrained\u201d to write better prompts</li> </ul>"},{"location":"concepts/workflows/#prompt-enrichment-from-knowledge-base-rag","title":"Prompt Enrichment from Knowledge Base (RAG)","text":"<ul> <li>In this model, we enrich your prompt with content from a local knowledge based.</li> <li>The key to enrichment is to find the right content related to the question in your prompt.</li> <li>Enriching your prompt with local knowledge is much more cost effective than fine tuning your own language model.</li> </ul>"},{"location":"concepts/workflows/#prompt-enrichment-from-knowledge-graph","title":"Prompt Enrichment from Knowledge Graph","text":"<ul> <li>In this workflow we replace the generic knowledge based with a knowledge graph that has similarity indexes on reinvent components</li> <li>Knowledge graphs store models of the real world with better precision than tabular data such as CSV files or flat files with per-query relational JOIN operations</li> </ul>"},{"location":"concepts/workflows/#matching-a-developer-request-to-a-developer-tool","title":"Matching a Developer Request to a Developer Tool","text":"<p>If you are a software developer and you want an agent to help you the first task is to match your prompt to a tool.  So for example your prompt might be: \"move these new files into the sims folder\".  That would required the agent to use the unix \"mv\" command to move the files.</p> <p></p> <p>The bottom row of the workflow above is a list of all the tools that the IDE agent knows how to use and how to call them with the right parameters.</p>"},{"location":"concepts/workflows/#matching-a-developer-request-to-a-workflow","title":"Matching a Developer Request to a Workflow","text":"<p>We can take this matching process one step further to not just match a single tool, but a set of tools executing in a specific order.  For example you might need to first create a new directory and then move the files into that new directory.</p> <p></p> <p>Each of these \"actions\" will match not with a single command, but with a complex workflow of operations that can be executed and then tested for correctness.</p> <p>Remember that workflows are graphs!  So what we are really doing is finding the nearness of a prompt to a region of the graph that has similar values in a vector store.</p>"},{"location":"concepts/workflows/#the-role-of-context","title":"The Role of Context","text":"<p>The primary use of the knowledge graph is to analyze the current state of a task and then bring in the right sections of the knowledge graph that are relevant to the contest.</p> <p>We know that LLMs pay \"attention\" to the words around the last token in a sequence to predict the next token.  We need to apply the same attention systems to finding out what parts of the model of our world, the knowledge graph, is relevant to our task.</p> <p>We also need some way of measuring conceptual distance and knowing that we don't have ANY match and inform the user that no appropriate tools have been found.</p>"},{"location":"concepts/workflows/#the-reasoning-model","title":"The Reasoning Model","text":"<p>The way that we match these prompts to tools can be thought of as being done in steps.  The first step is to come up with a plan of action.  The second step is to execute those actions.</p>"},{"location":"concepts/workflows/#summary","title":"Summary","text":"<p>One of the best ways to summarize this is that agents work with LLMs and tools to match prompts to regions of a knowledge graph.  Once a plan is created it can be executed and the result of that execution can be tested.  After the test pass we consider the workflow complete.</p>"},{"location":"concepts/workflows/types-of-coding/","title":"Types of Coding","text":""},{"location":"concepts/workflows/types-of-coding/#procedural-code","title":"Procedural Code","text":"<p>Procedural code is a programming paradigm that focuses on defining a sequence of operations for a computer to execute. It emphasizes how a program should accomplish tasks through explicit step-by-step instructions, control flow structures, and state changes. Procedural code typically organizes functionality into procedures or functions that are called in a predetermined order.</p>"},{"location":"concepts/workflows/types-of-coding/#comparison-with-declarative-and-agentic-code","title":"Comparison with Declarative and Agentic Code","text":"Aspect Procedural Code Declarative Code Agentic Code Focus How to perform tasks (step-by-step instructions) What should be accomplished (desired outcome) Goals to achieve (with autonomous execution) --- --- --- --- Control Flow Explicitly defined by the programmer Implicit, determined by the execution environment Dynamic, determined by the agent based on goals and environment State Management Direct manipulation of program state Minimized state changes, often immutable Internal state that influences decision-making Abstraction Level Lower-level, focused on operations Higher-level, focused on relationships High-level, focused on objectives Examples Imperative languages like C, traditional Python scripts SQL, HTML, CSS, functional programming AI agents, automated planning systems Decision Making Predetermined by the programmer Rules-based evaluation Autonomous reasoning and adaptation Adaptability Requires explicit programming for different scenarios Adapts within defined constraints Can discover novel approaches to solve problems <p>Procedural code explicitly states the sequence of steps to follow, while declarative code describes the desired result without specifying the exact implementation. Agentic code, the newest paradigm, introduces autonomy and goal-oriented behavior, allowing the system to determine appropriate actions based on its understanding of objectives and environmental conditions.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This section describes the steps you need to take to build and test your own intelligent agents on your own local computer.  The ideal configuration is to have a local GPU with at least 12GB of RAM on the GPU.  Although you can still run agents on a local computer without a GPU, the response times might be so slow that you will become frustrated.  My local GPU is a NVIDIA RTX 2080 Ti which returns 52 tokens per second.</p> <p>You can test the tokens per second on your local computer using our lab on the Deep Learning Course.</p> <p>If you are not an expert on setting up Python, then setting up your local system can be VERY frustrating.</p>"},{"location":"getting-started/#picking-the-right-agent-library","title":"Picking the Right Agent Library","text":"<p>Picking the Right Library</p>"},{"location":"getting-started/#installing-gpu-drivers","title":"Installing GPU Drivers","text":"<p>GPU Driver Version</p>"},{"location":"getting-started/#conda-setup","title":"Conda Setup","text":"<p>Conda Setup</p>"},{"location":"getting-started/#testing-python-libraries","title":"Testing Python Libraries","text":"<p>Testing Libraries</p>"},{"location":"getting-started/#ollama-setup","title":"Ollama Setup","text":"<p>Ollama Setup</p>"},{"location":"getting-started/conda-setup/","title":"Conda Setup","text":""},{"location":"getting-started/conda-setup/#install-conda","title":"Install Conda","text":""},{"location":"getting-started/conda-setup/#creating-a-conda-python-environment","title":"Creating a Conda Python Environment","text":"<pre><code>conda create -n \"agents\" python=3\nconda activate agents\n# note we are using cuda 12.6\npip install --upgrade torch torchvision torchaudio \\\n    --index-url https://download.pytorch.org/whl/cu126\npip install --upgrade transformers accelerate bitsandbytes\n</code></pre> <p>Before you activate the conda environment your prompt will have the string 'base' in it like this:</p> <pre><code>(base) $\n</code></pre> <pre><code>conda activate agents\n(agents) $\n</code></pre> <p>You can get a full list of all the Python libraries by using the <code>pip freeze</code> command.</p> <pre><code>pip freeze\n</code></pre>"},{"location":"getting-started/gpu-drivers/","title":"Installing GPU Drivers","text":"<p>We need to find out the exact version of CUDA we have on our local GPU before we install Python libraries.  We can find this out by running the <code>nvcc</code> command at the UNIX shell.</p> <pre><code>nvcc --version\n</code></pre> <p>Response</p> <pre><code>$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2024 NVIDIA Corporation\nBuilt on Fri_Jun_14_16:34:21_PDT_2024\nCuda compilation tools, release 12.6, V12.6.20\nBuild cuda_12.6.r12.6/compiler.34431801_0\n</code></pre> <p>Note that the CUDA release is 12.6.  We will use this later in our installation process.</p>"},{"location":"getting-started/install-smolagents/","title":"Intalling SmolAgents","text":""},{"location":"getting-started/install-smolagents/#why-smolagents","title":"Why SmolAgents","text":"<p>AI agent frameworks are often criticized with two points:</p> <ol> <li>They build too many layers of abstraction, making them rigid and challenging to learn and use.</li> <li>They focus on \"workflows\" rather than building agents that can dynamically collaborate on their own using Python data structures.</li> </ol> <p>Smolagents has qualities that make ideal for simple agentic applications.</p> <ul> <li>The framework's abstractions are kept at a minimum.</li> <li>While most frameworks have the agents define their actions in JSON/text format, smolagents' main approach is <code>Code Agents</code> in which actions are written as Python code snippets (this is different from agents that write code).</li> <li>Being a Hugging Face framework, smolagents integrates well with the Hub and the Transformers library. You can use many models from the hub (some of them you can only use as a Pro user), and you can also work with proprietary models from OpenAI, Anthropic, etc.</li> <li>You can easily utilize the already-provided tools, or define your custom tools with minimum effort, almost as simple as writing a Python function.</li> </ul> <p>These qualities are what, on paper, make smolagents a plug-and-play with AI agents with little effort, so let's see if they hold in practice.</p>"},{"location":"getting-started/install-smolagents/#installing-smolagents","title":"Installing SmolAgents","text":"<p>Make sure you are in the correct environment.  In this class we used conda to create an environment called \"agents\"</p> <pre><code>conda activate agents\n</code></pre> <p>Next, use <code>pip</code> to install the smolagents Python library</p> <pre><code>pip install smolagents\npip install 'smolagents[litellm]'\n</code></pre> <p>The second line also includes the litellm module.</p>"},{"location":"getting-started/install-smolagents/#testing-smolagents","title":"Testing SmolAgents","text":""},{"location":"getting-started/install-smolagents/#basic-test","title":"Basic Test","text":"<pre><code>from smolagents import SmoLAgent\nprint(\"SmoLAgents installed successfully!\")\n</code></pre>"},{"location":"getting-started/install-smolagents/#test-with-version-and-dir","title":"Test with Version and Dir","text":"<p>We can also use the Python <code>dir</code> function on the smolagents module to get a detailed list of the items in the smolagents Python library.</p> <pre><code>import smolagents\n\n# Print version and available modules\nprint(f\"SmoLAgents version: {smolagents.__version__}\")\nprint(f\"Available in smolagents: {dir(smolagents)}\")\n\n# Try to create a \"TestAgent\"\ntry:\n    from smolagents.agents import Agent\n    agent = Agent(name=\"TestAgent\")\n    print(f\"Agent created successfully with name: {agent.name}\")\nexcept ImportError as e:\n    print(f\"Could not import Agent: {e}\")\n\nprint(\"SmoLAgents is installed!\")\n</code></pre> <pre><code>SmoLAgents version: 1.9.2\nAvailable in smolagents: ['AUTHORIZED_TYPES', 'ActionStep', 'AgentAudio', 'AgentError', 'AgentExecutionError', 'AgentGenerationError', 'AgentImage', 'AgentLogger', 'AgentMaxStepsError', 'AgentMemory', 'AgentParsingError', 'AgentText', 'AgentType', 'Any', 'AzureOpenAIServerModel', 'BASE_BUILTIN_MODULES', 'Callable', 'ChatMessage', 'CodeAgent', 'Dict', 'DuckDuckGoSearchTool', 'E2BExecutor', 'EMPTY_PROMPT_TEMPLATES', 'FinalAnswerPromptTemplate', 'FinalAnswerTool', 'Generator', 'GoogleSearchTool', 'GradioUI', 'Group', 'HfApiModel', 'List', 'LiteLLMModel', 'LocalPythonInterpreter', 'LogLevel', 'MLXModel', 'ManagedAgentPromptTemplate', 'MessageRole', 'Model', 'Monitor', 'MultiStepAgent', 'OpenAIServerModel', 'Optional', 'Panel', 'Path', 'PlanningPromptTemplate', 'PlanningStep', 'PromptTemplates', 'PythonInterpreterTool', 'Rule', 'Set', 'SpeechToTextTool', 'StrictUndefined', 'SystemPromptStep', 'TOOL_MAPPING', 'TaskStep', 'Template', 'Text', 'Tool', 'ToolCall', 'ToolCallingAgent', 'ToolCollection', 'TransformersModel', 'Tuple', 'TypedDict', 'Union', 'UserInputTool', 'VisitWebpageTool', 'YELLOW_HEX', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_function_type_hints_utils', 'agent_types', 'agents', 'argparse', 'cli', 'create_repo', 'default_tools', 'deque', 'e2b_executor', 'evaluate_python_code', 'fix_final_answer_code', 'getLogger', 'get_clean_message_list', 'get_variable_names', 'gradio_ui', 'handle_agent_output_types', 'importlib', 'inspect', 'jinja2', 'json', 'launch_gradio_demo', 'leopard_prompt', 'load_dotenv', 'load_model', 'load_tool', 'local_python_executor', 'logger', 'main', 'make_init_file', 'memory', 'metadata_update', 'models', 'monitoring', 'os', 'parse_arguments', 'parse_code_blobs', 'parse_json_tool_call', 'populate_template', 're', 'snapshot_download', 'stream_to_gradio', 'tempfile', 'textwrap', 'time', 'tool', 'tool_role_conversions', 'tool_validation', 'tools', 'truncate_content', 'upload_folder', 'utils', 'yaml']\n\nCould not import Agent: cannot import name 'Agent' from 'smolagents.agents' (/home/dan/miniconda3/envs/agents/lib/python3.13/site-packages/smolagents/agents.py)\n\nSmoLAgents is installed!\n</code></pre>"},{"location":"getting-started/install-smolagents/#creating-a-model","title":"Creating a Model","text":"<p>We can get a list of the agent types by looking for all method names with the string \"Agent\" it it.</p> <pre><code># test-smolagents-final.py\nimport smolagents\nfrom smolagents import FinalAnswerTool  # Import directly from smolagents\nfrom smolagents.models import Model\n\nprint(f\"SmoLAgents version: {smolagents.__version__}\")\n\ntry:\n    # Create a dummy model\n    dummy_model = Model(name=\"dummy-model\")\n\n    # Create a basic tool\n    final_answer_tool = FinalAnswerTool()\n\n    print(\"Successfully created tool and model!\")\n\n    # Print available agent types\n    print(\"Available agent types:\")\n    print([name for name in dir(smolagents) if 'Agent' in name])\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\nprint(\"SmoLAgents installation test complete!\")\n</code></pre> <pre><code>SmoLAgents version: 1.9.2\nSuccessfully created tool and model!\n\nAvailable agent types:\n['AgentAudio', 'AgentError', 'AgentExecutionError', 'AgentGenerationError', 'AgentImage', 'AgentLogger', 'AgentMaxStepsError', 'AgentMemory', 'AgentParsingError', 'AgentText', 'AgentType', 'CodeAgent', 'ManagedAgentPromptTemplate', 'MultiStepAgent', 'ToolCallingAgent']\n\nSmoLAgents installation test complete!\n</code></pre>"},{"location":"getting-started/install-smolagents/#references","title":"References","text":"<p>Build Fast with AI Video</p>"},{"location":"getting-started/ollama-python/","title":"Ollama Python","text":""},{"location":"getting-started/ollama-python/#calling-deepseek-r1-via-ollama","title":"Calling DeepSeek R1 via Ollama","text":"<pre><code>#!/usr/bin/env python3\n# working-code-agent.py\nimport smolagents\nfrom smolagents import FinalAnswerTool, CodeAgent\nimport requests\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass OllamaModel:\n    \"\"\"Simple Ollama model that ignores unsupported parameters\"\"\"\n\n    def __init__(self, model_name, base_url=\"http://localhost:11434\"):\n        self.model_name = model_name\n        self.base_url = base_url\n        self.generate_endpoint = f\"{base_url}/api/generate\"\n        logger.info(f\"Initialized OllamaModel with model: {model_name}\")\n\n    def __call__(self, prompt, **kwargs):\n        \"\"\"Generate response from Ollama model, ignore any kwargs\"\"\"\n        # Handle various prompt formats\n        if isinstance(prompt, list):\n            # Convert list to text\n            if len(prompt) &gt; 0 and isinstance(prompt[0], dict):\n                # Extract from messages format\n                full_prompt = \"\\n\".join([msg.get(\"content\", \"\") for msg in prompt if isinstance(msg, dict) and \"content\" in msg])\n            else:\n                # Join list items\n                full_prompt = \"\\n\".join([str(item) for item in prompt if item])\n        else:\n            # Use as is\n            full_prompt = str(prompt)\n\n        payload = {\n            \"model\": self.model_name,\n            \"prompt\": full_prompt,\n            \"stream\": False\n        }\n\n        try:\n            logger.info(f\"Sending request to Ollama with prompt length: {len(full_prompt)} chars\")\n            response = requests.post(self.generate_endpoint, json=payload)\n\n            if response.status_code == 200:\n                result = response.json()\n                return result.get(\"response\", \"\")\n            else:\n                logger.error(f\"Ollama request failed: {response.status_code}\")\n                return \"Error: Failed to get response from Ollama\"\n        except Exception as e:\n            logger.error(f\"Error: {e}\")\n            return f\"Error: {str(e)}\"\n\ndef main():\n    try:\n        # Create the model\n        model = OllamaModel(\"deepseek-r1\")\n\n        # Test direct model use\n        response = model(\"What is 2 + 2?\")\n        logger.info(f\"Direct model response: {response[:100]}...\")\n\n        # Create a simple task for the agent\n        task = \"Calculate the sum of numbers from 1 to 5\"\n\n        # Create the agent with minimal setup - REMOVED verbose parameter\n        agent = CodeAgent(\n            model=model,\n            tools=[FinalAnswerTool()]\n        )\n\n        # Run the agent\n        logger.info(f\"Running agent with task: {task}\")\n        result = agent.run(task)\n\n        logger.info(f\"Agent result: {result}\")\n\n        # If that works, try a more complex task\n        complex_task = \"\"\"\n        I need to solve this problem:\n        1. Calculate the area of a rectangle with width 6 meters and length 9 meters\n        2. Then calculate what percentage of that area would be covered by a square with side length 4 meters\n        \"\"\"\n\n        logger.info(f\"\\nRunning agent with complex task...\")\n        complex_result = agent.run(complex_task)\n\n        logger.info(f\"Complex task result: {complex_result}\")\n\n    except Exception as e:\n        logger.error(f\"Error: {e}\")\n        import traceback\n        logger.error(traceback.format_exc())\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"getting-started/ollama-setup/","title":"Ollama Deepseek Setup","text":"<p>This is an example of installing Ollama and the DeepSeek R1 7b model on my local system that has a NVIDIA RTX 2070 GPU with 12GB RAM.</p>"},{"location":"getting-started/ollama-setup/#install-ollama","title":"Install Ollama","text":"<pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre>"},{"location":"getting-started/ollama-setup/#test-ollama","title":"Test Ollama","text":"<pre><code>ollama --version\n</code></pre> <p>Response:</p> <pre><code>ollama version is 0.3.6\n</code></pre>"},{"location":"getting-started/ollama-setup/#setup-ollama-as-a-unix-service","title":"Setup Ollama as a UNIX service","text":"<pre><code>sudo systemctl edit ollama.service\n</code></pre> <p>Add the following lines:</p> <pre><code>[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0\"\nEnvironment=\"OLLAMA_ORIGINS=*\"\n</code></pre>"},{"location":"getting-started/ollama-setup/#test-the-ollama-rest-service","title":"Test the Ollama REST Service","text":"<pre><code>curl http://localhost:11434/api/version\n</code></pre> <p>Result:</p> <pre><code>{\"version\":\"0.3.6\"}\n</code></pre>"},{"location":"getting-started/ollama-setup/#ollama-shell-commands","title":"Ollama Shell Commands","text":"<pre><code>ollama --help\n</code></pre> <pre><code>Large language model runner\n\nUsage:\n  ollama [flags]\n  ollama [command]\n\nAvailable Commands:\n  serve       Start ollama\n  create      Create a model from a Modelfile\n  show        Show information for a model\n  run         Run a model\n  pull        Pull a model from a registry\n  push        Push a model to a registry\n  list        List models\n  ps          List running models\n  cp          Copy a model\n  rm          Remove a model\n  help        Help about any command\n\nFlags:\n  -h, --help      help for ollama\n  -v, --version   Show version information\n\nUse \"ollama [command] --help\" for more information about a command.\n</code></pre>"},{"location":"getting-started/ollama-setup/#download-deepseek-r1","title":"Download DeepSeek-r1","text":"<p>You can download the full 7b model by adding the keyword <code>pull</code> and the model name to ollama.</p> <pre><code>ollama pull deepseek-r1:7b\n</code></pre> <p></p>"},{"location":"getting-started/ollama-setup/#test-that-deepseek-r1-7b-is-working","title":"Test that Deepseek-r1 7B is working","text":"<pre><code>ollama run deepseek-r1:7b\n&gt;&gt;&gt; /show info\n  Model                          \n    arch                qwen2            \n    parameters          7.6B             \n    quantization        Q4_K_M           \n    context length      131072           \n    embedding length    3584             \n\n  Parameters                     \n    stop    \"&lt;\uff5cbegin\u2581of\u2581sentence\uff5c&gt;\"   \n    stop    \"&lt;\uff5cend\u2581of\u2581sentence\uff5c&gt;\"     \n    stop    \"&lt;\uff5cUser\uff5c&gt;\"                \n    stop    \"&lt;\uff5cAssistant\uff5c&gt;\"           \n\n  License                        \n    MIT License                     \n    Copyright (c) 2023 DeepSeek     \n\n&gt;&gt;&gt; what are the 10 most difficult concepts in high-school geometry to understand?\n</code></pre>"},{"location":"getting-started/ollama-setup/#deepseek-response","title":"DeepSeek Response","text":""},{"location":"getting-started/ollama-setup/#thinking","title":"Thinking","text":"<p> Okay, so I need to figure out what the top 10 most difficult concepts in high school geometry are. Hmm, where do I start? I've taken geometry before, but it's been a while, so some of these might have faded from my  memory. Let me think step by step. <p>First off, when I was learning geometry, one topic that always confused me was proofs. There were so many theorems and properties to remember, and constructing a logical argument seemed like a whole other language. I can't even recall exactly what an auxiliary line is, but I know it's used in proofs somehow.</p> <p>Then there are circles. I remember they have circumference, area, radius, diameter, and all those formulas, but when you start talking about angles subtended by chords or tangents, things got really tangled for me. It was hard to keep track of the different theorems related to circles, like the inscribed angle theorem or  something.</p> <p>Triangles are another big topic in geometry. There are so many types\u2014scalene, isosceles, equilateral\u2014and each has its own properties. Then there's Pythagoras' theorem for right-angled triangles, which I sometimes mixed  up with other formulas because they all looked similar. Also, triangle congruence and similarity were confusing at times; remembering SAS and ASA rules was a struggle.</p> <p>Coordinate geometry also tripped me up. It combines algebra and geometry, so you have to visualize shapes on the coordinate plane and use equations to find distances, slopes, and intersections. I remember struggling with finding the distance between two points using the distance formula and calculating midpoints or perpendicular lines.</p> <p>Transformations like translations, rotations, reflections, and dilations were another area where I had trouble. Understanding how figures change position or size on the coordinate plane was confusing without a clear visual. Also, similarity transformations confused me because they involved scaling, which adds another  layer of complexity.</p> <p>Trigonometry in geometry felt separate from what I had learned before. Learning sine, cosine, and tangent functions to find side lengths and angles in right triangles was initially tough. Then, when we moved on to  the unit circle, everything shifted, and it became even more abstract. I had trouble understanding how trigonometric ratios extended beyond right triangles.</p> <p>Three-dimensional geometry confused me a lot. Moving from 2D shapes to 3D solids like prisms, pyramids, cylinders, cones, and spheres introduced new concepts and formulas. Calculating surface area and volume required knowing which formula to apply when, and I often mixed them up or used the wrong dimensions in  calculations.</p> <p>Parallelograms had their own set of properties that needed to be memorized for proofs. Proving whether a quadrilateral was a parallelogram based on given information was tricky because there were multiple ways to  show it, like using opposite sides equal or parallel lines and angles.</p> <p>Probability in geometry was another point where I struggled. It wasn't just basic probability; integrating geometric principles made the problems more complex. Calculating probabilities based on areas or lengths required a good understanding of both geometry and probability concepts.</p> <p>Lastly, constructions with a compass and straightedge were a nightmare. It involved precise steps without any measurements, which felt restrictive. Forgetting a step in a construction could lead to an incorrect figure, and figuring out the right sequence without trial and error was frustrating.</p> <p>Wait, did I miss anything? Let me think... Maybe locus problems where you have to find points satisfying certain conditions. Or maybe transformations again; they were mentioned before but perhaps not explicitly listed as top 10. Hmm, since it's about difficulty, maybe those are already covered.</p> <p>I should also consider if the order makes sense. Proofs seem foundational but can be really challenging because they require logical reasoning. Then coordinate geometry and constructions build on previous knowledge with algebra and precise drawing skills. Trigonometry and circles add layers of complexity with their formulas and applications. Three-dimensional geometry might come last as it's more abstract.</p> <p>Yeah, I think that covers the main areas where students usually struggle in high school geometry. Each topic has its own challenges, but some require a strong foundation in other areas like algebra or logical reasoning. </p>"},{"location":"getting-started/ollama-setup/#answer","title":"Answer","text":"<p>The top 10 most difficult concepts in high school geometry can be categorized based on their complexity and the skills they require:</p> <ol> <li> <p>Geometric Proofs: Constructing logical arguments using axioms and previously proven theorems is challenging due to its abstract nature.</p> </li> <li> <p>Circle Theorems: Understanding properties of circles, such as angles subtended by chords, tangents, and arcs requires memorization and application of multiple theorems.</p> </li> <li> <p>Triangle Properties and Congruence/Similarity: Distinguishing between different types of triangles and applying congruence/similarity rules (e.g., SAS, ASA) can be confusing.</p> </li> <li> <p>Coordinate Geometry: Combining algebra with geometry to find distances, slopes, and intersections on the coordinate plane is complex.</p> </li> <li> <p>Transformations: Grasping concepts like translations, rotations, reflections, and dilations, especially their effects on figures in the coordinate plane, is challenging.</p> </li> <li> <p>Trigonometry: Extending trigonometric ratios beyond right triangles using the unit circle abstracts the subject and complicates problem-solving.</p> </li> <li> <p>Three-Dimensional Geometry: Calculating surface areas and volumes of 3D solids like prisms and pyramids requires applying specific formulas with attention to detail.</p> </li> <li> <p>Parallelogram Properties: Proving properties of parallelograms based on given information involves multiple proof strategies.</p> </li> <li> <p>Geometric Probability: Integrating probability concepts with geometric principles, such as area-based calculations, adds complexity.</p> </li> <li> <p>Compass and Straightedge Constructions: Performing precise constructions without measurement tools demands careful step-by-step execution.</p> </li> </ol> <p>Each concept builds upon previous knowledge, requiring a blend of logical reasoning, algebraic skills, and spatial understanding. Mastery often necessitates practice and a solid grasp of foundational topics.</p>"},{"location":"getting-started/ollama-setup/#other-ollama-commands","title":"Other Ollama Commands","text":"<p>/help Available Commands:   /set            Set session variables   /show           Show model information   /load    Load a session or model   /save    Save your current session   /clear          Clear session context   /bye            Exit   /?, /help       Help for a command   /? shortcuts    Help for keyboard shortcuts <p>Use \"\"\" to begin a multi-line message.</p>"},{"location":"getting-started/picking-agent-library/","title":"Selecting Agent Libraries","text":"<p>As of February of 2025, there are three AI Agent Libraries that are popular: </p> <ol> <li>LangChain - the first popular agent library</li> <li>LlamaIndex - a more recent library that fixed many problems with LangChain</li> <li>[SmoLAgents] - a library created by HuggingFace to work with small models on your local GPU</li> </ol> <p>Let's compare these three AI agent libraries based on their features, strengths, and limitations:</p>"},{"location":"getting-started/picking-agent-library/#langchain","title":"LangChain","text":""},{"location":"getting-started/picking-agent-library/#pros","title":"Pros","text":"<ul> <li>Comprehensive framework for building LLM-powered applications</li> <li>Strong focus on chains and composability of components</li> <li>Extensive tool and service integrations (databases, APIs, etc.)</li> <li>Robust memory and state management capabilities</li> <li>Large community with abundant documentation and examples</li> <li>Flexible architecture allowing for customization</li> <li>Well-established ecosystem with many extensions</li> </ul>"},{"location":"getting-started/picking-agent-library/#cons","title":"Cons","text":"<ul> <li>Can be complex for beginners due to its extensive feature set</li> <li>Sometimes considered overly modular, leading to complexity</li> <li>Rapid development pace can lead to breaking changes</li> <li>Higher computational requirements for some components</li> <li>Can be overkill for simpler applications</li> </ul>"},{"location":"getting-started/picking-agent-library/#llamaindex","title":"LlamaIndex","text":""},{"location":"getting-started/picking-agent-library/#pros_1","title":"Pros","text":"<ul> <li>Specialized in data indexing and retrieval for LLMs</li> <li>Excellent for RAG (Retrieval-Augmented Generation) applications</li> <li>Strong document processing capabilities</li> <li>Efficient handling of various data sources and formats</li> <li>Good performance with large document collections</li> <li>Focused architecture making it easier to learn for specific use cases</li> <li>Growing integration ecosystem</li> </ul>"},{"location":"getting-started/picking-agent-library/#cons_1","title":"Cons","text":"<ul> <li>More focused on data retrieval than end-to-end agent capabilities</li> <li>Less comprehensive agent framework compared to LangChain</li> <li>May require additional libraries for complete agent implementations</li> <li>Fewer built-in tools for complex reasoning chains</li> <li>Still evolving, with some features in experimental stages</li> </ul>"},{"location":"getting-started/picking-agent-library/#smolagents","title":"SmoLAgents","text":""},{"location":"getting-started/picking-agent-library/#pros_2","title":"Pros","text":"<ul> <li>Focuses on small language model agents, making it more efficient for certain tasks</li> <li>Designed for resource-constrained environments such as a local GPU</li> <li>Lower computational requirements compared to frameworks requiring larger models</li> <li>Good for edge devices or applications with limited processing power</li> <li>Emphasizes agent simplicity and minimalism</li> </ul>"},{"location":"getting-started/picking-agent-library/#cons_2","title":"Cons","text":"<ul> <li>Less comprehensive feature set than more established frameworks</li> <li>Smaller community and ecosystem</li> <li>More limited documentation and tutorials</li> <li>Fewer integrations with external tools and services</li> <li>May not handle extremely complex reasoning tasks as effectively as libraries using larger models</li> </ul>"},{"location":"getting-started/picking-agent-library/#summary","title":"Summary","text":"<ul> <li>SmoLAgents is best for lightweight applications with limited resources where efficiency is critical.</li> <li>LangChain excels as a comprehensive framework for complex, feature-rich AI applications with extensive integration needs.</li> <li>LlamaIndex shines for data-intensive applications where efficient document retrieval and processing are the primary concerns.</li> </ul> <p>Your choice should depend on your specific use case, resource constraints, and whether you need a complete agent framework or specialized functionality for particular aspects of AI application development.</p>"},{"location":"getting-started/smolagents-ollama/","title":"Testing SmolAgents with Ollama","text":"<pre><code># minimal-smolagents-ollama.py\nimport smolagents\nfrom smolagents import FinalAnswerTool\nimport requests\nimport logging\nimport time\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass SimplerOllamaModel:\n    \"\"\"Simplified Ollama model adapter for smolagents\"\"\"\n\n    def __init__(self, model_name, base_url=\"http://localhost:11434\"):\n        self.model_name = model_name\n        self.base_url = base_url\n        self.generate_endpoint = f\"{base_url}/api/generate\"\n        logger.info(f\"Initialized SimplerOllamaModel with model: {model_name}\")\n\n    def __call__(self, prompt, **kwargs):\n        \"\"\"Handle both direct prompts and formatted messages\"\"\"\n        # Process the prompt to handle different formats\n        processed_prompt = self._process_prompt(prompt)\n\n        # Prepare the request payload\n        payload = {\n            \"model\": self.model_name,\n            \"prompt\": processed_prompt,\n            \"stream\": False\n        }\n\n        # Send request to Ollama\n        try:\n            logger.info(f\"Sending request to Ollama, prompt length: {len(processed_prompt)} chars\")\n            response = requests.post(self.generate_endpoint, json=payload)\n\n            if response.status_code == 200:\n                result = response.json()\n                return result.get(\"response\", \"\")\n            else:\n                logger.error(f\"Ollama request failed: {response.status_code}\")\n                return f\"Error: Ollama request failed with status {response.status_code}\"\n        except Exception as e:\n            logger.error(f\"Error: {e}\")\n            return f\"Error: {str(e)}\"\n\n    def _process_prompt(self, prompt):\n        \"\"\"Convert various prompt formats to a string\"\"\"\n        if prompt is None:\n            return \"\"\n\n        if isinstance(prompt, str):\n            return prompt\n\n        if isinstance(prompt, list):\n            # If it's a list of messages (dicts with role and content)\n            if len(prompt) &gt; 0 and isinstance(prompt[0], dict):\n                result = \"\"\n                for msg in prompt:\n                    if isinstance(msg, dict) and \"content\" in msg:\n                        role = msg.get(\"role\", \"\")\n                        content = msg.get(\"content\", \"\")\n                        if role and content:\n                            result += f\"{role.upper()}: {content}\\n\\n\"\n                        else:\n                            result += f\"{content}\\n\\n\"\n                return result.strip()\n\n            # If it's a list of strings\n            return \"\\n\".join([str(item) for item in prompt if item])\n\n        # Default case - convert to string\n        return str(prompt)\n\ndef use_direct_ollama():\n    \"\"\"Use Ollama directly without smolagents\"\"\"\n    try:\n        # Create model\n        model = SimplerOllamaModel(\"deepseek-r1\")\n\n        # Test with simple prompt\n        result = model(\"What is 2 + 2? Explain step by step.\")\n        logger.info(f\"Direct Ollama result: {result[:200]}...\")\n\n        # Test with ChatML format\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"What is the sum of numbers from 1 to 5?\"}\n        ]\n\n        result2 = model(messages)\n        logger.info(f\"Messages format result: {result2[:200]}...\")\n\n        return True\n    except Exception as e:\n        logger.error(f\"Direct Ollama test failed: {e}\")\n        return False\n\ndef create_custom_agent():\n    \"\"\"Create a custom agent that uses Ollama without relying on smolagents classes\"\"\"\n    try:\n        # Create our Ollama model\n        model = SimplerOllamaModel(\"deepseek-r1\")\n\n        # Define a simple agent function\n        def simple_agent(prompt):\n            # Format the prompt to encourage step-by-step thinking\n            enhanced_prompt = f\"\"\"\nPlease solve this problem step by step:\n\n{prompt}\n\nThink through this problem carefully. First, understand what is being asked.\nThen break it down into steps and solve each step.\nFinally, provide your answer.\n\"\"\"\n            # Get response from model\n            response = model(enhanced_prompt)\n\n            # Return the result\n            return response\n\n        # Test the simple agent\n        task = \"Calculate the sum of numbers from 1 to 5.\"\n        logger.info(f\"Running simple agent with task: {task}\")\n\n        result = simple_agent(task)\n        logger.info(f\"Simple agent result: {result[:200]}...\")\n\n        # Test with complex task\n        complex_task = \"\"\"\nI need to solve this problem:\n1. Calculate the area of a rectangle with width 6 meters and length 9 meters\n2. Then calculate what percentage of that area would be covered by a square with side length 4 meters\n\"\"\"\n        logger.info(f\"Running simple agent with complex task\")\n        complex_result = simple_agent(complex_task)\n        logger.info(f\"Complex task result: {complex_result[:200]}...\")\n\n        return True\n    except Exception as e:\n        logger.error(f\"Custom agent test failed: {e}\")\n        return False\n\ndef main():\n    logger.info(\"Starting Ollama integration test...\")\n\n    # First, test direct Ollama usage\n    if use_direct_ollama():\n        logger.info(\"Direct Ollama test successful!\")\n\n        # Then, test our custom agent\n        if create_custom_agent():\n            logger.info(\"Custom agent test successful!\")\n        else:\n            logger.warning(\"Custom agent test failed.\")\n    else:\n        logger.error(\"Direct Ollama test failed. Cannot proceed.\")\n\n    logger.info(\"All tests completed.\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"getting-started/testing-libraries/","title":"Testing Supporting Libraries","text":""},{"location":"getting-started/testing-libraries/#python-library-version-test","title":"Python Library Version Test","text":"<pre><code>import torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\n</code></pre>"},{"location":"getting-started/testing-libraries/#sample-response","title":"Sample Response","text":"<pre><code>PyTorch version: 2.6.0+cu126\nCUDA available: True\nCUDA version: 12.6\n</code></pre>"},{"location":"prompts/concept-classification/","title":"Concept Classification","text":"<p>Prompt</p> <p>Now please generate a new CSV file based on the existing concept-dependency.csv file. Your job will be to just add one new column to this new CSV file called the TaxonomyID.  In this column, put the number from the taxonomy that is the closet match to that taxonomy.md file in the project knowledge. Some concepts might match more than one TaxonomyID.  Just put the best match.</p> <p>The new format is:</p> <ul> <li>Column 1 is the same ConceptID from the concept dependency list</li> <li>Column 2 is the same ConceptLabel from the concept dependency list</li> <li>Column 3 is the Dependencies.  This is the same column from concept dependency list</li> <li>Column 4 is the new TaxonomyID.  It matches the number in the taxonomy</li> </ul> <p>Here is a summary of the TaxonomyID and the Classifier Name</p> <ol> <li>Programming Fundamentals</li> <li>Agent Theory</li> <li>Knowledge Representation</li> <li>Tool Engineering</li> <li>LLM Implementation</li> <li>Security &amp; Safety Framework</li> <li>Performance &amp; Optimization</li> <li>Knowledge Integration &amp; Reasoning</li> <li>User Interaction &amp; Experience</li> <li>Agent Development</li> <li>Midterm and Capstone Projects Ideas</li> <li>Miscellaneous</li> </ol>"},{"location":"prompts/concept-dependency/","title":"Concept Dependency","text":"<p>Prompt</p> <p>Please create a fully connected concept dependency graph for the concepts in this course. In this step, for each Concept on this list below, create a new list of other concepts on which it depends.  If a concept depends on a specific concept that is not listed, then we will need to add it as a new concept. This will help us determine the teaching order of the Concepts.</p> <p>Return the new list as a CSV file using the following format:</p> <ol> <li>The first column in the CSV file is a numeric concept ID you assigned in the list above.</li> <li>The second column is the concept name.</li> <li>The third column is a pipe-delimited list of the Concept IDs that the concept depends upon.</li> </ol> <p>Check your results to make sure that all concepts are connected to at least one other concept.</p> <p>Foundation Concepts are concepts that have no dependant concepts in this set. They have an empty third column.</p> <p>Here is a numbered list of 150 core concepts for my course, presented in the order they should be taught, from foundational to advanced:</p>"},{"location":"prompts/concept-dependency/#results","title":"Results","text":"<p>I'll create a concept dependency graph for this course based on the information provided. First, let me analyze the conceptlist.csv file to understand the concepts mentioned in the course.</p> <p>Now I'll create a concept dependency graph based on the conceptlist.csv file. I'll analyze the relationships between concepts and determine which concepts depend on others to create a directed acyclic graph.</p> <p>I've created a comprehensive concept dependency graph for the Intelligent Software Agents course based on the conceptlist.csv file provided. The graph is structured as a CSV file with three columns:</p> <ol> <li>ConceptID: The numeric identifier from the original CSV file</li> <li>ConceptLabel: The name of the concept</li> <li>Dependencies: A pipe-delimited list of ConceptIDs that this concept depends upon</li> </ol>"},{"location":"prompts/concept-dependency/#key-observations","title":"Key Observations:","text":""},{"location":"prompts/concept-dependency/#foundation-concepts-prerequisites-with-no-dependencies","title":"Foundation Concepts (Prerequisites with no dependencies):","text":"<ul> <li>Python Programming (#1)</li> <li>Version Control (#4)</li> <li>Integrated Development Environment (#6)</li> <li>Command Line Interface (#8)</li> <li>Knowledge Representation (#11)</li> <li>Large Language Models (#17)</li> <li>Graphics Processing Unit (#21)</li> <li>Matrix Operations (#78)</li> <li>Parallel Processing (#79)</li> <li>Private Knowledge (#34)</li> <li>ISO 11179 Definitions (#49)</li> <li>Code Security (#52)</li> <li>Business Process Modeling Notation (#92)</li> <li>Software Development Life Cycle (#94)</li> <li>Rule-Based Systems (#99)</li> <li>Explainable AI (#111)</li> <li>Fluid Interactions (#113)</li> <li>Project Planning (#164)</li> </ul>"},{"location":"prompts/concept-dependency/#highly-dependent-concepts","title":"Highly Dependent Concepts:","text":"<p>Concepts that have numerous dependencies include: - Multi-Agent Systems (#36) - depends on agent collaboration, communication, coordination - Knowledge Graph Construction (#184) - depends on entity relationships and properties - Interpretable Agent Behavior (#189) - depends on transparency and explainability concepts</p>"},{"location":"prompts/concept-dependency/#concept-clusters","title":"Concept Clusters:","text":"<p>The dependencies reveal several natural clusters of related concepts: 1. Programming Fundamentals: Python, data types, structures, docstrings 2. Agent Theory: Software agents, agency levels, autonomy 3. Knowledge Representation: Knowledge graphs, ontologies, semantic networks 4. Tool Engineering: Tool calling, catalogs, documentation 5. LLM Implementation: Model architectures, inference, deployment 6. Agent Development: Frameworks, integration, deployment strategies</p>"},{"location":"prompts/concept-dependency/#teaching-sequence-implications","title":"Teaching Sequence Implications:","text":"<p>This dependency graph suggests a natural progression for the course:</p> <ol> <li>Begin with foundational concepts like Python programming, version control, and development environments</li> <li>Introduce LLM basics and knowledge representation fundamentals</li> <li>Move to tool design and integration concepts</li> <li>Progress to basic agent frameworks and capabilities</li> <li>Advance to specialized agents (code agents, multi-agent systems)</li> <li>Culminate with complex integration patterns and deployment strategies</li> </ol> <p>The dependencies ensure that students learn prerequisites before advancing to more complex topics, creating a logical learning path from basic programming to sophisticated agent systems.</p>"},{"location":"prompts/concept-enumeration/","title":"Concept Enumeration","text":"<p>Prompt</p> <p>You are an expert at generating Concept Labels for learning dependency graphs for courses. </p> <p>I am writing a book on how to create intelligent software agents that use both LLMs and knowledge graphs.  There is a course-description.md file in the Project Knoweldge</p> <p>I want you to generate a list of the 200 most important Concept Labels in this course.</p> <p>Just return a simple ordered list of the in 200 most important concepts in markdown with no other content.  Use title case for each Concept Label.</p> <p>Use abbreviations when the labels get over 30 characters long.</p> <p>Return the list in the order the concepts should be learned.  A concept can only be added if the prerequisite concepts are already listed.</p> <p>Use the course-description.md file to find the prerequsites and list them as the first concepts in the list.</p>"},{"location":"prompts/concept-enumeration/#response","title":"Response","text":"<p>I'll create a list of the 200 most important concept labels for a course on intelligent software agents with LLMs and knowledge graphs, based on the documentation provided. The list will be in learning order, with prerequisites first.</p>"},{"location":"prompts/concept-enumeration/#intelligent-software-agents-concept-labels","title":"Intelligent Software Agents Concept Labels","text":"<ol> <li>Python Programming</li> <li>Data Types</li> <li>Data Structures</li> <li>Version Control</li> <li>GitHub</li> <li>Integrated Development Environment</li> <li>Visual Studio Code</li> <li>Command Line Interface</li> <li>Software Agents</li> <li>Agency Levels</li> <li>Knowledge Representation</li> <li>Tools</li> <li>Tool Calling</li> <li>Tool Catalogs</li> <li>Local Development Environment</li> <li>Ollama</li> <li>Large Language Models</li> <li>DeepSeek R1</li> <li>Prompt Engineering</li> <li>Basic Tool-Calling Agents</li> <li>Graphics Processing Unit</li> <li>GPU Optimization</li> <li>SmolAgents Library</li> <li>Agent Frameworks</li> <li>LangChain</li> <li>PydanticAI</li> <li>ReAct Framework</li> <li>Custom Tools</li> <li>Tool Documentation</li> <li>Code Agents</li> <li>Direct Code Generation</li> <li>Code Execution</li> <li>Public Knowledge</li> <li>Private Knowledge</li> <li>Knowledge Sources Integration</li> <li>Multi-Agent Systems</li> <li>Error Handling</li> <li>Safety Considerations</li> <li>Intelligent Textbook</li> <li>Lightweight LLMs</li> <li>GPU Memory Requirements</li> <li>Token Generation Rate</li> <li>Python Docstrings</li> <li>Decorators</li> <li>Tool Decorator</li> <li>Function Parameters</li> <li>Parameter Types</li> <li>Return Types</li> <li>ISO 11179 Definitions</li> <li>Controlled Imports</li> <li>Sandboxed Environment</li> <li>Code Security</li> <li>LLM Integration</li> <li>Agent Architecture</li> <li>Decision-Making Agents</li> <li>Function-Calling Agents</li> <li>Iterative Agents</li> <li>Workflow Agents</li> <li>Code-Generating Agents</li> <li>Agentic Execution</li> <li>LLM Agent Control</li> <li>Agency Spectrum</li> <li>Knowledge Graphs</li> <li>Ontologies</li> <li>Semantic Networks</li> <li>Frames</li> <li>Rules-Based Systems</li> <li>Logic-Based Systems</li> <li>Information Organization</li> <li>Reasoning Mechanisms</li> <li>Semantic Grounding</li> <li>Procedural Code</li> <li>Declarative Code</li> <li>Agentic Code</li> <li>JSON Formatting</li> <li>LLM Response Parsing</li> <li>Python Code Snippets</li> <li>Matrix Operations</li> <li>Parallel Processing</li> <li>Model Inference</li> <li>Local LLM Deployment</li> <li>FinalAnswerTool</li> <li>Reasoning Trace</li> <li>Interleaved Reasoning and Action</li> <li>Retrieval Augmented Generation</li> <li>Knowledge Representation Trade-offs</li> <li>Tool API Design</li> <li>Action Definition</li> <li>Agntcy Standards</li> <li>Model Context Protocol</li> <li>Tool Name Conventions</li> <li>Business Process Modeling Notation</li> <li>Goal-Driven Process</li> <li>Software Development Life Cycle</li> <li>Computational Entities</li> <li>Autonomous Systems</li> <li>User Goal Representation</li> <li>Domain Knowledge</li> <li>Rule-Based Systems</li> <li>Code Agent Approach</li> <li>Efficiency in Execution</li> <li>Expressiveness in Programming</li> <li>Security Risks</li> <li>Module Access Control</li> <li>Isolation Contexts</li> <li>Agent Collaboration</li> <li>Agent Communication</li> <li>Agent Coordination</li> <li>Specialized Agent Roles</li> <li>Transparent Decision-Making</li> <li>Explainable AI</li> <li>Response Time Optimization</li> <li>Fluid Interactions</li> <li>External Knowledge Integration</li> <li>Context Management</li> <li>Task Decomposition</li> <li>Orchestration of Planning</li> <li>Dynamic Adjustments</li> <li>Python Exception Handling</li> <li>Symbolic Structures</li> <li>Entity Relationships</li> <li>Constraint Modeling</li> <li>Knowledge Access Efficiency</li> <li>Inference Mechanisms</li> <li>Truth Value Evaluation</li> <li>Incomplete Information Handling</li> <li>Abstract Symbol Mapping</li> <li>Real-World Referents</li> <li>Expressiveness vs Efficiency</li> <li>Computational Resource Constraints</li> <li>Local vs Cloud Execution</li> <li>Latency Reduction</li> <li>Cost Optimization</li> <li>Parameter Documentation</li> <li>Enumerated Values</li> <li>Error Return Formats</li> <li>Python Function Definition</li> <li>Automated Documentation</li> <li>DeepSeek Architecture</li> <li>Mixture of Experts</li> <li>Reinforcement Learning</li> <li>Parameter Efficiency</li> <li>Knowledge Base Integration</li> <li>Token Prediction</li> <li>Next-Token Prediction</li> <li>Deep Learning Models</li> <li>Transformer Architecture</li> <li>Attention Mechanisms</li> <li>Context Windows</li> <li>Prompt Templates</li> <li>Few-Shot Learning</li> <li>Zero-Shot Capabilities</li> <li>Agent Memory Systems</li> <li>Conversation State Management</li> <li>External Tool Connection</li> <li>API Integration</li> <li>Database Connectivity</li> <li>System Autonomy Degree</li> <li>Code Generation Quality</li> <li>Code Testing</li> <li>Code Debugging</li> <li>Agent Performance Metrics</li> <li>Peer Review Process</li> <li>Project Planning</li> <li>Milestone Reviews</li> <li>Feedback Integration</li> <li>Interactive Agent Design</li> <li>User Experience Considerations</li> <li>Agent Interface Design</li> <li>Command Parsing</li> <li>Intent Recognition</li> <li>Natural Language Understanding</li> <li>Tool Selection Logic</li> <li>Parameter Extraction</li> <li>Content Generation</li> <li>Content Refinement</li> <li>Multimodal Agents</li> <li>Education Applications</li> <li>Code Assistant Applications</li> <li>Knowledge Graph Traversal</li> <li>Graph Database Integration</li> <li>Relationship Types</li> <li>Entity Properties</li> <li>Knowledge Graph Construction</li> <li>Knowledge Graph Maintenance</li> <li>Knowledge Inference</li> <li>Reasoning Chain Documentation</li> <li>Decision Transparency</li> <li>Interpretable Agent Behavior</li> <li>Domain-Specific Knowledge</li> <li>Tool Interoperability</li> <li>Result Validation</li> <li>Output Formatting</li> <li>Code Documentation Generation</li> <li>Error Messages Interpretation</li> <li>LLM Limitations</li> <li>Context Window Management</li> <li>Tool Selection Strategies</li> <li>System Architecture Design</li> <li>Agent Deployment Strategies</li> </ol>"},{"location":"prompts/concept-taxonomy/","title":"Concept Taxonomy","text":""},{"location":"prompts/concept-taxonomy/#from-prior-step","title":"From Prior Step:","text":""},{"location":"prompts/concept-taxonomy/#concept-clusters","title":"Concept Clusters:","text":"<p>The dependencies reveal several natural clusters of related concepts:</p>"},{"location":"prompts/concept-taxonomy/#concept-clusters_1","title":"Concept Clusters:","text":"<p>The dependencies reveal several natural clusters of related concepts:</p> <ol> <li>Programming Fundamentals: Python, data types, structures, docstrings</li> <li>Agent Theory: Software agents, agency levels, autonomy</li> <li>Knowledge Representation: Knowledge graphs, ontologies, semantic networks</li> <li>Tool Engineering: Tool calling, catalogs, documentation</li> <li>LLM Implementation: Model architectures, inference, deployment</li> <li>Agent Development: Frameworks, integration, deployment strategies</li> </ol> <p>Prompt</p> <p>Your observation of concept clusters is excellent.  I would like to color-code the concepts when they are displayed on the network graph.  Can you suggest four other clusters of related concepts we can use to show the concept types?</p>"},{"location":"prompts/concept-taxonomy/#response","title":"Response","text":"<p>I'd be happy to suggest additional concept clusters for color-coding in your network graph visualization. Based on my analysis of the dependency structure, here are four additional meaningful clusters that would complement the ones already identified:</p>"},{"location":"prompts/concept-taxonomy/#additional-concept-clusters-for-color-coding","title":"Additional Concept Clusters for Color-Coding","text":""},{"location":"prompts/concept-taxonomy/#1-security-safety-framework","title":"1. Security &amp; Safety Framework","text":"<p>Concepts related to responsible agent development, security concerns, and safe execution: - Code Security (#52) - Safety Considerations (#38) - Controlled Imports (#50) - Sandboxed Environment (#51) - Error Handling (#37) - Module Access Control (#104) - Isolation Contexts (#105) - Security Risks (#103) - Result Validation (#192) - Error Messages Interpretation (#195)</p>"},{"location":"prompts/concept-taxonomy/#2-performance-optimization","title":"2. Performance &amp; Optimization","text":"<p>Concepts focused on efficiency, speed, and resource management: - GPU Optimization (#22) - Token Generation Rate (#42) - Response Time Optimization (#112) - Latency Reduction (#132) - Cost Optimization (#133) - Parameter Efficiency (#142) - Computational Resource Constraints (#130) - Context Window Management (#197) - GPU Memory Requirements (#41) - Local vs Cloud Execution (#131)</p>"},{"location":"prompts/concept-taxonomy/#3-knowledge-integration-reasoning","title":"3. Knowledge Integration &amp; Reasoning","text":"<p>Concepts dealing with how agents access, process, and reason with information: - Knowledge Sources Integration (#35) - External Knowledge Integration (#114) - Knowledge Base Integration (#143) - Retrieval Augmented Generation (#85) - Reasoning Trace (#83) - Reasoning Mechanisms (#70) - Inference Mechanisms (#124) - Truth Value Evaluation (#125) - Incomplete Information Handling (#126) - Reasoning Chain Documentation (#187)</p>"},{"location":"prompts/concept-taxonomy/#4-user-interaction-experience","title":"4. User Interaction &amp; Experience","text":"<p>Concepts related to how agents interact with and serve users: - Interactive Agent Design (#167) - User Experience Considerations (#168) - Agent Interface Design (#169) - Natural Language Understanding (#172) - Intent Recognition (#171) - Command Parsing (#170) - Content Generation (#175) - Content Refinement (#176) - Output Formatting (#193) - User Goal Representation (#97)</p>"},{"location":"prompts/concept-taxonomy/#visualization-strategy","title":"Visualization Strategy","text":"<p>When implementing this color-coding in your network graph:</p> <ol> <li>Use distinctive colors - Choose colors with high contrast for easy visual differentiation</li> <li>Add a legend - Include a clear legend showing what each color represents</li> <li>Consider using gradient shading - Within each cluster, you might use darker shades for more foundational concepts and lighter shades for derived concepts</li> <li>Highlight cross-cluster connections - Edge connections between different clusters could be emphasized to show interdisciplinary connections</li> </ol> <p>These additional clusters, combined with your original clusters, provide a comprehensive categorization scheme that covers the major conceptual areas of the course. This visualization approach will help students see the relationships between different concept types and understand how knowledge in one area supports development in others.</p>"},{"location":"prompts/concept-taxonomy/#concept-taxonomy-for-intelligent-software-agents-course","title":"Concept Taxonomy for Intelligent Software Agents Course","text":"<p>Here's a summary of the 10 concept classifiers with their definitions:</p>"},{"location":"prompts/concept-taxonomy/#1-programming-fundamentals","title":"1. Programming Fundamentals","text":"<p>Concepts related to core programming skills, languages, and data handling that form the technical foundation for agent development. This includes Python programming, data structures, function definitions, and exception handling.</p>"},{"location":"prompts/concept-taxonomy/#2-agent-theory","title":"2. Agent Theory","text":"<p>Theoretical concepts describing the nature, capabilities, and classification of software agents. This covers agency levels, autonomy, computational entities, and the spectrum of agent capabilities from basic to advanced.</p>"},{"location":"prompts/concept-taxonomy/#3-knowledge-representation","title":"3. Knowledge Representation","text":"<p>Concepts focused on how information is structured, stored, and made accessible to agents. This includes ontologies, semantic networks, frames, and the trade-offs between different representation approaches.</p>"},{"location":"prompts/concept-taxonomy/#4-tool-engineering","title":"4. Tool Engineering","text":"<p>Concepts related to creating, documenting, and managing the functions that agents can call to interact with external systems. This covers tool design, documentation standards, naming conventions, and APIs.</p>"},{"location":"prompts/concept-taxonomy/#5-llm-implementation","title":"5. LLM Implementation","text":"<p>Technical concepts specific to large language models, their architecture, deployment, and operation. This includes model types, inference techniques, context management, and transformer architecture.</p>"},{"location":"prompts/concept-taxonomy/#6-security-safety-framework","title":"6. Security &amp; Safety Framework","text":"<p>Concepts focused on ensuring agent systems operate responsibly and securely. This includes sandboxed environments, controlled imports, error handling, and validation of agent outputs.</p>"},{"location":"prompts/concept-taxonomy/#7-performance-optimization","title":"7. Performance &amp; Optimization","text":"<p>Concepts related to improving speed, efficiency, and resource utilization of agent systems. This covers GPU optimization, token generation rates, latency reduction, and computational resource management.</p>"},{"location":"prompts/concept-taxonomy/#8-knowledge-integration-reasoning","title":"8. Knowledge Integration &amp; Reasoning","text":"<p>Concepts addressing how agents combine information sources and perform logical reasoning. This includes retrieval augmented generation, reasoning mechanisms, knowledge base integration, and inference.</p>"},{"location":"prompts/concept-taxonomy/#9-user-interaction-experience","title":"9. User Interaction &amp; Experience","text":"<p>Concepts centered on how agents communicate with and serve users. This includes interface design, natural language understanding, content generation and refinement, and understanding user goals.</p>"},{"location":"prompts/concept-taxonomy/#10-agent-development","title":"10. Agent Development","text":"<p>Practical implementation concepts for building agent systems. This includes agent frameworks, integration patterns, deployment strategies, and specialized implementations like code agents and multi-agent systems.</p> <p>These classifications provide a comprehensive framework for organizing the 200 concepts in the course, highlighting the multidisciplinary nature of intelligent software agent development spanning from theoretical foundations to practical implementation considerations.</p>"},{"location":"prompts/taxonomy/","title":"Concept Taxonomy for Intelligent Software Agents Course","text":"<p>Here's a summary of the 12 concept classifiers with their definitions:</p>"},{"location":"prompts/taxonomy/#1-programming-fundamentals","title":"1. Programming Fundamentals","text":"<p>Concepts related to core programming skills, languages, and data handling that form the technical foundation for agent development. This includes Python programming, data structures, function definitions, and exception handling.</p>"},{"location":"prompts/taxonomy/#2-agent-theory","title":"2. Agent Theory","text":"<p>Theoretical concepts describing the nature, capabilities, and classification of software agents. This covers agency levels, autonomy, computational entities, and the spectrum of agent capabilities from basic to advanced.</p>"},{"location":"prompts/taxonomy/#3-knowledge-representation","title":"3. Knowledge Representation","text":"<p>Concepts focused on how information is structured, stored, and made accessible to agents. This includes ontologies, semantic networks, frames, and the trade-offs between different representation approaches.</p>"},{"location":"prompts/taxonomy/#4-tool-engineering","title":"4. Tool Engineering","text":"<p>Concepts related to creating, documenting, and managing the functions that agents can call to interact with external systems. This covers tool design, documentation standards, naming conventions, and APIs.</p>"},{"location":"prompts/taxonomy/#5-llm-implementation","title":"5. LLM Implementation","text":"<p>Technical concepts specific to large language models, their architecture, deployment, and operation. This includes model types, inference techniques, context management, and transformer architecture.</p>"},{"location":"prompts/taxonomy/#6-security-safety-framework","title":"6. Security &amp; Safety Framework","text":"<p>Concepts focused on ensuring agent systems operate responsibly and securely. This includes sandboxed environments, controlled imports, error handling, and validation of agent outputs.</p>"},{"location":"prompts/taxonomy/#7-performance-optimization","title":"7. Performance &amp; Optimization","text":"<p>Concepts related to improving speed, efficiency, and resource utilization of agent systems. This covers GPU optimization, token generation rates, latency reduction, and computational resource management.</p>"},{"location":"prompts/taxonomy/#8-knowledge-integration-reasoning","title":"8. Knowledge Integration &amp; Reasoning","text":"<p>Concepts addressing how agents combine information sources and perform logical reasoning. This includes retrieval augmented generation, reasoning mechanisms, knowledge base integration, and inference.</p>"},{"location":"prompts/taxonomy/#9-user-interaction-experience","title":"9. User Interaction &amp; Experience","text":"<p>Concepts centered on how agents communicate with and serve users. This includes interface design, natural language understanding, content generation and refinement, and understanding user goals.</p>"},{"location":"prompts/taxonomy/#10-agent-development","title":"10. Agent Development","text":"<p>Practical implementation concepts for building agent systems. This includes agent frameworks, integration patterns, deployment strategies, and specialized implementations like code agents and multi-agent systems.</p>"},{"location":"prompts/taxonomy/#11-midterm-and-capstone-projects-ideas","title":"11. Midterm and Capstone Projects Ideas","text":"<p>These are the concepts associated with possible midterm an d end of course capstone project ideas that students might work on.  This list is highly variable and will depend on student interests an school budgets.</p>"},{"location":"prompts/taxonomy/#12-miscellaneous","title":"12. Miscellaneous","text":"<p>Use this category if a concept does not fit into the prior categories.</p> <p>These classifications provide a comprehensive framework for organizing the 200 concepts in the course, highlighting the multidisciplinary nature of intelligent software agent development spanning from theoretical foundations to practical implementation considerations.</p>"},{"location":"sims/","title":"MicroSims for Learning Intelligent Agents","text":""},{"location":"sims/#learning-graph","title":"Learning Graph","text":"<p>Learning Graph</p>"},{"location":"sims/agentic-learning-steps/","title":"Agentic Learning Steps","text":""},{"location":"sims/agentic-learning-steps/#introduction-level","title":"Introduction Level","text":"<ul> <li>Language Models</li> <li>Prompt Engineering</li> <li>System Prompts</li> <li>Context Windows</li> <li>Prompt Enrichment (RAG)</li> <li>Adding Private Knowledge</li> <li>Text Similarity</li> <li>Agentic Tools</li> </ul>"},{"location":"sims/agentic-learning-steps/#intermediate-level","title":"Intermediate Level","text":"<ul> <li>Problem Decomposition</li> <li>Planning</li> <li>Reasoning</li> <li>Chatbots</li> <li>Function Calling</li> <li>Agentic Frameworks</li> <li>Case Study: DeepSeek R1</li> </ul>"},{"location":"sims/agentic-learning-steps/#advanced-level","title":"Advanced Level","text":"<ul> <li>Workflows</li> <li>Decisions</li> <li>Agent-Tool Interfaces</li> <li>Model Context Protocol</li> <li>Agent-Agent Communication</li> <li>Reinforcement Learning</li> <li>Agentic Evaluation</li> </ul>"},{"location":"sims/learning-graph/","title":"Learning Graph","text":""},{"location":"sims/learning-graph/#learning-graph-for-intelligent-agents","title":"Learning Graph for Intelligent Agents","text":"<p>Run the Learning Graph MicroSim</p>"},{"location":"tutorials/","title":"Agents Course Step-by-Step Tutorials","text":""},{"location":"tutorials/#ollama","title":"Ollama","text":"<p>Ollama</p>"},{"location":"tutorials/#pydanticai","title":"PydanticAI","text":"<p>PydanticAI</p>"},{"location":"tutorials/ollama/","title":"Ollama: A Step-by-Step Tutorial","text":""},{"location":"tutorials/ollama/#setting-up-and-using-ollama-a-comprehensive-guide","title":"Setting Up and Using Ollama: A Comprehensive Guide","text":"<p>I'll walk you through a complete tutorial on setting up and using Ollama for local LLM deployment, including installation, model management, command-line usage, and Python integration.</p>"},{"location":"tutorials/ollama/#1-installing-ollama","title":"1. Installing Ollama","text":""},{"location":"tutorials/ollama/#for-macos","title":"For macOS","text":"<ol> <li>Download the official installer from ollama.ai</li> <li>Open the downloaded file and follow the installation prompts</li> <li>Once installed, Ollama will run as a background service</li> </ol>"},{"location":"tutorials/ollama/#for-linux","title":"For Linux","text":"<pre><code>curl -fsSL https://ollama.ai/install.sh | sh\n</code></pre>"},{"location":"tutorials/ollama/#for-windows","title":"For Windows","text":"<ol> <li>Download the installer from ollama.ai</li> <li>Run the installer and follow the prompts</li> <li>Ollama will be available as a background service after installation</li> </ol>"},{"location":"tutorials/ollama/#2-downloading-and-managing-models","title":"2. Downloading and Managing Models","text":"<p>Once Ollama is installed, you can download models using the <code>pull</code> command.</p>"},{"location":"tutorials/ollama/#basic-model-download","title":"Basic Model Download","text":"<pre><code># Pull the DeepSeek R1 model\nollama pull deepseek-r1:7b\n</code></pre>"},{"location":"tutorials/ollama/#list-available-models","title":"List Available Models","text":"<pre><code>ollama list\n</code></pre> <p>Output will look similar to:</p> <pre><code>NAME                    ID             SIZE    MODIFIED\ndeepseek-r1:latest      0a8c26691023   4.7 GB  3 weeks ago\ndeepseek-r1:7b          0a8c26691023   4.7 GB  3 weeks ago\n</code></pre>"},{"location":"tutorials/ollama/#remove-a-model","title":"Remove a Model","text":"<pre><code>ollama rm deepseek-r1:7b\n</code></pre>"},{"location":"tutorials/ollama/#additional-popular-models","title":"Additional Popular Models","text":"<pre><code># Pull other useful models\nollama pull llama3:8b\nollama pull mistral:7b\nollama pull gemma:7b\n</code></pre>"},{"location":"tutorials/ollama/#3-using-ollama-via-command-line","title":"3. Using Ollama via Command Line","text":""},{"location":"tutorials/ollama/#basic-generation","title":"Basic Generation","text":"<pre><code>ollama run deepseek-r1:7b \"What is a knowledge graph?\"\n</code></pre>"},{"location":"tutorials/ollama/#interactive-chat-session","title":"Interactive Chat Session","text":"<p><pre><code>ollama run deepseek-r1:7b\n</code></pre> This opens an interactive chat session where you can type prompts and get responses. Use Ctrl+D to exit.</p>"},{"location":"tutorials/ollama/#advanced-parameters","title":"Advanced Parameters","text":"<pre><code># Run with specific parameters\nollama run deepseek-r1:7b --temperature 0.7 --top_p 0.9\n</code></pre>"},{"location":"tutorials/ollama/#creating-custom-model-versions-with-modelfiles","title":"Creating Custom Model Versions with Modelfiles","text":"<p>Create a file named <code>Modelfile</code>: <pre><code>FROM deepseek-r1:7b\nSYSTEM \"You are a helpful AI assistant specialized in Python programming.\"\nPARAMETER temperature 0.7\n</code></pre></p> <p>Build your custom model: <pre><code>ollama create code-helper -f Modelfile\n</code></pre></p> <p>Run your custom model: <pre><code>ollama run code-helper \"Write a function to calculate Fibonacci numbers\"\n</code></pre></p>"},{"location":"tutorials/ollama/#4-using-ollama-from-python","title":"4. Using Ollama from Python","text":""},{"location":"tutorials/ollama/#basic-api-usage","title":"Basic API Usage","text":"<pre><code>import requests\nimport json\n\n# Define the API endpoint\napi_url = \"http://localhost:11434/api/generate\"\n\n# Configure the request\npayload = {\n    \"model\": \"deepseek-r1:7b\",\n    \"prompt\": \"What are the key features of a code agent?\",\n    \"stream\": False\n}\n\n# Make the API call\nresponse = requests.post(api_url, json=payload)\nresult = response.json()\n\n# Print the response\nprint(result['response'])\n</code></pre>"},{"location":"tutorials/ollama/#streaming-responses","title":"Streaming Responses","text":"<pre><code>import requests\nimport json\n\napi_url = \"http://localhost:11434/api/generate\"\n\npayload = {\n    \"model\": \"deepseek-r1:7b\",\n    \"prompt\": \"Explain knowledge representation in AI agents\",\n    \"stream\": True\n}\n\n# Stream the response\nwith requests.post(api_url, json=payload, stream=True) as response:\n    for line in response.iter_lines():\n        if line:\n            json_response = json.loads(line)\n            if 'response' in json_response:\n                print(json_response['response'], end='', flush=True)\n            # Check if this is the final response\n            if json_response.get('done', False):\n                print()  # Add a newline at the end\n</code></pre>"},{"location":"tutorials/ollama/#using-the-ollama-python-client-library","title":"Using the Ollama Python Client Library","text":"<p>First, install the official Python client: <pre><code>pip install ollama\n</code></pre></p> <p>Then, use it in your code:</p> <pre><code>import ollama\n\n# Simple generation\nresponse = ollama.generate(\n    model='deepseek-r1:7b',\n    prompt='What is a knowledge graph in the context of AI?'\n)\nprint(response['response'])\n\n# Chat completion with history\nmessages = [\n    {\n        'role': 'user',\n        'content': 'What are the key components of a multi-agent system?'\n    }\n]\n\nresponse = ollama.chat(\n    model='deepseek-r1:7b',\n    messages=messages\n)\n\n# Add the response to the conversation\nmessages.append({\n    'role': 'assistant',\n    'content': response['message']['content']\n})\n\n# Continue the conversation\nmessages.append({\n    'role': 'user',\n    'content': 'How can these components communicate with each other?'\n})\n\nresponse = ollama.chat(\n    model='deepseek-r1:7b',\n    messages=messages\n)\n\nprint(response['message']['content'])\n</code></pre>"},{"location":"tutorials/ollama/#integration-with-smolagents-framework","title":"Integration with SmolAgents Framework","text":"<p>Based on your documents, here's an example of how to integrate Ollama with the SmolAgents framework:</p> <pre><code>import smolagents\nfrom smolagents.models import Model\nfrom smolagents import CodeAgent\n\n# Create a model using the local Ollama endpoint\nollama_model = Model(\n    name=\"ollama-deepseek\",\n    base_url=\"http://localhost:11434\",\n    api_type=\"ollama\",\n    model_name=\"deepseek-r1:7b\",\n    temperature=0.2\n)\n\n# Create tools for the agent\ndef search_documentation(query):\n    \"\"\"\n    Search documentation for the given query.\n\n    Args:\n        query (str): The search query\n\n    Returns:\n        str: Search results\n    \"\"\"\n    # Implementation would go here\n    return f\"Documentation results for: {query}\"\n\n# Create a code agent with the model\nagent = CodeAgent(\n    model=ollama_model,\n    tools=[search_documentation],\n    system_prompt=\"You are a helpful coding assistant that writes Python code.\"\n)\n\n# Use the agent\nresponse = agent.run(\"Create a function to parse CSV files using the pandas library\")\nprint(response)\n</code></pre>"},{"location":"tutorials/ollama/#5-performance-optimization","title":"5. Performance Optimization","text":""},{"location":"tutorials/ollama/#gpu-acceleration","title":"GPU Acceleration","text":"<p>Ollama automatically uses GPU if available. To check if GPU is being utilized:</p> <pre><code># For NVIDIA GPUs\nnvidia-smi\n\n# For AMD GPUs\nrocm-smi\n</code></pre>"},{"location":"tutorials/ollama/#memory-requirements","title":"Memory Requirements","text":"<p>As indicated in your course materials, models like DeepSeek R1:7b work best with GPUs that have at least 12GB of VRAM. Here are some compatible GPU models: - GeForce RTX 4090 (24 GB) - GeForce RTX 4080 (16 GB) - GeForce RTX 3090 Ti (24 GB) - GeForce RTX 3090 (24 GB) - GeForce RTX 3080 Ti (12 GB) - GeForce RTX 3080 (12 GB version)</p>"},{"location":"tutorials/ollama/#model-quantization","title":"Model Quantization","text":"<p>Ollama supports different quantization levels to reduce memory requirements:</p> <pre><code># Pull a quantized version\nollama pull deepseek-r1:7b-q4_0\n\n# List to see the size difference\nollama list\n</code></pre>"},{"location":"tutorials/ollama/#6-advanced-use-cases","title":"6. Advanced Use Cases","text":""},{"location":"tutorials/ollama/#creating-a-simple-code-agent","title":"Creating a Simple Code Agent","text":"<pre><code>import ollama\nfrom typing import Dict, Any, List\n\ndef code_agent(prompt: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    A simple code agent that leverages Ollama to generate Python code.\n\n    Args:\n        prompt (str): The coding task description\n\n    Returns:\n        Dict[str, Any]: Results including code and explanation\n    \"\"\"\n    # Enhance the prompt\n    enhanced_prompt = f\"\"\"\n    Write Python code for the following task:\n    {prompt}\n\n    Provide well-commented code with docstrings and explanations.\n    \"\"\"\n\n    # Generate the code\n    response = ollama.generate(\n        model='deepseek-r1:7b',\n        prompt=enhanced_prompt,\n        system=\"You are an expert Python programmer. Generate clean, efficient, and well-documented code.\"\n    )\n\n    # Extract and format the code\n    code_text = response['response']\n\n    return {\n        \"code\": code_text,\n        \"prompt\": prompt,\n        \"model\": \"deepseek-r1:7b\"\n    }\n\n# Example usage\nresult = code_agent(\"Create a function to calculate the Fibonacci sequence up to n terms\")\nprint(result[\"code\"])\n</code></pre>"},{"location":"tutorials/ollama/#building-a-react-agent-with-ollama","title":"Building a ReAct Agent with Ollama","text":"<pre><code>import ollama\nimport json\nfrom typing import Dict, List, Any, Callable\n\nclass ReActAgent:\n    def __init__(self, model_name: str, tools: Dict[str, Callable]):\n        self.model_name = model_name\n        self.tools = tools\n        self.messages = []\n        self.max_iterations = 5\n\n    def add_system_message(self, content: str):\n        self.messages.append({\"role\": \"system\", \"content\": content})\n\n    def add_user_message(self, content: str):\n        self.messages.append({\"role\": \"user\", \"content\": content})\n\n    def add_assistant_message(self, content: str):\n        self.messages.append({\"role\": \"assistant\", \"content\": content})\n\n    def run(self, query: str) -&gt; str:\n        self.add_user_message(query)\n\n        for _ in range(self.max_iterations):\n            # Get the next action from the model\n            response = ollama.chat(\n                model=self.model_name,\n                messages=self.messages\n            )\n\n            response_content = response['message']['content']\n            self.add_assistant_message(response_content)\n\n            # Check if the response contains a tool call\n            if \"Action:\" in response_content:\n                try:\n                    # Extract the action and arguments\n                    action_line = [line for line in response_content.split('\\n') if line.startswith('Action:')][0]\n                    action_name = action_line.replace('Action:', '').strip()\n\n                    args_line = [line for line in response_content.split('\\n') if line.startswith('Action Input:')][0]\n                    args_text = args_line.replace('Action Input:', '').strip()\n\n                    # Execute the tool\n                    if action_name in self.tools:\n                        tool_result = self.tools[action_name](args_text)\n                        self.add_user_message(f\"Tool result: {tool_result}\")\n                    else:\n                        self.add_user_message(f\"Error: Tool '{action_name}' not found\")\n                except Exception as e:\n                    self.add_user_message(f\"Error executing action: {str(e)}\")\n            else:\n                # If no action is requested, return the final answer\n                return response_content\n\n        return \"Max iterations reached without final answer\"\n\n# Example tools\ndef search_web(query: str) -&gt; str:\n    return f\"Search results for '{query}': [Sample results would appear here]\"\n\ndef get_current_weather(location: str) -&gt; str:\n    return f\"Weather in {location}: 72\u00b0F, Sunny\"\n\n# Create and use the agent\ntools = {\n    \"search_web\": search_web,\n    \"get_weather\": get_current_weather\n}\n\nagent = ReActAgent(model_name=\"deepseek-r1:7b\", tools=tools)\nagent.add_system_message(\"\"\"\nYou are a helpful assistant that can use tools to get information.\nWhen you need information, specify the tool to use with:\nAction: tool_name\nAction Input: input for the tool\n\nWhen you have the final answer, provide it directly without using a tool.\n\"\"\")\n\nresult = agent.run(\"What's the weather like in New York?\")\nprint(result)\n</code></pre>"},{"location":"tutorials/ollama/#7-troubleshooting-common-issues","title":"7. Troubleshooting Common Issues","text":""},{"location":"tutorials/ollama/#connection-refused","title":"Connection Refused","text":"<p>If you see \"Connection refused\" errors when trying to use the API:</p> <pre><code># Check if Ollama service is running\nps aux | grep ollama\n\n# Restart the service if needed\nollama serve\n</code></pre>"},{"location":"tutorials/ollama/#high-memory-usage","title":"High Memory Usage","text":"<p>If you're experiencing memory issues:</p> <ol> <li>Use a more quantized model (e.g., q4_0 instead of q8_0)</li> <li>Adjust context window size:</li> </ol> <pre><code>response = ollama.generate(\n    model='deepseek-r1:7b',\n    prompt='Your prompt here',\n    options={\"num_ctx\": 2048}  # Reduce context window size\n)\n</code></pre>"},{"location":"tutorials/ollama/#slow-response-times","title":"Slow Response Times","text":"<p>If responses are too slow:</p> <ol> <li>Verify GPU is being used correctly</li> <li>Consider a smaller model</li> <li>Check other applications using GPU resources</li> </ol>"},{"location":"tutorials/ollama/#conclusion","title":"Conclusion","text":"<p>Ollama provides a flexible and powerful way to run LLMs locally. With the DeepSeek R1:7b model mentioned in your course materials, you can achieve token generation rates of 50+ tokens per second with suitable hardware, making it viable for development and testing of intelligent software agents. The framework's integration capabilities with Python make it an excellent choice for building code agents, implementing ReAct patterns, and developing other advanced agent systems.</p>"},{"location":"tutorials/pydanticai/","title":"PydanticAI Agent System Tutorial: A Comprehensive Guide","text":""},{"location":"tutorials/pydanticai/#introduction","title":"Introduction","text":"<p>PydanticAI is a Python agent framework designed for building production-grade applications with Generative AI. It offers a streamlined approach compared to more abstract frameworks like LangChain, making it easier to develop, deploy, and maintain agent-based systems.</p> <p>This tutorial will guide you through setting up PydanticAI, creating basic agents, implementing more complex agent systems, and providing best practices for debugging and monitoring.</p>"},{"location":"tutorials/pydanticai/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Installation</li> <li>Basic Concepts</li> <li>Creating Your First Agent</li> <li>Tool Creation and Integration</li> <li>Advanced Agent Patterns</li> <li>Debugging Strategies</li> <li>Performance Optimization</li> <li>Real-World Example</li> <li>Troubleshooting Common Issues</li> </ol>"},{"location":"tutorials/pydanticai/#1-installation","title":"1. Installation","text":"<p>There are two options to setup virtual environments: Conda and VENV. Unless you know you are going to ONLY use Python, we suggest you use Conda since it can also be used to manage non-Python languages.</p>"},{"location":"tutorials/pydanticai/#conda-setup","title":"Conda Setup","text":"<pre><code>conda deactivate\nconda create -n \"agents\" python=3.10\nconda activate \n</code></pre>"},{"location":"tutorials/pydanticai/#python-venv","title":"Python VENV","text":"<p>Alternatively you can install PydanticAI and its dependencies in a Python VENV</p> <pre><code># Create a virtual environment (recommended)\npython -m venv pydantic_env\nsource pydantic_env/bin/activate  # On Windows: pydantic_env\\Scripts\\activate\n</code></pre>"},{"location":"tutorials/pydanticai/#pip","title":"Pip","text":"<pre><code>pip install pydantic-ai pydantic\n</code></pre> <p>You can also install various LLM model tools</p> <pre><code>pip install ollama openai\n</code></pre>"},{"location":"tutorials/pydanticai/#sample-test-code","title":"Sample Test Code","text":"<pre><code>from pydantic_ai import Agent\n\n# Create an agent using the local DeepSeek-R1 model from Ollama\nagent = Agent(\n    'ollama:deepseek-r1',  # Use the Ollama model name format\n    system_prompt='Be concise, reply with one sentence.',\n    model_kwargs={\n        # Additional parameters for the Ollama API\n        'base_url': 'http://localhost:11434',  # Default Ollama API URL\n        'temperature': 0.7,\n    }\n)\n\n# Run a simple query\nresult = agent.run_sync('Where does \"hello world\" come from?')\nprint(result.data)\n\"\"\"\nExpected output similar to:\nThe first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\n\"\"\"\n\n# You can also try a more complex query\ncode_query = agent.run_sync('Write a Python function to calculate the Fibonacci sequence')\nprint(\"\\nCode generation example:\")\nprint(code_query.data)\n</code></pre> <p>For local development with Ollama, you'll need to have Ollama installed and a compatible model like DeepSeek R1:</p> <pre><code># Download and install Ollama from https://ollama.ai/\n# Then pull the DeepSeek R1 model\nollama pull deepseek-r1:7b\n</code></pre> <p>Verify your installation:</p> <pre><code>import pydantic_ai\nprint(f\"PydanticAI version: {pydantic_ai.__version__}\")\n</code></pre>"},{"location":"tutorials/pydanticai/#2-basic-concepts","title":"2. Basic Concepts","text":"<p>Before diving into code, let's understand key PydanticAI concepts:</p> <ul> <li>Agents: Autonomous entities that use LLMs to make decisions and perform actions</li> <li>Tools: Functions that agents can call to interact with external systems</li> <li>Pydantic Models: Used to define structured inputs and outputs for tools</li> <li>Prompts: Templates that guide the LLM's behavior within the agent</li> </ul> <p>PydanticAI uses Pydantic's data validation to ensure type safety and proper documentation of tools, making it easier for LLMs to understand and use them correctly.</p>"},{"location":"tutorials/pydanticai/#3-creating-your-first-agent","title":"3. Creating Your First Agent","text":"<p>Let's create a simple agent that can respond to user queries:</p> <pre><code>from pydantic_ai import Agent\nfrom pydantic import BaseModel\nimport logging\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[logging.StreamHandler()]\n)\nlogger = logging.getLogger(\"pydantic_agent\")\n\n# Define our agent\nclass SimpleAgent(Agent):\n    \"\"\"A simple agent that can respond to user queries.\"\"\"\n\n    def get_system_prompt(self):\n        return \"\"\"You are a helpful assistant that provides information about AI and programming.\n                 Answer questions concisely and accurately.\"\"\"\n\n# Initialize the agent\nagent = SimpleAgent(\n    model=\"openai/gpt-3.5-turbo\",  # For OpenAI\n    # Alternatively for local Ollama model:\n    # model=\"ollama/deepseek-r1:7b\",\n)\n\n# Run the agent\nresponse = agent.run(\"What is an intelligent software agent?\")\nprint(response)\n</code></pre>"},{"location":"tutorials/pydanticai/#4-tool-creation-and-integration","title":"4. Tool Creation and Integration","text":"<p>Tools extend an agent's capabilities. Let's create some tools and integrate them:</p> <pre><code>from pydantic_ai import Agent, tool\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport datetime\n\n# Define input/output models for our tools\nclass WeatherRequest(BaseModel):\n    location: str\n    date: Optional[datetime.date] = None\n\nclass WeatherResponse(BaseModel):\n    temperature: float\n    conditions: str\n    humidity: Optional[float] = None\n\nclass SearchRequest(BaseModel):\n    query: str\n    max_results: int = 5\n\nclass SearchResult(BaseModel):\n    title: str\n    url: str\n    snippet: str\n\nclass SearchResponse(BaseModel):\n    results: List[SearchResult]\n    total_found: int\n\n# Create tool functions\n@tool\ndef get_weather(request: WeatherRequest) -&gt; WeatherResponse:\n    \"\"\"\n    Get current weather information for a specific location.\n\n    Args:\n        request: Contains the location and optional date for weather information\n\n    Returns:\n        Weather data including temperature and conditions\n    \"\"\"\n    # In a real implementation, you would call a weather API here\n    logger.info(f\"Getting weather for {request.location}\")\n    return WeatherResponse(\n        temperature=72.5,\n        conditions=\"Sunny\",\n        humidity=45.0\n    )\n\n@tool\ndef search_information(request: SearchRequest) -&gt; SearchResponse:\n    \"\"\"\n    Search for information on a specific topic.\n\n    Args:\n        request: Contains the search query and maximum number of results to return\n\n    Returns:\n        A list of search results with titles, URLs, and snippets\n    \"\"\"\n    # In a real implementation, you would call a search API\n    logger.info(f\"Searching for: {request.query} (max: {request.max_results})\")\n    return SearchResponse(\n        results=[\n            SearchResult(\n                title=\"Example search result\",\n                url=\"https://example.com/result1\",\n                snippet=\"This is an example search result snippet\"\n            )\n        ],\n        total_found=1\n    )\n\n# Create an agent with tools\nclass AssistantAgent(Agent):\n    \"\"\"An assistant agent with access to tools.\"\"\"\n\n    def get_system_prompt(self):\n        return \"\"\"You are a helpful assistant with access to tools.\n                 Use the appropriate tool when needed to answer questions accurately.\"\"\"\n\n    # Register tools with the agent\n    tools = [get_weather, search_information]\n\n# Initialize and use the agent\nassistant = AssistantAgent(model=\"openai/gpt-3.5-turbo\")\nresponse = assistant.run(\"What's the weather like in New York?\")\nprint(response)\n</code></pre>"},{"location":"tutorials/pydanticai/#5-advanced-agent-patterns","title":"5. Advanced Agent Patterns","text":"<p>Now let's implement more advanced patterns like memory and multi-step reasoning:</p> <pre><code>from pydantic_ai import Agent, tool, AgentExecutor\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Any\nimport logging\n\n# Enhanced logging for debugging\nlogger = logging.getLogger(\"advanced_agent\")\nhandler = logging.FileHandler(\"agent_debug.log\")\nhandler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\nlogger.addHandler(handler)\nlogger.setLevel(logging.DEBUG)\n\n# Memory model\nclass ConversationMemory(BaseModel):\n    history: List[Dict[str, str]] = []\n\n    def add_interaction(self, user_message: str, agent_response: str):\n        self.history.append({\n            \"user\": user_message,\n            \"agent\": agent_response\n        })\n        logger.debug(f\"Added to memory: User: {user_message[:50]}... Agent: {agent_response[:50]}...\")\n\n    def get_recent_history(self, limit: int = 5) -&gt; List[Dict[str, str]]:\n        return self.history[-limit:] if self.history else []\n\n# ReAct pattern implementation\nclass ReActAgent(Agent):\n    \"\"\"An agent that uses the ReAct framework to interleave reasoning and action.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.memory = ConversationMemory()\n\n    def get_system_prompt(self):\n        recent_history = self.memory.get_recent_history()\n        history_text = \"\\n\".join([\n            f\"User: {interaction['user']}\\nAgent: {interaction['agent']}\"\n            for interaction in recent_history\n        ])\n\n        return f\"\"\"You are an assistant that carefully thinks through problems step by step.\n                 When faced with a complex task, break it down into smaller steps and solve each one.\n\n                 Recent conversation history:\n                 {history_text}\n\n                 Follow this format when using tools:\n                 Thought: I need to figure out what to do\n                 Action: Choose which tool to use\n                 Observation: Note the result\n                 ... (repeat as needed)\n                 Answer: Provide the final answer to the user\n                 \"\"\"\n\n    # Register tools and track execution\n    def run(self, user_input: str) -&gt; str:\n        logger.info(f\"Processing user input: {user_input}\")\n\n        try:\n            # Execute the agent with detailed logging\n            response = super().run(user_input)\n\n            # Store interaction in memory\n            self.memory.add_interaction(user_input, response)\n\n            return response\n        except Exception as e:\n            logger.error(f\"Error in agent execution: {str(e)}\", exc_info=True)\n            return f\"I encountered an error: {str(e)}\"\n\n# Define tools for our ReAct agent\n@tool\ndef calculate(expression: str) -&gt; float:\n    \"\"\"\n    Calculate the result of a mathematical expression.\n\n    Args:\n        expression: A string containing a mathematical expression\n\n    Returns:\n        The calculated result\n    \"\"\"\n    logger.debug(f\"Calculating: {expression}\")\n    try:\n        # Warning: eval can be dangerous in production, use a safer alternative\n        result = eval(expression)\n        logger.debug(f\"Calculation result: {result}\")\n        return result\n    except Exception as e:\n        logger.error(f\"Calculation error: {str(e)}\")\n        raise ValueError(f\"Error calculating expression: {str(e)}\")\n\n# Initialize and use the ReAct agent\nreact_agent = ReActAgent(\n    model=\"ollama/deepseek-r1:7b\",\n    temperature=0.2,  # Lower temperature for more deterministic responses\n    max_tokens=1000\n)\n\n# Example multi-step reasoning task\nresponse = react_agent.run(\n    \"If I have 5 apples and give 2 to my friend, then buy 3 more, how many do I have in total?\"\n)\nprint(response)\n</code></pre>"},{"location":"tutorials/pydanticai/#6-debugging-strategies","title":"6. Debugging Strategies","text":"<p>Effective debugging is crucial for agent development. Here's a comprehensive approach:</p> <pre><code>from pydantic_ai import Agent, tool\nfrom pydantic import BaseModel\nimport logging\nimport json\nimport time\nfrom typing import Any, Dict, List, Optional\n\n# Create a custom logger for agent debugging\nclass AgentDebugLogger:\n    def __init__(self, log_file=\"agent_debug.log\", console_level=logging.INFO, file_level=logging.DEBUG):\n        self.logger = logging.getLogger(\"agent_debugger\")\n        self.logger.setLevel(logging.DEBUG)\n\n        # Clear existing handlers to avoid duplication\n        self.logger.handlers = []\n\n        # Console handler\n        console = logging.StreamHandler()\n        console.setLevel(console_level)\n        console.setFormatter(logging.Formatter('%(levelname)s: %(message)s'))\n        self.logger.addHandler(console)\n\n        # File handler for detailed logs\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setLevel(file_level)\n        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n        self.logger.addHandler(file_handler)\n\n    def log_tool_call(self, tool_name: str, inputs: Dict[str, Any], outputs: Any, duration: float):\n        \"\"\"Log details of a tool call\"\"\"\n        self.logger.debug(f\"TOOL CALL: {tool_name}\")\n        self.logger.debug(f\"INPUTS: {json.dumps(inputs, default=str)}\")\n        self.logger.debug(f\"OUTPUTS: {json.dumps(outputs, default=str)}\")\n        self.logger.debug(f\"DURATION: {duration:.4f}s\")\n\n    def log_llm_call(self, prompt: str, response: str, duration: float):\n        \"\"\"Log details of an LLM call\"\"\"\n        self.logger.debug(\"LLM CALL:\")\n        self.logger.debug(f\"PROMPT: {prompt[:200]}... (truncated)\")\n        self.logger.debug(f\"RESPONSE: {response[:200]}... (truncated)\")\n        self.logger.debug(f\"DURATION: {duration:.4f}s\")\n\n        # Save full prompt and response to files for detailed analysis\n        timestamp = int(time.time())\n        with open(f\"debug_prompt_{timestamp}.txt\", \"w\") as f:\n            f.write(prompt)\n        with open(f\"debug_response_{timestamp}.txt\", \"w\") as f:\n            f.write(response)\n\n    def log_error(self, error_type: str, message: str, details: Optional[Dict] = None):\n        \"\"\"Log error information\"\"\"\n        self.logger.error(f\"ERROR - {error_type}: {message}\")\n        if details:\n            self.logger.error(f\"DETAILS: {json.dumps(details, default=str)}\")\n\n# Create a debuggable agent wrapper\nclass DebuggableAgent(Agent):\n    \"\"\"A wrapper for agents that adds detailed debugging capabilities\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.debug_logger = AgentDebugLogger()\n        self.execution_stats = {\n            \"tool_calls\": 0,\n            \"llm_calls\": 0,\n            \"errors\": 0,\n            \"total_duration\": 0\n        }\n\n    # Override the run method to add debugging\n    def run(self, user_input: str) -&gt; str:\n        self.debug_logger.logger.info(f\"AGENT RUN: Processing user input: {user_input}\")\n        start_time = time.time()\n\n        try:\n            # Capture the original tool calling mechanism\n            original_tool_call = self._call_tool\n\n            # Define a wrapper to log tool calls\n            def logged_tool_call(tool_name, **kwargs):\n                self.execution_stats[\"tool_calls\"] += 1\n                tool_start = time.time()\n\n                try:\n                    result = original_tool_call(tool_name, **kwargs)\n                    tool_duration = time.time() - tool_start\n                    self.debug_logger.log_tool_call(tool_name, kwargs, result, tool_duration)\n                    return result\n                except Exception as e:\n                    self.execution_stats[\"errors\"] += 1\n                    self.debug_logger.log_error(\"Tool Error\", f\"Error in {tool_name}: {str(e)}\", kwargs)\n                    raise\n\n            # Replace the tool call method with our logged version\n            self._call_tool = logged_tool_call\n\n            # Run the agent\n            response = super().run(user_input)\n\n            # Restore original method\n            self._call_tool = original_tool_call\n\n            # Update statistics\n            self.execution_stats[\"total_duration\"] = time.time() - start_time\n\n            # Log completion\n            self.debug_logger.logger.info(\n                f\"AGENT COMPLETE: Duration={self.execution_stats['total_duration']:.2f}s, \"\n                f\"Tool calls={self.execution_stats['tool_calls']}, \"\n                f\"Errors={self.execution_stats['errors']}\"\n            )\n\n            return response\n\n        except Exception as e:\n            self.execution_stats[\"errors\"] += 1\n            self.debug_logger.log_error(\"Agent Error\", str(e))\n            return f\"I encountered an error: {str(e)}\"\n\n# Example usage of the debuggable agent\ndebuggable_agent = DebuggableAgent(\n    model=\"ollama/deepseek-r1:7b\",\n    temperature=0.2\n)\n\n# Add tools to the agent\n@tool\ndef fetch_data(url: str) -&gt; Dict[str, Any]:\n    \"\"\"Fetch data from a URL\"\"\"\n    # Simulation of fetching data\n    if \"error\" in url:\n        raise ValueError(\"Failed to fetch data: connection error\")\n    return {\"status\": \"success\", \"data\": {\"sample\": \"value\"}}\n\ndebuggable_agent.tools = [fetch_data]\n\n# Test the debuggable agent\nresponse = debuggable_agent.run(\"Can you fetch data from https://example.com/api/data?\")\nprint(f\"Agent response: {response}\")\n\n# Intentionally trigger an error for demonstration\nerror_response = debuggable_agent.run(\"Can you fetch data from https://error.example.com?\")\nprint(f\"Error response: {error_response}\")\n\n# Print execution stats\nprint(f\"Execution stats: {json.dumps(debuggable_agent.execution_stats, indent=2)}\")\n</code></pre>"},{"location":"tutorials/pydanticai/#7-performance-optimization","title":"7. Performance Optimization","text":"<p>When working with local models like DeepSeek R1 through Ollama, performance optimization becomes crucial:</p> <pre><code>from pydantic_ai import Agent, tool\nimport time\nimport psutil\nimport logging\nimport json\nfrom typing import Dict, Any\n\n# Set up performance monitoring logger\nperformance_logger = logging.getLogger(\"agent_performance\")\nperformance_logger.setLevel(logging.INFO)\nfile_handler = logging.FileHandler(\"performance_metrics.log\")\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))\nperformance_logger.addHandler(file_handler)\n\nclass PerformanceMonitor:\n    \"\"\"Monitor and optimize agent performance\"\"\"\n\n    def __init__(self):\n        self.metrics = {\n            \"start_time\": None,\n            \"end_time\": None,\n            \"cpu_percent\": [],\n            \"memory_percent\": [],\n            \"response_times\": []\n        }\n\n    def start_monitoring(self):\n        \"\"\"Start monitoring system resources\"\"\"\n        self.metrics[\"start_time\"] = time.time()\n        self._monitor_resources_thread = self._start_resource_monitoring()\n\n    def _start_resource_monitoring(self):\n        \"\"\"Start a thread to monitor CPU and memory usage\"\"\"\n        import threading\n\n        def monitor():\n            while time.time() - self.metrics[\"start_time\"] &lt; 120:  # Monitor for 2 minutes max\n                self.metrics[\"cpu_percent\"].append(psutil.cpu_percent())\n                self.metrics[\"memory_percent\"].append(psutil.virtual_memory().percent)\n                time.sleep(0.5)\n\n        thread = threading.Thread(target=monitor)\n        thread.daemon = True\n        thread.start()\n        return thread\n\n    def record_response_time(self, duration: float):\n        \"\"\"Record an individual response time\"\"\"\n        self.metrics[\"response_times\"].append(duration)\n\n    def stop_monitoring(self):\n        \"\"\"Stop monitoring and save metrics\"\"\"\n        self.metrics[\"end_time\"] = time.time()\n        total_duration = self.metrics[\"end_time\"] - self.metrics[\"start_time\"]\n\n        # Summarize the metrics\n        summary = {\n            \"total_duration\": total_duration,\n            \"avg_response_time\": sum(self.metrics[\"response_times\"]) / len(self.metrics[\"response_times\"]) if self.metrics[\"response_times\"] else 0,\n            \"max_response_time\": max(self.metrics[\"response_times\"]) if self.metrics[\"response_times\"] else 0,\n            \"avg_cpu_percent\": sum(self.metrics[\"cpu_percent\"]) / len(self.metrics[\"cpu_percent\"]) if self.metrics[\"cpu_percent\"] else 0,\n            \"max_cpu_percent\": max(self.metrics[\"cpu_percent\"]) if self.metrics[\"cpu_percent\"] else 0,\n            \"avg_memory_percent\": sum(self.metrics[\"memory_percent\"]) / len(self.metrics[\"memory_percent\"]) if self.metrics[\"memory_percent\"] else 0,\n            \"request_count\": len(self.metrics[\"response_times\"])\n        }\n\n        # Log the summary\n        performance_logger.info(f\"Performance Summary: {json.dumps(summary, indent=2)}\")\n        return summary\n\nclass OptimizedAgent(Agent):\n    \"\"\"An agent optimized for performance with local LLMs\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        # Set optimal parameters for local LLM usage\n        kwargs.setdefault(\"temperature\", 0.2)  # Lower temperature for faster, more consistent responses\n        kwargs.setdefault(\"max_tokens\", 500)   # Limit token generation for speed\n\n        super().__init__(*args, **kwargs)\n        self.performance_monitor = PerformanceMonitor()\n        self.performance_monitor.start_monitoring()\n\n    def get_system_prompt(self):\n        # Optimized prompt - shorter prompts process faster\n        return \"\"\"You are a helpful, efficient assistant. Keep responses concise and focused.\n               Answer questions directly or use tools when appropriate.\"\"\"\n\n    def run(self, user_input: str) -&gt; str:\n        start_time = time.time()\n\n        try:\n            response = super().run(user_input)\n            duration = time.time() - start_time\n            self.performance_monitor.record_response_time(duration)\n\n            # Log performance for this request\n            performance_logger.info(f\"Request processed in {duration:.2f}s\")\n\n            return response\n        except Exception as e:\n            duration = time.time() - start_time\n            self.performance_monitor.record_response_time(duration)\n            performance_logger.error(f\"Error processing request: {str(e)}, duration: {duration:.2f}s\")\n            return f\"Error: {str(e)}\"\n\n    def finalize(self):\n        \"\"\"Clean up and display final performance metrics\"\"\"\n        summary = self.performance_monitor.stop_monitoring()\n        print(f\"Agent Performance Summary:\")\n        print(f\"  Total duration: {summary['total_duration']:.2f}s\")\n        print(f\"  Average response time: {summary['avg_response_time']:.2f}s\")\n        print(f\"  Requests processed: {summary['request_count']}\")\n        print(f\"  Avg CPU usage: {summary['avg_cpu_percent']:.1f}%\")\n        print(f\"  Avg memory usage: {summary['avg_memory_percent']:.1f}%\")\n\n# Example usage with DeepSeek through Ollama\noptimized_agent = OptimizedAgent(\n    model=\"ollama/deepseek-r1:7b\",\n    # Additional optimization parameters for Ollama\n    model_kwargs={\n        \"num_ctx\": 2048,      # Smaller context window for faster processing\n        \"repeat_penalty\": 1.1  # Slight penalty to reduce token repetition\n    }\n)\n\n# Run a series of queries to test performance\ntest_queries = [\n    \"What is the capital of France?\",\n    \"Explain how a binary search algorithm works\",\n    \"What are the main features of Python?\",\n    \"Define what an intelligent software agent is\",\n    \"What's the difference between supervised and unsupervised learning?\"\n]\n\nfor query in test_queries:\n    print(f\"\\nQuery: {query}\")\n    response = optimized_agent.run(query)\n    print(f\"Response: {response[:100]}...\")  # Truncated for brevity\n\n# Get final performance metrics\noptimized_agent.finalize()\n</code></pre>"},{"location":"tutorials/pydanticai/#8-real-world-example-building-a-code-agent","title":"8. Real-World Example: Building a Code Agent","text":"<p>Now let's build a more complex, practical agent for a real-world use case - a code assistant agent using PydanticAI that can help with Python programming tasks:</p> <pre><code>from pydantic_ai import Agent, tool\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Any, Optional\nimport logging\nimport os\nimport json\nimport subprocess\nimport time\n\n# Advanced logging configuration\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(\"code_agent.log\"),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(\"code_agent\")\n\n# Tool input/output models\nclass CodeGenerationRequest(BaseModel):\n    \"\"\"Request for generating code based on a description\"\"\"\n    description: str = Field(..., description=\"Description of what the code should do\")\n    language: str = Field(\"python\", description=\"Programming language to use\")\n    libraries: List[str] = Field(default_factory=list, description=\"List of libraries to use\")\n\nclass CodeGenerationResponse(BaseModel):\n    \"\"\"Response containing the generated code\"\"\"\n    code: str = Field(..., description=\"The generated code\")\n    explanation: str = Field(..., description=\"Explanation of how the code works\")\n\nclass CodeExecutionRequest(BaseModel):\n    \"\"\"Request for executing code\"\"\"\n    code: str = Field(..., description=\"Code to execute\")\n    input_data: Optional[Dict[str, Any]] = Field(None, description=\"Input data for code execution\")\n\nclass CodeExecutionResponse(BaseModel):\n    \"\"\"Response from code execution\"\"\"\n    output: str = Field(..., description=\"Output from code execution\")\n    success: bool = Field(..., description=\"Whether execution was successful\")\n    error: Optional[str] = Field(None, description=\"Error message if execution failed\")\n    execution_time: float = Field(..., description=\"Time taken to execute the code in seconds\")\n\nclass CodeReviewRequest(BaseModel):\n    \"\"\"Request for reviewing code\"\"\"\n    code: str = Field(..., description=\"Code to review\")\n    focus_areas: Optional[List[str]] = Field(None, description=\"Areas to focus on in the review\")\n\nclass CodeReviewResponse(BaseModel):\n    \"\"\"Response from code review\"\"\"\n    feedback: List[str] = Field(..., description=\"List of feedback points\")\n    suggestions: List[str] = Field(..., description=\"List of suggestions for improvement\")\n    overall_rating: int = Field(..., description=\"Rating from 1-10\")\n\n# Define tools for our code agent\n@tool\ndef generate_code(request: CodeGenerationRequest) -&gt; CodeGenerationResponse:\n    \"\"\"\n    Generate code based on a description.\n\n    Args:\n        request: Contains the description of what the code should do, language, and libraries\n\n    Returns:\n        Generated code and explanation\n    \"\"\"\n    logger.info(f\"Generating {request.language} code for: {request.description}\")\n\n    start_time = time.time()\n\n    # In a real implementation, you might use a code-specialized model here\n    # This is a simplified example\n    code = f\"# {request.language} code for: {request.description}\\n\"\n\n    if request.language.lower() == \"python\":\n        # Add imports\n        if request.libraries:\n            for lib in request.libraries:\n                code += f\"import {lib}\\n\"\n            code += \"\\n\"\n\n        # Add a simple function template\n        code += f\"def main():\\n\"\n        code += f\"    # TODO: Implement {request.description}\\n\"\n        code += f\"    pass\\n\\n\"\n        code += f\"if __name__ == '__main__':\\n\"\n        code += f\"    main()\\n\"\n\n    duration = time.time() - start_time\n    logger.debug(f\"Code generation took {duration:.2f}s\")\n\n    return CodeGenerationResponse(\n        code=code,\n        explanation=f\"This is a basic template for {request.description}. It includes the necessary imports and a main function structure.\"\n    )\n\n@tool\ndef execute_code(request: CodeExecutionRequest) -&gt; CodeExecutionResponse:\n    \"\"\"\n    Execute Python code in a safe environment and return the output.\n\n    Args:\n        request: Contains the code to execute and optional input data\n\n    Returns:\n        Execution output, success status, any error messages, and execution time\n    \"\"\"\n    logger.info(\"Executing code\")\n    logger.debug(f\"Code to execute: {request.code[:100]}...\")\n\n    # Create a temporary Python file\n    temp_file = \"temp_execution.py\"\n    with open(temp_file, \"w\") as f:\n        f.write(request.code)\n\n    start_time = time.time()\n\n    try:\n        # Execute in a controlled environment\n        # Note: In production, you should use a proper sandbox\n        result = subprocess.run(\n            [\"python\", temp_file],\n            capture_output=True,\n            text=True,\n            timeout=10  # Limit execution time\n        )\n\n        success = result.returncode == 0\n        output = result.stdout if success else result.stderr\n        error = None if success else result.stderr\n\n    except subprocess.TimeoutExpired:\n        success = False\n        output = \"Execution timed out after 10 seconds\"\n        error = \"Timeout error\"\n    except Exception as e:\n        success = False\n        output = f\"Error executing code: {str(e)}\"\n        error = str(e)\n    finally:\n        # Clean up\n        if os.path.exists(temp_file):\n            os.remove(temp_file)\n\n    execution_time = time.time() - start_time\n    logger.info(f\"Code execution completed in {execution_time:.2f}s with success={success}\")\n\n    return CodeExecutionResponse(\n        output=output,\n        success=success,\n        error=error,\n        execution_time=execution_time\n    )\n\n@tool\ndef review_code(request: CodeReviewRequest) -&gt; CodeReviewResponse:\n    \"\"\"\n    Review code and provide feedback for improvement.\n\n    Args:\n        request: Contains the code to review and optional focus areas\n\n    Returns:\n        Feedback, suggestions, and overall rating\n    \"\"\"\n    logger.info(\"Reviewing code\")\n\n    # In a real implementation, you would analyze the code\n    # This is a simplified example\n    feedback = [\"Code structure is clean\", \"Good use of comments\"]\n    suggestions = [\"Consider adding error handling\", \"Add type hints for better readability\"]\n    rating = 7\n\n    focus_areas = request.focus_areas or []\n    if \"performance\" in focus_areas:\n        feedback.append(\"No obvious performance issues detected\")\n        suggestions.append(\"Consider using list comprehensions for better performance\")\n\n    if \"security\" in focus_areas:\n        feedback.append(\"No obvious security vulnerabilities detected\")\n        suggestions.append(\"Validate user inputs to prevent injection attacks\")\n\n    return CodeReviewResponse(\n        feedback=feedback,\n        suggestions=suggestions,\n        overall_rating=rating\n    )\n\n# Create our code assistant agent\nclass CodeAssistantAgent(Agent):\n    \"\"\"An agent specialized in code generation, execution, and review\"\"\"\n\n    def get_system_prompt(self):\n        return \"\"\"You are a Python programming assistant with expertise in software development.\n                 You can help users by generating code, executing it, and providing code reviews.\n                 When helping users with code:\n                 1. Understand their requirements clearly\n                 2. Generate appropriate code using the generate_code tool\n                 3. Test the code using the execute_code tool if needed\n                 4. Provide explanations and improvements using the review_code tool\n\n                 Keep your explanations clear and focused on the user's needs.\n                 \"\"\"\n\n    # Register tools\n    tools = [generate_code, execute_code, review_code]\n\n# Initialize the agent\ncode_assistant = CodeAssistantAgent(\n    model=\"ollama/deepseek-r1:7b\",\n    temperature=0.3,\n    max_tokens=1000\n)\n\n# Example usage\ntest_requests = [\n    \"Can you write a Python function to calculate the Fibonacci sequence?\",\n    \"I need a script to read a CSV file and calculate the average of a column\",\n    \"Can you help me understand how decorators work in Python?\"\n]\n\nfor request in test_requests:\n    print(f\"\\n\\n===== PROCESSING REQUEST: {request} =====\\n\")\n    response = code_assistant.run(request)\n    print(response)\n</code></pre>"},{"location":"tutorials/pydanticai/#9-troubleshooting-common-issues","title":"9. Troubleshooting Common Issues","text":"<p>Here are solutions for common PydanticAI issues:</p>"},{"location":"tutorials/pydanticai/#issue-1-model-integration-problems","title":"Issue 1: Model Integration Problems","text":"<p><pre><code># Troubleshooting model connection issues\nfrom pydantic_ai import Agent\nimport logging\n\n# Enable verbose logging\nlogging.basicConfig(level=logging.DEBUG)\n\ndef test_model_connection(model_name):\n    \"\"\"Test connection to an LLM model\"\"\"\n    try:\n        agent = Agent(model=model_name)\n        test_response = agent.run(\"Hello, are you working correctly?\")\n        print(f\"Response from {model_name}: {test_response}\")\n        return True\n    except Exception as e:\n        print(f\"Error connecting to {model_name}: {str(e)}\")\n        return False\n\n# Test different model configurations\nmodels_to_test = [\n    \"openai/gpt-3.5-turbo\",  # OpenAI model\n    \"ollama/deepseek-r1:7b\", # Local Ollama model\n    \"anthropic/claude-3-sonnet\"  # Anthropic model\n]\n\nfor model in models_to_test:\n    print(f\"\\nTesting connection to {model}...\")\n    success = test_model_connection(model)\n    print(f\"Connection test {'succeeded' if success else 'failed'}\")\n</code></pre> <pre><code>### Issue 2: Tool Execution Failures\n\n```python\nfrom pydantic_ai import Agent, tool\nfrom pydantic import BaseModel, ValidationError\nimport logging\nimport traceback\n\n# Set up detailed logging for tool execution\nlogging.basicConfig(level=logging.DEBUG)\ntool_logger = logging.getLogger(\"tool_execution\")\n\n# Wrapper to debug tool execution issues\ndef debug_tool_execution(tool_func):\n    \"\"\"Decorator to add debugging to tool execution\"\"\"\n    def wrapper(*args, **kwargs):\n        tool_logger.debug(f\"Executing tool: {tool_func.__name__}\")\n        tool_logger.debug(f\"Arguments: {args}\")\n        tool_logger.debug(f\"Keyword arguments: {kwargs}\")\n        try:\n            result = tool_func(*args, **kwargs)\n            tool_logger.debug(f\"Tool execution successful\")\n            tool_logger.debug(f\"Result: {result}\")\n            return result\n        except ValidationError as ve:\n            tool_logger.error(f\"Validation error in tool {tool_func.__name__}: {str(ve)}\")\n            tool_logger.error(f\"Validation error details: {ve.errors()}\")\n            raise\n        except Exception as e:\n            tool_logger.error(f\"Error executing tool {tool_func.__name__}: {str(e)}\")\n            tool_logger.error(f\"Traceback: {traceback.format_exc()}\")\n            raise\n    return wrapper\n\n# Example of using the debug wrapper with a tool\nclass MathRequest(BaseModel):\n    expression: str\n\n@tool\n@debug_tool_execution\ndef calculate(request: MathRequest) -&gt; float:\n    \"\"\"Calculate the result of a mathematical expression\"\"\"\n    return eval(request.expression)\n\n# Test the tool directly\ntry:\n    result = calculate(MathRequest(expression=\"2 + 2\"))\n    print(f\"Calculation result: {result}\")\n\n    # Intentionally trigger an error\n    result = calculate(MathRequest(expression=\"2 / 0\"))\nexcept Exception as e:\n    print(f\"Caught error: {str(e)}\")\n</code></pre></p>"},{"location":"tutorials/pydanticai/#issue-3-llm-context-management","title":"Issue 3: LLM Context Management","text":"<pre><code>from pydantic_ai import Agent\nfrom typing import List, Dict, Any\nimport logging\nimport json\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\ncontext_logger = logging.getLogger(\"context_management\")\n\nclass ContextManager:\n    \"\"\"Helper class to manage LLM context size and debug context issues\"\"\"\n\n    def __init__(self, max_tokens=4000):\n        self.max_tokens = max_tokens\n        self.conversation_history: List[Dict[str, Any]] = []\n        self.estimated_token_count = 0\n\n    def add_message(self, role: str, content: str):\n        \"\"\"Add a message to the conversation history\"\"\"\n        # Rough estimation: 1 token \u2248 4 characters\n        estimated_tokens = len(content) // 4\n\n        message = {\"role\": role, \"content\": content}\n        self.conversation_history.append(message)\n        self.estimated_token_count += estimated_tokens\n\n        context_logger.info(f\"Added {role} message with ~{estimated_tokens} tokens\")\n        context_logger.info(f\"Current estimated token count: {self.estimated_token_count}/{self.max_tokens}\")\n\n        # If we're approaching the limit, summarize older messages\n        if self.estimated_token_count &gt; self.max_tokens * 0.8:\n            context_logger.warning(f\"Approaching token limit, summarizing conversation\")\n            self._summarize_history()\n\n    def _summarize_history(self):\n        \"\"\"Summarize older conversation messages to reduce token count\"\"\"\n        if len(self.conversation_history) &lt;= 4:\n            return  # Keep at least the last few messages\n\n        # Extract messages to summarize (all but the last 4)\n        to_summarize = self.conversation_history[:-4]\n\n        # Create a summary message\n        summary_content = f\"[Summary of {len(to_summarize)} previous messages]\"\n        summary_message = {\"role\": \"system\", \"content\": summary_content}\n\n        # Replace old messages with summary\n        self.conversation_history = [summary_message] + self.conversation_history[-4:]\n\n        # Recalculate token count\n        self.estimated_token_count = sum(len(msg[\"content\"]) // 4 for msg in self.conversation_history)\n        context_logger.info(f\"Summarized conversation history. New token count: {self.estimated_token_count}\")\n\n    def get_formatted_history(self) -&gt; str:\n        \"\"\"Get the conversation history formatted for the LLM prompt\"\"\"\n        formatted = \"\"\n        for msg in self.conversation_history:\n            formatted += f\"{msg['role'].upper()}: {msg['content']}\\n\\n\"\n        return formatted\n\n    def debug_token_usage(self):\n        \"\"\"Print detailed token usage for debugging\"\"\"\n        context_logger.debug(\"==== Token Usage Breakdown ====\")\n        for i, msg in enumerate(self.conversation_history):\n            tokens = len(msg[\"content\"]) // 4\n            percent = (tokens / self.estimated_token_count) * 100 if self.estimated_token_count &gt; 0 else 0\n            context_logger.debug(f\"Message {i+1} ({msg['role']}): ~{tokens} tokens ({percent:.1f}%)\")\n        context_logger.debug(f\"Total: ~{self.estimated_token_count} tokens\")\n\n# Example usage with an agent\nclass ContextAwareAgent(Agent):\n    \"\"\"An agent that is aware of context limitations\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.context_manager = ContextManager(max_tokens=4000)\n\n    def get_system_prompt(self):\n        # Include conversation history in the prompt\n        history = self.context_manager.get_formatted_history()\n\n        return f\"\"\"You are a helpful assistant with memory of our conversation.\n\nPrevious conversation:\n{history}\n\nRespond to the user's latest message.\n\"\"\"\n\n    def run(self, user_input: str) -&gt; str:\n        # Add the user message to history\n        self.context_manager.add_message(\"user\", user_input)\n\n        # Run the agent\n        response = super().run(user_input)\n\n        # Add the agent response to history\n        self.context_manager.add_message(\"assistant\", response)\n\n        # Debug token usage\n        self.context_manager.debug_token_usage()\n\n        return response\n\n# Test the context-aware agent\ncontext_agent = ContextAwareAgent(model=\"ollama/deepseek-r1:7b\")\n\n# Simulate a conversation\nresponses = []\nfor i in range(5):\n    user_message = f\"This is test message {i+1}. Tell me something interesting about Python programming.\"\n    print(f\"\\n&gt;&gt; USER: {user_message}\")\n\n    response = context_agent.run(user_message)\n    responses.append(response)\n\n    print(f\"&lt;&lt; ASSISTANT: {response[:100]}...\")\n</code></pre>"},{"location":"tutorials/pydanticai/#deploying-in-production","title":"Deploying in Production","text":"<p>For production deployment of your PydanticAI agents, consider these best practices:</p> <pre><code>from pydantic_ai import Agent, tool\nfrom fastapi import FastAPI, BackgroundTasks, HTTPException\nfrom pydantic import BaseModel\nimport logging\nimport time\nimport uuid\nimport json\nfrom typing import Dict, Any, List, Optional\n\n# Set up production-ready logging\nlogging.config.dictConfig({\n    'version': 1,\n    'formatters': {\n        'default': {\n            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        }\n    },\n    'handlers': {\n        'console': {\n            'class': 'logging.StreamHandler',\n            'level': 'INFO',\n            'formatter': 'default',\n        },\n        'file': {\n            'class': 'logging.handlers.RotatingFileHandler',\n            'level': 'DEBUG',\n            'formatter': 'default',\n            'filename': 'production.log',\n            'maxBytes': 10485760,  # 10MB\n            'backupCount': 5,\n        }\n    },\n    'root': {\n        'level': 'INFO',\n        'handlers': ['console', 'file']\n    }\n})\n\nlogger = logging.getLogger(\"production_agent\")\n\n# Define API models\nclass AgentRequest(BaseModel):\n    message: str\n    session_id: Optional[str] = None\n\nclass AgentResponse(BaseModel):\n    response: str\n    session_id: str\n    request_id: str\n    processing_time: float\n\n# Create a production agent with monitoring and error handling\nclass ProductionAgent(Agent):\n    \"\"\"Production-ready agent with error handling and monitoring\"\"\"\n\n    def __init__(self, *args, fallback_response=\"I'm sorry, I'm having trouble processing that request right now. Please try again in a moment.\", **kwargs):\n        super().__init__(*args, **kwargs)\n        self.fallback_response = fallback_response\n        self.sessions: Dict[str, Dict[str, Any]] = {}\n\n    def get_system_prompt(self, session_id=None):\n        # Get session-specific prompt if available\n        if session_id and session_id in self.sessions:\n            return self.sessions[session_id].get(\"system_prompt\", self._default_system_prompt())\n        return self._default_system_prompt()\n\n    def _default_system_prompt(self):\n        return \"\"\"You are a helpful, accurate, and professional assistant.\n               Provide clear and concise responses to user inquiries.\"\"\"\n\n    def run_with_monitoring(self, user_input: str, session_id: Optional[str] = None) -&gt; Dict[str, Any]:\n        \"\"\"Run the agent with comprehensive monitoring and error handling\"\"\"\n        start_time = time.time()\n        request_id = str(uuid.uuid4())\n\n        # Create session if it doesn't exist\n        if session_id and session_id not in self.sessions:\n            self.sessions[session_id] = {\"created_at\": time.time(), \"requests\": 0}\n\n        # Update session stats\n        if session_id:\n            self.sessions[session_id][\"requests\"] = self.sessions[session_id].get(\"requests\", 0) + 1\n            self.sessions[session_id][\"last_activity\"] = time.time()\n\n        logger.info(f\"Processing request {request_id} for session {session_id}\")\n\n        try:\n            # Run the agent with the session-specific prompt\n            original_get_system_prompt = self.get_system_prompt\n            self.get_system_prompt = lambda: self.get_system_prompt(session_id)\n\n            response = super().run(user_input)\n\n            # Restore original method\n            self.get_system_prompt = original_get_system_prompt\n\n            processing_time = time.time() - start_time\n            logger.info(f\"Request {request_id} processed in {processing_time:.2f}s\")\n\n            # Log performance metrics (could be sent to monitoring system)\n            if processing_time &gt; 5.0:\n                logger.warning(f\"Slow response detected: {processing_time:.2f}s for request {request_id}\")\n\n            return {\n                \"response\": response,\n                \"session_id\": session_id or request_id,\n                \"request_id\": request_id,\n                \"processing_time\": processing_time,\n                \"status\": \"success\"\n            }\n\n        except Exception as e:\n            processing_time = time.time() - start_time\n            logger.error(f\"Error processing request {request_id}: {str(e)}\", exc_info=True)\n\n            return {\n                \"response\": self.fallback_response,\n                \"session_id\": session_id or request_id,\n                \"request_id\": request_id,\n                \"processing_time\": processing_time,\n                \"status\": \"error\",\n                \"error\": str(e)\n            }\n\n# Initialize FastAPI\napp = FastAPI(title=\"PydanticAI Agent API\")\n\n# Initialize the production agent\nproduction_agent = ProductionAgent(\n    model=\"ollama/deepseek-r1:7b\",\n    # For high-volume production, consider using a cloud model with higher throughput\n    # model=\"openai/gpt-4-turbo\",\n    temperature=0.2,\n    max_tokens=1000\n)\n\n# Add tools for the production agent\n@tool\ndef search_knowledge_base(query: str) -&gt; List[Dict[str, str]]:\n    \"\"\"Search the knowledge base for information\"\"\"\n    # In production, this would connect to your actual knowledge base\n    return [{\"title\": \"Sample result\", \"content\": \"This is a sample search result\"}]\n\nproduction_agent.tools = [search_knowledge_base]\n\n# API endpoints\n@app.post(\"/api/agent\", response_model=AgentResponse)\nasync def query_agent(request: AgentRequest, background_tasks: BackgroundTasks):\n    \"\"\"Process an agent request synchronously\"\"\"\n    result = production_agent.run_with_monitoring(request.message, request.session_id)\n\n    # Schedule cleanup in the background\n    background_tasks.add_task(cleanup_old_sessions)\n\n    if result[\"status\"] == \"error\":\n        logger.error(f\"Error in request {result['request_id']}: {result.get('error')}\")\n\n    return AgentResponse(\n        response=result[\"response\"],\n        session_id=result[\"session_id\"],\n        request_id=result[\"request_id\"],\n        processing_time=result[\"processing_time\"]\n    )\n\nasync def cleanup_old_sessions():\n    \"\"\"Clean up inactive sessions\"\"\"\n    current_time = time.time()\n    inactive_threshold = 3600  # 1 hour\n\n    inactive_sessions = [\n        session_id for session_id, data in production_agent.sessions.items()\n        if current_time - data.get(\"last_activity\", 0) &gt; inactive_threshold\n    ]\n\n    for session_id in inactive_sessions:\n        del production_agent.sessions[session_id]\n        logger.info(f\"Cleaned up inactive session {session_id}\")\n\n# Health check endpoint\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint for monitoring\"\"\"\n    return {\"status\": \"healthy\", \"timestamp\": time.time()}\n</code></pre>"},{"location":"tutorials/pydanticai/#conclusion","title":"Conclusion","text":"<p>PydanticAI offers a well-structured, efficient framework for building AI agents that balances simplicity with power. Key takeaways from this tutorial:</p> <ol> <li>Start Simple: Begin with basic agents and gradually add complexity as needed.</li> <li>Documentation is Key: Well-documented tools with clear Pydantic models help LLMs understand how to use them correctly.</li> <li>Debugging is Essential: Implement comprehensive logging and monitoring from the start to identify and fix issues quickly.</li> <li>Performance Matters: Optimize your agents for local LLM deployment when running models like DeepSeek R1 with Ollama.</li> <li>Test Thoroughly: Verify agent behavior across a range of inputs before deploying to production.</li> </ol> <p>Remember that agent development is an iterative process. Start with a minimal viable agent, test it thoroughly, and progressively enhance its capabilities based on real-world performance and user feedback.</p> <p>As you build more complex agent systems, consider implementing the ReAct framework to interleave reasoning and action, and explore multi-agent architectures for tasks that benefit from specialized capabilities working together.</p> <p>Happy agent building!</p>"}]}