{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Building Intelligent Agents with AI","text":"<p>Welcome to our course on building intelligent agents with AI.  This course is unique in that it teaches you how to use generative AI to create intelligent agents that run on your local computer.</p> <p>This website is based on our intelligent textbook</p> <p>Please contact me on LinkedIn if you have any questions about this textbook.</p> <p>Thanks! - Dan</p>"},{"location":"about/","title":"About this Course","text":"<p>This course is designed for individuals that would like to learn the concepts involved in using and building intelligent software agents.</p>"},{"location":"about/#prerequisites","title":"Prerequisites","text":"<ul> <li>Be curious about what software agents are and how organizations are using them</li> <li>Have some knowledge of using generative AI tools such as ChatGPT, Anthropic Clause or similar tools.</li> <li>Have some experience with creating precise prompts and correcting prompts to get the desired outcome</li> <li>Be somewhat familiar with basic Python programming including the steps to setup a Python</li> <li>Have access to the terminal or shell on your local computer or a remote host</li> </ul>"},{"location":"about/#related-course-for-beginners","title":"Related Course for Beginners","text":"<p>If you don't have this background we recommend our Prompt Engineering course.</p> <p>If you are new to Python, we have a fun interactive Introduction to Python course.</p> <p>If you want to take a deep dive into deep learning is our Deep Learning Course.</p>"},{"location":"contact/","title":"Contact","text":"<p>Please contact me on LinkedIn</p> <p>Thanks! - Dan</p>"},{"location":"feedback/","title":"Feedback on Graph Data Modeling","text":"<p>You are welcome to connect with me on anytime on LinkedIn or submit any issues to GitHub Issue Log.  All pull-requests with fixes to errors or additions are always welcome.</p> <p>If you would like to fill out a short survey and give us ideas on how we can create better tools for intelligent textbooks in the future.</p>"},{"location":"glossary/","title":"Glossary of Terms","text":""},{"location":"glossary/#action","title":"Action","text":"<p>The process of calling a specific tool directly from within an agent.</p> <p>In a multi-step agent, at each step, the LLM can write an action, in the form of some calls to external tools.  Tools much be carefully described so that agents can match their capabilities with the needs of a the user.</p> <p>A common format (used by Anthropic, OpenAI, and many others) for writing these actions is generally different variations of \"writing actions as a JSON of tools names, description and arguments to use, which you then parse to know which tool to execute and with which arguments\".</p>"},{"location":"glossary/#agent","title":"Agent","text":"<p>An autonomous computational entity that performs tasks on behalf of users or other programs with some degree of independence or autonomy, and in doing so, employs a precise knowledge representation of the user's goals or desires.</p> <p>Agents are similar to the use of complex rules engines but they have a higher degree of independence or autonomy.</p> <ul> <li>See also: Rules Engine</li> </ul>"},{"location":"glossary/#agntcy","title":"Agntcy","text":"<p>A set of proposed standards for Agent interoperability.</p> <ul> <li>See Agntcy on Perplexity</li> </ul>"},{"location":"glossary/#business-process-modeling-notation","title":"Business Process Modeling Notation","text":"<p>BPMN References Guide</p>"},{"location":"glossary/#decorator","title":"Decorator","text":"<p>A decorator is a design pattern implemented as a special syntax that allows a function, method, or class to be modified or extended without changing its source code. </p> <p>In this course, we use Python decorators with the @ symbol followed by a decorator name such as <code>@tool</code>  placed above the definition of the function to be decorated. </p> <p>They effectively wrap the target function, enabling pre-processing of arguments, post-processing of return values, modification of behavior, or registration within a larger system.</p>"},{"location":"glossary/#deepseek-r1","title":"DeepSeek R1","text":"<p>A large language model developed in China that uses an innovative architecture that combines mixture of experts with reinforcement learning to create smaller models at lower cost.</p> <p>In this course we use a 7 billion parameter variant of DeepSeek R1 created by the Ollama system that runs on a local GPU with only 12GB RAM but still generates over 50 tokens per second.</p> <ul> <li>See Also DeepSeek R1 in Concepts</li> </ul>"},{"location":"glossary/#iso-definition","title":"ISO Definition","text":"<p>A term definition is considered to be consistent with ISO metadata registry guideline 11179 if it meets the following criteria:</p> <ol> <li>Precise</li> <li>Concise</li> <li>Distinct</li> <li>Non-circular</li> <li>Unencumbered with business rules</li> </ol>"},{"location":"glossary/#docstring","title":"Docstring","text":"<p>A docstring in Python is a string literal that appears as the first statement in a module, function, class, or method definition.</p> <p>Python docstrings are enclosed by triple quotes (<code>\"\"\"</code> or <code>'''</code>) and can span multiple lines.</p> <p>Docstrings combined with the <code>@tool</code> decorator are used to document the purpose, behavior, parameters, and return values of the code object. </p> <p>This approach allows agents to understand not just what functions are available, but when and how to use them appropriately based on their documented purpose and parameters.</p>"},{"location":"glossary/#goal-driven-process-gdp","title":"Goal-Driven Process (GDP)","text":"<p>In the context of the Software Development Life Cycle (SDLC), GDP typically refers to \"Goal-Driven Process\" or \"Goal-Directed Planning.\" This is a methodological approach within software development that emphasizes:</p> <p>Defining clear goals and objectives for the software project Aligning development processes with these established goals Making decisions throughout the development lifecycle based on how they contribute to achieving the defined goals</p> <p>The GDP approach within SDLC helps teams:</p> <ul> <li>Maintain focus on delivering value that aligns with business or user objectives</li> <li>Prioritize features and development tasks based on goal contribution</li> <li>Establish measurable criteria for evaluating progress and success</li> <li>Create a framework for making consistent decisions when trade-offs are necessary</li> </ul> <p>This approach can be implemented across various SDLC methodologies, including Agile, Waterfall, or hybrid approaches, as it's more about the strategic alignment of development efforts rather than the specific technical implementation process.</p>"},{"location":"glossary/#graphics-processing-unit","title":"Graphics Processing Unit","text":"<p>A specialized device designed to accelerate matrix operations using parallel processing.</p> <p>In the context of intelligent software agents, GPUs are relevant as they provide parallel processing capabilities that significantly accelerate machine learning model training and inference operations, enabling more complex and performant AI systems. The parallel architecture of GPUs allows for efficient matrix operations that form the computational backbone of many neural network implementations.</p> <p>Example: When deploying a code agent for real-time coding tasks, a developer would typically configure the environment to utilize a GPU to ensure the agent can perform inference operations at sufficient speed, rather than relying solely on CPU processing.</p>"},{"location":"glossary/#intelligent-textbook","title":"Intelligent Textbook","text":"<p>An interactive digital learning resource that employs artificial intelligence to generate personalized educational content and adaptive learning experiences in response to individual student needs and progress.</p> <p>In the context of intelligent software agents, intelligent textbooks represent an advanced application domain where multiple specialized agents collaborate to deliver customized educational experiences. These agents work together to analyze student performance, identify knowledge gaps, generate appropriate lesson content, create interactive simulations, and adapt teaching methodologies in real-time. The intelligent textbook serves as both a platform for agent integration and an interface through which students interact with these educational AI systems.</p> <p>Example: A computer science student using an intelligent textbook to learn about data structures might receive dynamically generated lesson content on binary trees based on their previous interaction patterns, followed by an interactive coding simulation that adjusts its complexity based on the student's demonstrated proficiency, all orchestrated by specialized AI agents working in concert to optimize the learning experience.</p>"},{"location":"glossary/#integrated-development-environment","title":"Integrated Development Environment","text":"<p>A software application that provides comprehensive facilities to programmers for software development, combining multiple development tools into a single graphical user interface.</p> <p>In the context of intelligent software agents, IDEs are relevant as they provide environments where code agents can be integrated to assist developers with tasks such as code completion, error detection, refactoring suggestions, and automated documentation. Modern IDEs often incorporate AI-powered features that enhance developer productivity by leveraging language models to understand code context and provide intelligent assistance.</p> <p>Example: A developer working with a code agent might configure their IDE to seamlessly interact with the agent, allowing it to analyze code as it's being written, suggest improvements based on best practices, and even generate test cases automatically based on the function implementations.</p> <ul> <li>Codium Windsurf</li> <li>Lovable</li> <li>Cursor</li> </ul>"},{"location":"glossary/#knowledge-representation","title":"Knowledge Representation","text":"<p>The methods and structures used to encode information about the world.</p> <p>In this book, we focus on knowledge representation in a form that an agent can utilize for reasoning and decision-making. It provides the foundation for an agent's understanding of its environment, goals, and possible actions.</p> <p>Knowledge representation encompasses:</p> <ol> <li> <p>Symbolic Structures: Formalized frameworks such as ontologies, semantic networks, frames, rules, or logic-based systems that capture entities, relationships, and constraints in the agent's domain.</p> </li> <li> <p>Information Organization: Methods for structuring knowledge in ways that facilitate efficient access, inference, and updates as the agent interacts with its environment.</p> </li> <li> <p>Reasoning Mechanisms: Techniques that allow agents to derive new knowledge from existing representations, evaluate truth values, and make decisions based on incomplete information.</p> </li> <li> <p>Semantic Grounding: Connections between abstract symbols and their real-world referents, allowing the agent to map internal representations to external phenomena.</p> </li> </ol> <p>Effective knowledge representation systems in agents balance expressiveness (the ability to represent complex knowledge) with computational efficiency (allowing the agent to reason within reasonable time and resource constraints).</p>"},{"location":"glossary/#langchain","title":"LangChain","text":"<p>An open-source framework designed to simplify the development of applications that leverages large language models (LLMs). </p> <p>LangChain provides a standardized interface for chaining together different components needed for LLM applications, such as prompt templates, language models, memory systems, and external tools or data sources.</p> <p>Key features of LangChain include:</p> <ol> <li>Chains: Sequences of operations that combine prompts, models, and other components to perform complex tasks.</li> <li>Agents: Systems that use LLMs as reasoning engines to determine which actions to take based on user input and tool feedback.</li> <li>Memory: Components that allow applications to maintain conversational state and context over interactions.</li> <li>Retrieval Augmented Generation (RAG): Tools for connecting LLMs to external knowledge sources and databases to enhance responses with specific information.</li> <li>Tool integration: Standardized methods for connecting LLMs to external APIs, databases, and computational resources.</li> </ol> <p>LangChain helps developers build sophisticated applications like chatbots, question-answering systems, summarization tools, and other AI assistants by providing reusable components and patterns that work across different LLM providers.</p> <p>The main disadvantage of LangChain is a long learning curve.  To use LangChain you must master several abstractions.</p>"},{"location":"glossary/#large-language-model","title":"Large Language Model","text":"<p>A deep learning model that is trained on natural language and used to predict the next token.</p> <p>Examples of LLMs include BERT, DeepSeek, GPTs and Llams.</p>"},{"location":"glossary/#llm-agents","title":"LLM Agents","text":"<p>A specialized type of software agent where LLM outputs control the workflow.</p> <p>Note that \"agency\" evolves on a continuous spectrum, as you give more or less power to the LLM on your workflow.</p>"},{"location":"glossary/#llm-agent-control-levels","title":"LLM Agent Control Levels","text":"<p>The amount you allow an LLM to control agentic execution. Her are some sample levels:</p> <ol> <li>Level 0 - LLMs have no control over workflows</li> <li>Level 1 - LLMs can impact an if/then/else decision within workflows</li> <li>Level 2 - LLMs determine what functions should be called</li> <li>Level 3 - LLMs control iteration, and program order and continuation</li> <li>Level 4 - LLMs can start another agentic workflow</li> <li>Level 5 - LLMs can generate new code and run it</li> </ol> <p>See also: HuggingFace Agent Definition</p>"},{"location":"glossary/#model-context-protocol-mcp","title":"Model Context Protocol (MCP)","text":"<p>An open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.</p> <ul> <li>Model Context Protocol</li> <li>Cursor Model Content Protocol</li> </ul>"},{"location":"glossary/#ollama","title":"Ollama","text":"<p>A software framework that enables deployment and execution of large language models (LLMs) locally on personal computing devices, designed to optimize resource utilization and provide standardized interfaces for model management, inference, and integration with agent-based systems.</p> <p>We use Ollama in this course to enable us to run LLMs on a local GPU.</p>"},{"location":"glossary/#private-knowledge","title":"Private Knowledge","text":"<p>Knowledge that is not publicly available.</p> <p>For organizations, private knowledge usually is stored on an intranet or private company databases such as ERP and CRM systems.</p> <p>Software Agents must combine both public knowledge (usually within a LLM) with private knowledge.</p>"},{"location":"glossary/#procedural-code","title":"Procedural Code","text":"<p>Procedural code is a programming paradigm that focuses on defining a sequence of operations for a computer to execute.</p> <p>It emphasizes how a program should accomplish tasks through explicit step-by-step instructions, control flow structures, and state changes. Procedural code typically organizes functionality into procedures or functions that are called in a predetermined order.</p> <p>Procedural code is contracted with declarative code and agentic code.</p>"},{"location":"glossary/#public-knowledge","title":"Public Knowledge","text":"<p>Knowledge that is openly available on public data sources such as the Internet and Wikipedia.</p> <p>Public knowledge is contrasted to private knowledge that has limited access.  For organizations, private knowledge usually is stored on an intranet or private company databases such as ERP and CRM systems.</p> <p>Software Agents must combine both public knowledge (usually within a LLM) with private knowledge.</p>"},{"location":"glossary/#react-framework","title":"ReAct framework","text":"<p>The use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner.  This interleaving provides better results than separated workflows.</p> <p>Reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. </p> <ul> <li>ReAct: Synergizing Reasoning and Acting in Language Models - March 2023</li> </ul>"},{"location":"glossary/#rules-engine","title":"Rules Engine","text":"<p>A software system that manages and executes a set of business rules in a runtime production environment. </p> <p>Rules engines separate business logic from application code by allowing rules to be defined, stored, and managed independently. Rules engines evaluate conditions against input data and execute corresponding actions when conditions are met, enabling non-technical users to modify business logic without changing the underlying application code.</p> <p>Key components typically include:</p> <ol> <li>A rule repository for storing and managing business rules</li> <li>A rule authoring interface for creating and modifying rules</li> <li>A rule execution engine that evaluates conditions and triggers actions</li> <li>Working memory that holds the data being evaluated against the rules</li> </ol> <p>Rules engines are commonly used in complex decision-making systems, regulatory compliance, fraud detection, pricing strategies, and other scenarios where business logic frequently changes or requires transparency.</p>"},{"location":"glossary/#smolagents","title":"SmolAgents","text":"<p>An agent framework released by HuggingFace in December of 2024 with the goal of creating a small efficient way to build agents</p> <ul> <li>HuggingFace SmolAgents Docs</li> </ul>"},{"location":"glossary/#tool","title":"Tool","text":"<p>An atomic function used by an agent. </p> <p>To be used by an LLM, tools needs a few attributes that constitute its API and will be used to describe to the LLM how to call this tool.  Here  are the four key attributes a tool needs:</p> <ol> <li>A tool name - this is usually the name of a python function</li> <li>A description - this comes from the Python Docstring</li> <li>Input parameter names, types and descriptions which include descriptions of enumerated values</li> <li> <p>An output format including types and how errors are returned</p> </li> <li> <p>See also: Docstring</p> </li> <li>See also: Decorator</li> </ol>"},{"location":"glossary/#tool-calling","title":"Tool Calling","text":"<p>Letting an LLM call a specific function usually with specific patterns.</p> <p>Example: Let an LLM call a function to get similar text to a question from a knowledge base.</p> <ul> <li>See also: Tool</li> <li>See also: Docstring</li> <li>See also: Decorator</li> </ul>"},{"location":"glossary/#tool-catalog","title":"Tool Catalog","text":"<p>A list of the tools available for an agent to use.  Each tool must be described by a name, description and list of parameters and their possible data types and enumerated values.</p> <p>In this course, the definition of Python functions that can be used by agents must have a Docstring with the <code>@tool</code> annotation.</p>"},{"location":"how-we-built-this-site/","title":"How We Built This Site","text":"<p>This page describes how we built this website and some of  the rationale behind why we made various design choices.</p>"},{"location":"how-we-built-this-site/#python","title":"Python","text":"<p>MicroSims are about how we use generative AI to create animations and simulations.  The language of AI is Python.  So we wanted to create a site that could be easily understood by Python developers.</p>"},{"location":"how-we-built-this-site/#mkdocs-vs-docusaurus","title":"Mkdocs vs. Docusaurus","text":"<p>There are two main tools used by Python developers to write documentation: Mkdocs and Docusaurus.  Mkdocs is easier to use and more popular than Docusaurus. Docusaurus is also optimized for single-page applications. Mkdocs also has an extensive library of themes and plugins. None of us are experts in JavaScript or React. Based on our ChatGPT Analysis of the Tradeoffs we chose mkdocs for this site management.</p>"},{"location":"how-we-built-this-site/#github-and-github-pages","title":"GitHub and GitHub Pages","text":"<p>GitHub is a logical choice to store our  site source code and documentation.  GitHub also has a Custom GitHub Action that does auto-deployment if any files on the site change. We don't currently have this action enabled, but other teams can use this feature if they don't have the ability to do a local build with mkdocs.</p> <p>GitHub also has Issues,  Projects and releases that we can use to manage our bugs and tasks.</p> <p>The best practice for low-cost websites that have public-only content is GitHub Pages. Mkdocs has a command (<code>mkdocs gh-deploy</code>) that does deployment directly to GitHub Pages.  This was an easy choice to make.</p>"},{"location":"how-we-built-this-site/#github-clone","title":"GitHub Clone","text":"<p>If you would like to clone this repository, here are the commands:</p> <pre><code>mkdir projects\ncd projects\ngit clone https://github.com/dmccreary/microsims\n</code></pre>"},{"location":"how-we-built-this-site/#after-changes","title":"After Changes","text":"<p>After you make local changes you must do the following:</p> <pre><code># add the new files to a a local commit transaction\ngit add FILES\n# Execute the a local commit with a message about what and why you are doing the commit\ngit commit -m \"comment\"\n# Update the central GitHub repository\ngit push\n</code></pre>"},{"location":"how-we-built-this-site/#material-theme","title":"Material Theme","text":"<p>We had several options when picking a mkdocs theme:</p> <ol> <li>Mkdocs default</li> <li>Readthedocs</li> <li>Third-Party Themes See Ranking</li> </ol> <p>The Material Theme had 16K stars.  No other theme had over a few hundred. This was also an easy design decision.</p> <p>One key criterial was the social Open Graph tags so that when our users post a link to a simulation, the image of the simulation is included in the link.  Since Material supported this, we used the Material theme. You can see our ChatGPT Design Decision Analysis if you want to check our decision process.</p>"},{"location":"how-we-built-this-site/#enable-edit-icon","title":"Enable Edit Icon","text":"<p>To enable the Edit icon on all pages, you must add the edit_uri and the content.action.edit under the theme features area.</p> <pre><code>edit_uri: edit/master/docs/\n</code></pre> <pre><code>    theme:\n        features:\n            - content.action.edit\n</code></pre>"},{"location":"how-we-built-this-site/#conda-vs-venv","title":"Conda vs VENV","text":"<p>There are two choices for virtual environments.  We can use the native Python venv or use Conda.  venv is simle but is only designed for pure Python projects.  We imagine that this site could use JavaScript and other langauges in the future, so we picked Conda. There is nothing on this microsite that prevents you from using one or the other.  See the ChatGPT Analysis Here.</p> <p>Here is the conda script that we ran to create a new mkdocs environment that also supports the material social imaging libraries.</p> <pre><code>conda deactivate\nconda create -n mkdocs python=3\nconda activate mkdocs\npip install mkdocs \"mkdocs-material[imaging]\"\n</code></pre>"},{"location":"how-we-built-this-site/#mkdocs-commands","title":"Mkdocs Commands","text":"<p>There are three simple mkdoc commands we use.</p>"},{"location":"how-we-built-this-site/#local-build","title":"Local Build","text":"<pre><code>mkdocs build\n</code></pre> <p>This builds your website in a folder called <code>site</code>.  Use this to test that the mkdocs.yml site is working and does not have any errors.</p>"},{"location":"how-we-built-this-site/#run-a-local-server","title":"Run a Local Server","text":"<pre><code>mkdocs serve\n</code></pre> <p>This runs a server on <code>http://localhost:8000</code>. Use this to test the display formatting locally before you push your code up to the GitHub repo.</p> <pre><code>mkdoc gh-deploy\n</code></pre> <p>This pushes everything up to the GitHub Pages site. Note that it does not commit your code to GitHub.</p>"},{"location":"how-we-built-this-site/#mkdocs-material-social-tags","title":"Mkdocs Material Social Tags","text":"<p>We are using the Material Social tags.  This is a work in progress!</p> <p>Here is what we have learned.</p> <ol> <li>There are extensive image processing libraries that can't be installed with just pip.  You will need to run a tool like brew on the Mac to get the libraries installed.</li> <li>Even after <code>brew</code> installs the libraries, you have to get your environment to find the libraries.  The only way I could get that to work was to set up a local UNIX environment variable.</li> </ol> <p>Here is the brew command that I ran:</p> <pre><code>brew install cairo freetype libffi libjpeg libpng zlib\n</code></pre> <p>I then had to add the following to my ~/.zshrc file:</p> <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib\n</code></pre> <p>Note that I am running on a Mac with Apple silicon.  This means that the image libraries that brew downloads must be specific to the Mac Arm instruction set.</p>"},{"location":"how-we-built-this-site/#image-generation-and-compression","title":"Image Generation and Compression","text":"<p>I have used ChatGPT to create most of my images.  However, they are too large for most websites.  To compress them down I used  https://tinypng.com/ which is a free tool  for compressing png images without significant loss of quality.  The files created with ChatGPT are typically around 1-2 MB.  After  using the TinyPNG site the size is typically around 200-300KB.</p> <ul> <li>Cover images for blog post #4364</li> <li>Discussion on overriding the Social Card Image</li> </ul>"},{"location":"license/","title":"Creative Commons License","text":"<p>All content in this repository is governed by the following license agreement:</p>"},{"location":"license/#license-type","title":"License Type","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED)</p>"},{"location":"license/#link-to-license-agreement","title":"Link to License Agreement","text":"<p>https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en</p>"},{"location":"license/#your-rights","title":"Your Rights","text":"<p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format</li> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#restrictions","title":"Restrictions","text":"<ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> <li>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</li> </ul> <p>Notices</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p> <p>This deed highlights only some of the key features and terms of the actual license. It is not a license and has no legal value. You should carefully review all of the terms and conditions of the actual license before using the licensed material.</p>"},{"location":"references/","title":"Site References","text":""},{"location":"references/#agentic-interoperability","title":"Agentic Interoperability","text":"<ul> <li>Agntcy on Perplexity</li> </ul>"},{"location":"references/#knowledge","title":"Knowledge","text":"<p>Deloitte Whitepaper: Knowledge Enriched Agentic Workflows</p>"},{"location":"references/#key-tutorials","title":"Key Tutorials","text":""},{"location":"references/#_1","title":"References","text":""},{"location":"references/#references-for-agents-that-do-web-search","title":"References for Agents that Do Web Search","text":"<ol> <li> <p>OpenAI Deep Research</p> </li> <li> <p>mkdocs - https://www.mkdocs.org/ - this is our tool for building the website.  It converts Markdown into HTML in the <code>site</code> directory.</p> </li> <li>mkdocs material theme - https://squidfunk.github.io/mkdocs-material/ - this is the theme for our site.  The theme adds the user interface elements that give our site the look and feel.  It also has the features such as social cards.</li> <li>GitHub Pages - https://pages.github.com/ - this is the free tool for hosting public websites created by mkdocs</li> <li>Markdown - https://www.mkdocs.org/user-guide/writing-your-docs/#writing-with-markdown - this is the format we use for text.  It allows us to have headers, lists, tables, links and images without learning HTML.</li> <li>Deploy Mkdocs GitHub Action - https://github.com/marketplace/actions/deploy-mkdocs - this is the tool we use to automatically build our site after edits are checked in with Git.</li> <li>Git Book - https://git-scm.com/book/en/v2 - a useful book on Git.  Just read the first two chapters to learn how to check in new code.</li> <li>Conda - https://conda.io/ - this is a command line tool that keeps our Python libraries organized for each project.</li> <li>VS Code - https://code.visualstudio.com/ - this is the integrated development environment we use to mange the files on our website.</li> <li>Markdown Paste - https://marketplace.visualstudio.com/items?itemName=telesoho.vscode-markdown-paste-image - this is the VS code extension we use to make sure we keep the markdown format generated by ChatGPT.</li> </ol>"},{"location":"concepts/","title":"Key Concepts in Agentic AI","text":"<p>Code Agents</p>"},{"location":"concepts/agents/","title":"Software Agents","text":"<p>A software agent is an autonomous computational entity that performs tasks on behalf of users or other programs with some degree of independence or autonomy, and in doing so, employs a precise knowledge representation of the user's goals or desires.</p>"},{"location":"concepts/agents/#agents-are-not-procedural-code","title":"Agents are Not Procedural Code","text":"<p>Here are some characteristics differentiating agents from standard procedural code:</p> <ol> <li>Autonomy: Agents operate without direct human intervention, making decisions and taking actions based on their internal state and perception of the environment. Unlike procedural code, which executes a predetermined sequence of instructions, and can follow precise rules, agents can decide when and how to act.</li> <li>Reactivity and Adaptability: Agents can perceive their environment and respond to changes in a timely fashion, adjusting their behavior accordingly. Procedural code follows a fixed execution path regardless of environmental changes unless explicitly programmed to handle specific scenarios.</li> <li>Goal-Orientation: Agents act in pursuit of goals rather than simply executing instructions. They can prioritize actions, resolve conflicts between competing objectives, and find alternative paths to achieve their goals when faced with obstacles. Procedural code lacks this inherent purpose-driven behavior and instead follows explicit instructions without considering overarching objectives.</li> </ol>"},{"location":"concepts/agents/#agents-and-rules-engines","title":"Agents and Rules Engines","text":""},{"location":"concepts/background/","title":"Useful Background Concepts","text":"<p>Here are some useful concepts that we will not cover too much in this course. This document is provided for you to review if you don't have a strong background in Python, LLMs, or knowledge representation.</p>"},{"location":"concepts/background/#what-is-a-llm","title":"What is a LLM?","text":""},{"location":"concepts/background/#how-do-you-write-a-good-prompt","title":"How do you write a good prompt?","text":""},{"location":"concepts/background/#what-is-a-knowledge-graph","title":"What is a knowledge graph?","text":""},{"location":"concepts/background/#how-do-we-represent-concepts","title":"How do we represent concepts?","text":""},{"location":"concepts/background/#how-do-we-measure-llm-quality","title":"How do we measure LLM quality?","text":""},{"location":"concepts/code-agents/","title":"What are Code Agents?","text":"<p>The CodeAgent in the Hugging Face SmolAgents library is a specialized type of AI agent designed to perform tasks by directly generating and executing Python code snippets. Unlike traditional agents that rely on structured formats like JSON or plain text to specify which tools to call and with what parameters, the CodeAgent takes a code-centric approach, allowing it to write Python code that executes actions immediately.</p>"},{"location":"concepts/code-agents/#key-features-of-the-codeagent","title":"Key Features of the CodeAgent","text":"<ul> <li>Direct Code Generation: The CodeAgent produces executable Python code to carry out its tasks, such as calling functions, performing computations, or interacting with external tools.</li> <li>Flexibility: By leveraging Python\u2019s programming constructs\u2014like loops, conditionals, and variables\u2014it can handle complex operations more naturally than JSON-based instructions.</li> <li>Reduced Overhead: Since the agent skips intermediate steps (e.g., parsing JSON to determine tool calls), it can execute actions more efficiently.</li> </ul>"},{"location":"concepts/code-agents/#how-it-differs-from-other-agents","title":"How It Differs from Other Agents","text":"<p>Traditional agents, including some supported by the SmolAgents library, typically operate as follows: - JSON or Text-Based Tool Calls: They generate structured outputs (e.g., JSON objects or text blobs) that describe which tools to invoke and with what arguments. For example:</p> <pre><code>  {\n    \"tool\": \"search\",\n    \"query\": \"HuggingFace SmolAgents\"\n  }\n</code></pre> <p>This JSON would then be parsed and mapped to a function call by the system. - Multi-Step Process: These agents require an additional layer of interpretation to translate the structured output into executable actions, which can introduce complexity and latency.</p> <p>In contrast, the CodeAgent:</p> <ul> <li>Writes Code Directly: Instead of producing JSON, it might generate something like:</li> </ul> <pre><code>result = search_tool(\"HuggingFace SmolAgents\")\n</code></pre> <p>This code is then executed in a controlled environment, eliminating the need for parsing or mapping.</p> <ul> <li>Streamlined Execution: By bypassing structured intermediaries, the CodeAgent reduces the number of steps between decision-making and action, potentially improving performance and enabling more sophisticated workflows.</li> </ul>"},{"location":"concepts/code-agents/#advantages-of-the-codeagent-approach","title":"Advantages of the CodeAgent Approach","text":"<ul> <li>Efficiency: Fewer processing steps mean faster execution, which can be a significant advantage in performance-sensitive applications.</li> <li>Expressiveness: Python\u2019s full range of programming features allows the agent to tackle intricate tasks that might be cumbersome to encode in JSON or text.</li> <li>Innovation: This code-based method is a distinctive feature of the SmolAgents library, setting it apart from other frameworks that lean heavily on traditional tool-calling paradigms.</li> </ul>"},{"location":"concepts/code-agents/#safety-considerations","title":"Safety Considerations","text":"<p>Executing generated code poses security risks, such as unintended system access or malicious operations. The SmolAgents library addresses this by providing:</p> <ul> <li>Controlled Imports: Limiting the modules the CodeAgent can access.</li> <li>Sandboxed Environments: Running the code in isolated contexts to prevent harmful effects.</li> </ul>"},{"location":"concepts/code-agents/#conclusion","title":"Conclusion","text":"<p>The CodeAgent stands out in the Hugging Face SmolAgents library as an innovative agent that uses Python code snippets to perform actions, contrasting with traditional agents that rely on JSON or text-based tool calls. This approach offers greater flexibility, efficiency, and power, making it a compelling choice for developers building AI-driven solutions within a secure and streamlined framework.</p>"},{"location":"concepts/code-agents/#references","title":"References","text":"<ul> <li>HuggingFace: Secure code execution in SmolAgents</li> <li>HuggingFace Paper</li> <li>Arxiv.org: Executable Code Actions Elicit Better LLM Agents This work proposes to use executable Python code to consolidate LLM agents\u2019 actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions</li> </ul>"},{"location":"concepts/deepseek-r1/","title":"Deepseek R1","text":""},{"location":"concepts/deepseek-r1/#technical-breakdown","title":"Technical Breakdown","text":"<p>Architecture and Model Variants: DeepSeek R1 models are built on a transformer-based mixture-of-experts (MoE) architecture, with an enormous 671 billion total parameters, of which about 37 billion are active per inference forward pass (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat) (deepseek-ai/DeepSeek-R1 - Hugging Face). This design allows the model to leverage many expert subnetworks while keeping the inference budget similar to a ~37B dense model. The context window is extended to 128,000 tokens, supporting very long inputs and outputs (comparable to GPT-4's 128K context support) (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat) (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). The DeepSeek-R1 family includes the full MoE model (671B) and several distilled dense models (1.5B, 7B, 8B, 14B, 32B, 70B) that capture R1's reasoning patterns in smaller architectures (deepseek-ai/DeepSeek-R1 - Hugging Face) (deepseek-ai/DeepSeek-R1 - Hugging Face). The distilled models are based on popular backbones like Qwen (Alibaba) and Llama and use the same tokenizer modifications as their bases for compatibility (deepseek-ai/DeepSeek-R1 - Hugging Face).</p> <p>Training Methodology: The DeepSeek R1 training pipeline is novel in that it heavily uses reinforcement learning (RL) to instill reasoning skills. The initial \"DeepSeek-R1-Zero\" model was obtained by applying large-scale RL directly on a pre-trained base (DeepSeek-V3-Base) without any supervised fine-tuning (SFT) seed (deepseek-ai/DeepSeek-R1 - Hugging Face) (deepseek-ai/DeepSeek-R1 - Hugging Face). This demonstrated that complex reasoning behaviors (like chain-of-thought, self-reflection, and self-verification) can emerge solely from RL rewards, a first in open research (deepseek-ai/DeepSeek-R1 - Hugging Face). However, R1-Zero suffered from issues like repetitive outputs and mixed-language responses (deepseek-ai/DeepSeek-R1 - Hugging Face). To address this, the final DeepSeek R1 model incorporated a more balanced training pipeline with two SFT stages and two RL stages (deepseek-ai/DeepSeek-R1 - Hugging Face). In practice, a small \"cold-start\" SFT on curated data was used to prime the base model (improving coherence and following instructions), then an RL phase encouraged the model to discover better reasoning strategies (e.g. using longer chain-of-thought for complex problems). This was followed by another SFT (to integrate the high-quality RL-generated solutions and improve general readability), and a final RL fine-tuning focused on aligning with human preferences and instructions (deepseek-ai/DeepSeek-R1 - Hugging Face). The RL training signal uses both learned reward models and rule-based verifiers -- for example, math answers are checked by an internal solver and code solutions by execution, similar to techniques used by OpenAI and Alibaba's Qwen team (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). Notably, DeepSeek R1's RL-first approach proved that even without an initial human-supervised phase, an LLM can learn to reason through trial-and-error reinforcement -- a key architectural innovation of this model (deepseek-ai/DeepSeek-R1 - Hugging Face).</p> <p>Key Optimizations: Several optimizations in DeepSeek R1's architecture and training improved its efficiency and performance. In the underlying DeepSeek-V3 base (the foundation for R1), the team introduced an auxiliary-loss-free load balancing strategy for the MoE layers, ensuring that the many experts are utilized effectively without needing extra loss terms (GitHub - deepseek-ai/DeepSeek-V3). This addresses a common challenge in MoE models where some experts can become under-trained; DeepSeek's solution keeps expert usage balanced purely through careful gating architecture. The model also uses multi-token prediction objectives during pre-training (predicting multiple tokens per step) to boost training efficiency and downstream performance (GitHub - deepseek-ai/DeepSeek-V3). Standard enhancements like RoPE positional encodings for long context, SwiGLU activation, and RMSNorm are employed similar to other modern LLMs (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). Compared to dense models of similar scale, DeepSeek R1's MoE approach trades higher memory footprint for faster inference per token -- e.g. it has ~671B total weights (requiring ~1.5 TB VRAM to load fully), but only a fraction (~37B) are activated for each input, meaning inference compute is closer to a 37B model's cost (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). In practice, running the full DeepSeek-R1 requires a multi-GPU setup (e.g. 16\u00d7A100 GPUs) (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat), but the distilled R1 models provide much more accessible alternatives. For instance, the 32B distilled model (DeepSeek-R1-Distill-Qwen-32B) can achieve comparable results with just ~24GB GPU memory needed (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat), using a dense architecture distilled from R1's knowledge. This strategy of training one very large \"teacher\" model and then compressing its knowledge into smaller models is a key optimization that DeepSeek uses to make deployment practical. All DeepSeek R1 models are released under a permissive MIT license, allowing commercial use and further modification (deepseek-r1), which contrasts with the more restrictive licenses of some contemporary open models. In summary, DeepSeek R1's design innovates by combining extreme scale (hundreds of billions of parameters), advanced MoE engineering, and an RL-driven training regime focused on reasoning -- setting it apart from conventional LLM training pipelines (which usually rely heavily on supervised fine-tuning and human feedback alignment before any RL).</p> <p>Parameters and Efficiency vs. Other LLMs: In terms of size, DeepSeek R1 (671B) is one of the largest language models ever publicized. It far exceeds the dense parameter count of models like GPT-4 (estimated on the order of ~170B to 1T, though exact numbers are unpublished) and Meta's Llama 3 (which introduced a 405B dense variant as its largest model) (What Is Meta's Llama 3.1 405B? How It Works, Use Cases &amp; More) (GitHub - deepseek-ai/DeepSeek-V3). Even Google's PaLM 2 and Gemini prototypes are smaller in parameter count than R1's total. However, thanks to its MoE architecture, R1's effective inference footprint (37B) is lower than GPT-4's if GPT-4 is purely dense -- making R1 surprisingly efficient for its scale. For example, GPT-4's 32k-context version is known to be very resource-intensive, whereas DeepSeek R1 can handle 128k context with its MoE sharding of work across experts (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat) (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). Against Llama 3, the largest Llama 3.1 (405B dense) achieves strong performance but still slightly trails DeepSeek R1 on challenging benchmarks (e.g. MMLU) (GitHub - deepseek-ai/DeepSeek-V3), likely because R1's RL-enhanced reasoning gives it an edge beyond what scaling alone provided. Smaller open models like Mistral (7B) and Falcon (40B) are dramatically smaller than R1 and emphasize efficiency and speed on limited hardware, but they cannot match DeepSeek R1's state-of-the-art results in complex tasks due to the vast gap in training scale and techniques. For instance, Falcon-40B (one of the best pre-R1 open models) scored around 54% on MMLU (MMLU: Better Benchmarking for LLM Language Understanding), whereas R1 approaches ~91% on the same benchmark (see next section). In summary, DeepSeek R1 pushes the limits of model size and training innovation, going beyond the architectures of GPT-4 and others by using MoE at scale and RL-driven training. Its design balances the frontier of maximum capability with strategies (like distillation and MoE gating) to make such capability usable in practice.</p>"},{"location":"concepts/deepseek-r1/#benchmarks-and-performance","title":"Benchmarks and Performance","text":"<p>DeepSeek R1 has been evaluated on a wide range of standard NLP and reasoning benchmarks, where it achieves cutting-edge performance comparable to or surpassing other leading LLMs of 2024-2025. Below is a summary of how R1 stacks up on key tasks:</p> <ul> <li> <p>Knowledge and Reasoning (MMLU): On the Massive Multitask Language Understanding benchmark (MMLU, which tests knowledge across 57 subjects), DeepSeek R1 scores around 90.8% (accuracy) (deepseek-ai/DeepSeek-R1 - Hugging Face). This is essentially at GPT-4 level -- for comparison, an OpenAI model (\"o1-1217\", presumably GPT-4 or similar) scored 91.8% on the same test (deepseek-ai/DeepSeek-R1 - Hugging Face). R1's score also exceeds Anthropic's Claude 3.5 (which was ~88--90% (deepseek-ai/DeepSeek-R1 - Hugging Face)) and vastly outperforms open-source predecessors like Llama 2 (which was ~68% on MMLU) and Falcon-40B (~54% (MMLU: Better Benchmarking for LLM Language Understanding)). In a variant \"MMLU-Redux\" evaluation, R1 reached 92.9%, the highest among the compared models (deepseek-ai/DeepSeek-R1 - Hugging Face). These results demonstrate that R1 has acquired a broad range of world knowledge and can reason across domains nearly as well as the best proprietary models.</p> </li> <li> <p>Mathematical Reasoning (GSM8K, MATH, AIME): DeepSeek R1 particularly shines at math and quantitative reasoning tasks, likely due to its RL-based training that emphasized step-by-step solution verification. On GSM8K (Grade School Math problems), R1 achieves high accuracy -- sources indicate 85%+ range (DeepSeek R1 vs Qwen 2.5 Max: A Detailed Comparison of Features and Performance), which is on par with or slightly above GPT-4's performance on this dataset. For more advanced math, R1 reached 79.8% pass@1 on the AIME 2024 math competition problems (deepseek-ai/DeepSeek-R1 - Hugging Face), essentially tying OpenAI's model (which scored 79.2%). On the challenging MATH dataset (MATH-500) of high school math problems, R1's chain-of-thought solving ability yields an impressive 97.3% pass@1 (deepseek-ai/DeepSeek-R1 - Hugging Face) -- outperforming OpenAI's reference (96.4%). These numbers are remarkable: they suggest DeepSeek R1 can correctly solve nearly all math questions in that benchmark when allowed to generate and evaluate multiple solution attempts. In summary, R1 has state-of-the-art math reasoning skills, leveraging its self-reflection and verifier-enhanced training to check its work. This is a significant improvement over models like GPT-3.5 or Llama, which struggled with complex multi-step math. Even smaller distilled versions of R1 do well -- e.g. the 32B distilled model achieves over 72% on GSM8K and ~94% on MATH, rivaling much larger dense models (DeepSeek R1 vs Qwen 2.5 Max: A Detailed Comparison of Features and Performance) (deepseek-ai/DeepSeek-R1 - Hugging Face).</p> </li> <li> <p>Coding and Code Reasoning: DeepSeek R1 is highly proficient at coding tasks. On HumanEval (a benchmark of Python coding problems), R1 achieves about 71% pass@1 (solving rate) (DeepSeek R1 vs Qwen 2.5 Max: A Detailed Comparison of Features and Performance), slightly edging out Alibaba's Qwen-2.5 Max (69.3%) and comparable to GPT-4's level on this test. This indicates that given a programming prompt, R1 can write correct solutions the majority of the time without needing multiple attempts. Moreover, R1 was tested on more difficult coding benchmarks that involve reasoning and multi-step planning. For instance, on LiveCodeBench (which requires generating code with intermediate reasoning steps), R1 attained 65.9% pass@1 (with chain-of-thought), which is higher than OpenAI's smaller model (o1-mini 53.8%) and even slightly above OpenAI's full model (~63.4%) (deepseek-ai/DeepSeek-R1 - Hugging Face). Additionally, R1 was evaluated on Codeforces programming challenges, where it achieved a competitive Elo rating around 2030, very close to OpenAI's result (~2061) (deepseek-ai/DeepSeek-R1 - Hugging Face). These results suggest R1 not only writes correct code, but can handle complex programming puzzles with reasoning -- a testament to the effectiveness of its RL training on coding tasks (possibly using code execution feedback as a reward). In practical terms, DeepSeek R1's coding ability is among the best in class, making it useful for code generation, debugging, and assisting in software tasks at a level comparable to top closed models. Even the distilled R1-32B model shows strong coding performance (~57% on LiveCodeBench, Codeforces ~1690 rating) outperforming other open models of similar size (deepseek-ai/DeepSeek-R1 - Hugging Face).</p> </li> <li> <p>General NLP and Text Generation: Beyond specialized tasks, R1 was evaluated on general instruction-following and reasoning benchmarks. In AlpacaEval 2.0, a benchmark that compares models on helpfulness and clarity of responses to instructions, DeepSeek R1 had an 87.6% win rate in pairwise comparisons (beating a strong reference in the vast majority of test prompts) (deepseek-ai/DeepSeek-R1 - Hugging Face). This is notably higher than Claude 3.5 (~52%) or GPT-4 (~51%) in that evaluation, indicating R1 produces very high-quality, helpful responses in a chat/instruction setting (deepseek-ai/DeepSeek-R1 - Hugging Face). Similarly, on Big Bench Hard (BBH) -- a collection of challenging reasoning tasks -- R1 scored about 80.2%, slightly above Qwen-2.5 Max's 78.5% (DeepSeek R1 vs Qwen 2.5 Max: A Detailed Comparison of Features and Performance). These scores underline R1's strength in complex reasoning and following instructions: thanks to its reinforcement learning fine-tuning, it can perform lengthy chain-of-thought reasoning and self-check its answers, leading to very thorough and correct responses on difficult queries. However, some trade-offs were observed: in a Prompt Strictness test (following instructions to the letter), R1 scored a bit lower (83.3) than Claude 3.5 (86.5) (deepseek-ai/DeepSeek-R1 - Hugging Face), suggesting that R1's focus on deep reasoning sometimes comes at the cost of strictly obeying the prompt format or brevity. Overall, for text generation tasks requiring reasoning, depth, and accuracy, R1 is among the top performers, whereas for simple prompt adherence or safe-chat behavior, models like Claude (with heavy alignment training) might have an edge.</p> </li> <li> <p>Multilingual Understanding: DeepSeek R1 was trained on a large multilingual corpus (notably both English and Chinese data), and it exhibits excellent multilingual capabilities. On C-Eval, a comprehensive Chinese evaluation benchmark, DeepSeek R1 achieved 91.8% (exact match) (deepseek-ai/DeepSeek-R1 - Hugging Face), dramatically surpassing GPT-4's performance (~76.0% on the same test) (deepseek-ai/DeepSeek-R1 - Hugging Face). This indicates R1 has a superior grasp of Chinese academic and common knowledge tasks, likely due to targeted training data and perhaps specific rewards in RL for Chinese reasoning. On a Chinese WSC (winograd style coreference) test, R1 also tops the charts at 92.8% (deepseek-ai/DeepSeek-R1 - Hugging Face). In a Chinese simple QA benchmark, R1 is slightly ahead of GPT-4 (63.7% vs 58.7% accuracy) (deepseek-ai/DeepSeek-R1 - Hugging Face). These results demonstrate that R1 is not just an English-centric model but a bilingual powerhouse, especially strong in Chinese -- likely on par with or better than native Chinese models. For other languages, R1's multilingual training (and possibly larger context) help it perform robustly, though detailed scores weren't listed in the source, we can infer from related benchmarks (XGLUE, XTREME) that it performs competitively but might be edged out by models like Qwen 2.5 Max in some multilingual categories (DeepSeek R1 vs Qwen 2.5 Max: A Detailed Comparison of Features and Performance) (since Qwen was specifically optimized for broad multilingual support). Still, the availability of R1's model for multiple languages has been a boon for non-English NLP. Its performance in multilingual QA and cross-lingual tasks is state-of-the-art among open models, reducing the gap with specialized models built for those languages.</p> </li> </ul> <p>In summary, DeepSeek R1's benchmark performance is on par with the very best LLMs (GPT-4 class) in many areas. It excels at complex reasoning, coding, and math -- often matching or slightly surpassing GPT-4 and Claude on those tasks (deepseek-ai/DeepSeek-R1 - Hugging Face) (deepseek-ai/DeepSeek-R1 - Hugging Face). It also holds its own in knowledge and general understanding tasks (MMLU ~90+%). There are a few areas where OpenAI's or others maintain a lead, such as certain open-ended knowledge queries (for example, OpenAI's model scored higher on a broad GPQA benchmark: 75.7 vs R1's 71.5 (deepseek-ai/DeepSeek-R1 - Hugging Face)) -- suggesting GPT-4 might have an edge in extensive world knowledge or factual recall. But the gaps are small. Importantly, R1's distilled smaller models also achieve remarkable performance for their size. The 32B R1-Distill, for instance, outperforms OpenAI's smaller \"o1-mini\" model across many benchmarks (deepseek-ai/DeepSeek-R1 - Hugging Face), setting new state-of-art results among models in the 30B-70B range. This means R1's impact isn't limited to those with giant compute clusters -- its knowledge has effectively trickled down to models one can run on a single GPU, without losing too much accuracy. The upshot is that DeepSeek R1 established itself as one of the top-performing LLMs across the board, validating the team's emphasis on reasoning-centric training through strong benchmark results in reasoning, coding, text generation quality, and multilingual understanding.</p>"},{"location":"concepts/deepseek-r1/#comparison-with-other-llms","title":"Comparison with Other LLMs","text":"<p>DeepSeek R1 arrives in a landscape alongside both proprietary giants (like OpenAI's GPT-4 series and Anthropic's Claude) and open-source models (like Meta's Llama 3 and newcomers such as Mistral and Falcon). Below we highlight how DeepSeek R1 differs from and compares to these models in terms of capabilities, efficiency, and deployment:</p> <ul> <li> <p>GPT-4 (OpenAI \"o\" series): GPT-4 is the most prominent closed-source model and R1's performance peer. In head-to-head comparisons, DeepSeek R1 is roughly on par with GPT-4 on many academic and coding benchmarks -- for example, their MMLU scores differ by only ~1% (deepseek-ai/DeepSeek-R1 - Hugging Face), and they trade blows on coding tasks (GPT-4 slightly behind on some, slightly ahead on others) (deepseek-ai/DeepSeek-R1 - Hugging Face). Architecturally, GPT-4 is believed to be a dense transformer (possibly with some mixture-of-expert components internally, but not confirmed) with an estimated hundreds of billions of parameters. DeepSeek R1, by contrast, is explicitly a Mixture-of-Experts model with 671B parameters total (and 37B active) (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). This gives R1 an advantage in specialization (many experts can handle different aspects of a problem) but makes it heavy to deploy in full. GPT-4's training relied on a large supervised fine-tuning and human-feedback loop, making it very aligned and reliable in following instructions and staying within guardrails. R1's training put more emphasis on autonomous reasoning via RL, which yielded superior problem-solving but at times slightly less compliance (as seen in prompt strictness tests) (deepseek-ai/DeepSeek-R1 - Hugging Face). In practical use, GPT-4 might produce more concise or safe answers by default, whereas R1 might produce more detailed, step-by-step explanations (since it was rewarded for showing its reasoning). Capabilities: Both models handle a wide spectrum of tasks (coding, writing, reasoning, etc.). GPT-4 remains a bit better in open-domain knowledge QA (likely due to broader training data or retrieval strategies in deployment) (deepseek-ai/DeepSeek-R1 - Hugging Face), whereas R1 has a slight edge in systematic reasoning (e.g. long math proofs, complex code with self-checking). GPT-4 is multi-modal (can accept images) in some versions; DeepSeek R1 is text-only (no image input capability reported in R1). Efficiency and Deployment: GPT-4 is accessible only via API (cloud) and not runnable locally, with OpenAI tightly controlling its model. DeepSeek R1 is fully open-source -- anyone can download the weights (deepseek-ai/DeepSeek-R1 - Hugging Face) -- but running the full 671B model requires significant hardware (multi-GPU server). However, R1's distilled models (e.g. 32B, 70B) can be run on a single high-end GPU or modest GPU cluster, giving practitioners a local alternative that approximates GPT-4-level performance. In terms of context length, both are in the top tier: GPT-4 supports up to 128k tokens (with the 2024 updates), and R1 similarly supports 128k context out of the box (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). This makes them both suitable for lengthy documents or transcripts, unlike many smaller models. In sum, DeepSeek R1 has essentially brought GPT-4-like capabilities to the open source world, with slight differences in style and accessibility: GPT-4 is still the leader in fine alignment and perhaps breadth of training data, while R1 demonstrates the power of an RL-trained, openly available model that organizations can host themselves.</p> </li> <li> <p>Claude (Anthropic): Claude 2 and 3 (e.g. Claude 3.5 \"Sonnet\") are Anthropic's large models focusing on safety and large context. Claude 3.5 was one of the first to offer 100k token context windows and is known for being very conversationally aligned (trying to be helpful, harmless, honest). In evaluations, Claude 3.5 Sonnet reaches about 90.5% on MMLU, roughly equal to R1 and GPT-4. However, in coding and some reasoning tasks, Claude tends to lag slightly behind both GPT-4 and R1. For example, Claude 3.5 scored ~52% win-rate on AlpacaEval vs R1's 87% (deepseek-ai/DeepSeek-R1 - Hugging Face), indicating that on complex instruction-following, R1 can provide more robust answers. Claude's strength is its harmlessness and compliance -- it's less likely to produce problematic content and often follows user instructions very literally. R1, while aligned via RLHF in its final stage, may not have undergone as intensive a safety training as Claude, so it might be more prone to generating unfiltered content if prompted maliciously (though no specific issues have been reported publicly). Efficiency: Claude is closed-source and accessible through API (Anthropic's), with no known parameter count (estimated similar scale to GPT-4). R1's open availability is a contrast -- anyone worrying about data privacy or needing offline use can opt for R1's models instead of Claude. Context: Both offer huge context (Claude 100k vs R1 128k), making them suitable for long documents and conversations. Use cases: If an enterprise's priority is a pre-aligned, safety-first model with massive context (and doesn't mind cloud usage), Claude is attractive. If the priority is maximal reasoning ability, code proficiency, and self-hosting, DeepSeek R1 is a compelling alternative, delivering stronger problem-solving performance in many cases. It's worth noting that R1's RL-based reasoning is somewhat in line with Anthropic's philosophy of constitutional AI (models improving via feedback); both signify a move towards models that can reflect on their answers. But R1 took a more direct approach by training with explicit problem-solving rewards, whereas Claude was trained on a large mix of dialogue with a safety-focused objective.</p> </li> <li> <p>Llama 3 (Meta): Meta's Llama series (Llama 2 was 2023, Llama 3 in 2024) represents the cutting edge of open foundational models. Llama 3 introduced models at 8B, 70B, and a gargantuan 405B \"Llama 3.1\" model (What Is Meta's Llama 3.1 405B? How It Works, Use Cases &amp; More). The 405B Llama is a dense model that Meta made available to researchers and on platforms like Azure, touting it as the largest open model at the time. Despite its size and quality (it outperforms Llama 2 by a large margin and is competitive with many proprietary models (What Is Meta's Llama 3.1 405B? How It Works, Use Cases &amp; More)), Llama 3.1--405B slightly underperforms DeepSeek R1 on reasoning benchmarks -- for instance, Llama3.1 achieved ~88.6% on MMLU (GitHub - deepseek-ai/DeepSeek-V3), whereas R1 reached ~90.8%. This suggests that beyond a certain scale, training methodology becomes crucial: R1's specialized reinforcement learning approach likely extracted more reasoning ability than Meta's standard next-token prediction training plus instruction tuning. On coding, Llama 70B or 405B are strong (Llama 70B chat was around ~67% HumanEval), but R1 still has the edge (71%+ HumanEval) (DeepSeek R1 vs Qwen 2.5 Max: A Detailed Comparison of Features and Performance). Where Llama 3 excels is being a versatile base -- many fine-tuned variants exist for various tasks, and it is fully open (though with a Meta-specific license). DeepSeek R1 actually leverages Llama 3 in its distilled models -- for example, \"DeepSeek-R1-Distill-Llama-70B\" is R1's knowledge distilled into a Llama 3.3 70B instruct model (deepseek-r1) (deepseek-r1). This cross-pollination shows that R1 complements Llama: one can use R1 as an oracle teacher to improve Llama-family models. Efficiency: Llama 3's 70B and smaller models can run on a single GPU (with 70B needing ~2\u00d748GB GPUs or so), making them easier to deploy than R1's full model. The 405B Llama3 obviously is large (likely requiring 8+ GPUs), but still it's a dense model -- some simpler infrastructure than MoE. R1's MoE might be trickier to serve (needing a custom MoE runtime or partitioning across experts). Deployment: Both R1 and Llama 3 are open; however, Meta's license for Llama might restrict commercial use (as was the case with Llama 2's license for certain users). DeepSeek R1 is MIT licensed (very permissive) (deepseek-r1), which encourages broader adoption, even in commercial products, without legal worry. Another difference is training data focus: Llama was trained on a balanced multilingual corpus but not especially targeting any single domain. DeepSeek R1's base (DeepSeek-V3) was trained on a massive 14.8T token corpus including a lot of English/Chinese and technical data (GitHub - deepseek-ai/DeepSeek-V3), and then R1 was further tuned on reasoning heavy data. Thus R1 might have more baked-in \"problem-solving knowledge\" (e.g. math proofs, code debugging logs, etc.) than a vanilla Llama. For a developer deciding between them: if one wants a solid general-purpose model to fine-tune, Llama 70B is great; if one wants the best reasoning out-of-the-box and possibly to study novel training techniques, DeepSeek R1 is the exemplar.</p> </li> <li> <p>Mistral (Open-Source Efficient LLMs): Mistral 7B (2023) and any subsequent models from Mistral AI highlight efficiency: the 7B model was trained on a high-quality dataset and tuned to outperform larger models like Llama2-13B in many tasks, despite its small size. However, there is a clear gap between what can be achieved at single-digit billions of parameters and what R1 (with hundreds of billions + specialized training) achieves. For example, Mistral 7B's MMLU score is around mid-60s% (Mistral 7B vs DeepSeek R1 Performance: Which LLM is the Better Choice?), whereas R1 is ~90%. On coding, Mistral 7B might score ~30-40% on HumanEval, versus R1's 71%. These differences are huge -- indicating that while techniques like data quality, longer training, and smart initialization can boost a small model, they cannot fully compensate for scale and advanced training on the hardest tasks. That said, Mistral and similar models have an ultra-low footprint: a 7B model can run on a consumer laptop or even a phone in some cases. DeepSeek R1's distilled 7B (there is an R1-distill Qwen-7B model) actually leverages R1's strengths to also outperform standard 7B models (deepseek-ai/DeepSeek-R1 - Hugging Face). In fact, DeepSeek-R1-Distill-7B (Qwen base) reportedly achieves ~49% on HumanEval and ~83% on GSM8K (deepseek-ai/DeepSeek-R1 - Hugging Face), which is much higher than a naive 7B model could do -- showing the value of R1's knowledge transfer. So, comparing R1 to Mistral: R1 is a research-grade, maximum performance model, whereas Mistral 7B (and hypothetical 13B or 20B follow-ups) are deployment-grade, compact models. One might use R1 to generate data or insights to then fine-tune a Mistral model, for instance. Another difference: R1 uses 128k context; most small models have 4k to 8k context (Mistral 7B extended to 8k with RoPE scaling). So R1 also wins in context length by a large margin over typical small models. In sum, R1 isn't directly \"competing\" with a 7B model -- they occupy different ends of the spectrum -- but R1 has redefined the upper bound of open model capability, whereas Mistral et al. define how far one can go with minimal parameter budgets. It's worth noting that R1's advent may inspire more efficient techniques to approximate its performance: e.g., Alibaba's QwQ-32B showed that clever multi-stage RL on a 32B model could reach R1-like reasoning quality (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat) (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). So smaller, efficient LLMs are quickly learning from R1's example to narrow the gap.</p> </li> <li> <p>Falcon and Other Open Models: Falcon-40B (from TII UAE) was a leading open model prior to Llama 2, with good performance on English tasks but relatively weaker alignment and limited context (2048 tokens). DeepSeek R1 clearly outperforms Falcon-40B by a wide margin on virtually every benchmark (knowledge, reasoning, etc.), as indicated by Falcon's ~63% average on some evaluations vs R1's ~90% in the same (MMLU: Better Benchmarking for LLM Language Understanding). R1's release (and that of Llama 2/3) essentially eclipsed Falcon, though Falcon demonstrated the viability of large open models (and had an Apache license). Other models like PaLM 2 (Google) and Gemini (Google DeepMind) are closed-source but were expected to be strong; the VentureBeat article notes that by late 2024, OpenAI's \"o3\" series and Google's Gemini were also focusing on reasoning and extended context, partly influenced by the success of models like DeepSeek R1 (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). We see that R1 pushed the envelope, causing even the big players to iterate (e.g., OpenAI's hypothetical \"o3\" might be an even more reasoning-focused successor to GPT-4). In deployment terms, R1 being open means organizations that prefer self-hosting had, for the first time, an alternative to relying on API access to GPT-4/Claude for top-tier performance. This is a significant strategic difference from proprietary models -- R1 can be deployed on Azure AI Foundry, on local servers, or via community projects like Hugging Face and Ollama, giving users more control (DeepSeek R1 is now available on Azure AI Foundry and GitHub | Microsoft Azure Blog). Models like GPT-4/Claude require trusting a third-party service. On the flip side, proprietary models often integrate up-to-date knowledge (live browsing or tools) in their offerings, something R1 by itself doesn't do (though one can combine R1 with retrieval systems in a RAG pipeline to mitigate its knowledge cutoff).</p> </li> </ul>"},{"location":"concepts/deepseek-r1/#summary","title":"Summary","text":"<p>In summary, DeepSeek R1 stands out for its unprecedented combination of openness and high performance. Against closed models (GPT-4, Claude), it competes neck-and-neck in ability while offering the transparency and modifiability of open source. Against other open models (Llama 3, Falcon, Mistral), R1 defines the high end of capability, introducing training innovations that others are now adopting or distilling. Each model has its niche: GPT-4 remains the general-purpose gold standard with strong factuality and multi-modality, Claude is the aligned long-context specialist, Llama is the adaptable open base model, and DeepSeek R1 is the reasoning and problem-solving expert that bridges the gap between research and open deployment. The existence of R1 has effectively raised the bar for all LLMs, ensuring that new models (whether open or closed) must contend with a state-of-the-art performer that is freely available for anyone to use or build upon.</p>"},{"location":"concepts/deepseek-r1/#code-example-using-deepseek-r1-with-ollama","title":"Code Example (Using DeepSeek R1 with Ollama)","text":"<p>One convenient way to run DeepSeek R1 models locally is via Ollama, a tool for serving and using LLMs on local GPUs. The DeepSeek R1 family is available in Ollama's model library, including the distilled versions that are feasible to run on a single GPU. Below is a code snippet demonstrating how to download and run the DeepSeek R1 32B distilled model (which offers excellent performance) using Ollama on a local machine:</p> <pre><code># Install Ollama (if not already installed) and ensure you have a GPU with sufficient VRAM (e.g. 24GB for the 32B model).\n\n# Pull the DeepSeek-R1 32B model from Ollama's library:\nollama pull deepseek-r1:32b\n\n# Once the model is downloaded, you can run queries against it.\n# For example, ask a question or prompt:\nollama run deepseek-r1:32b --prompt \"Explain the architecture of DeepSeek R1 in a few sentences.\"\n</code></pre> <p>The above commands will load the DeepSeek-R1-Distill-Qwen-32B model and execute the given prompt on your local GPU. You should see the model's generated answer in the console. The <code>ollama pull</code> step is only needed the first time to download the weights; afterward, <code>ollama run deepseek-r1:32b \"&lt;your prompt&gt;\"</code> is sufficient. You can replace <code>32b</code> with other available model sizes (such as <code>70b</code> for the 70B Llama-derived model, or <code>7b</code> for a smaller model) depending on your hardware capacity (deepseek-r1) (deepseek-r1). For instance, <code>ollama run deepseek-r1:70b</code> would run the 70B distilled model (which may require 2\u00d7GPU for smooth operation), and <code>ollama run deepseek-r1:7b</code> would run a much smaller 7B model. All DeepSeek R1 models in Ollama are under the MIT license, so they can be used commercially. This local deployment option illustrates the practicality of R1's open-source approach -- even though the flagship 671B model is huge, its distilled offspring allow developers to experiment with R1's capabilities on everyday hardware.</p>"},{"location":"concepts/deepseek-r1/#strategic-consequences","title":"Strategic Consequences","text":"<p>The release of DeepSeek R1 has had significant strategic implications for the AI development landscape, especially in the open-source community and global AI competition:</p> <ul> <li> <p>Democratizing High-End AI: Perhaps the most immediate impact of DeepSeek R1 is that it bridged the performance gap between open-source and closed-source models. Prior to R1, the best open models (like Llama 2 or Falcon) were notably behind models like GPT-4 in capability. R1's emergence as an open model comparable to GPT-4 (deepseek-ai/DeepSeek-R1 - Hugging Face) proved that top-tier AI is not exclusive to tech giants. This has empowered researchers, startups, and even hobbyists worldwide -- they now have access to a GPT-4-class model that can be studied and integrated without needing permission or hefty API fees. Microsoft's inclusion of DeepSeek R1 in Azure AI Foundry (DeepSeek R1 is now available on Azure AI Foundry and GitHub | Microsoft Azure Blog) underscores this democratization: a major cloud provider saw value in offering R1 alongside proprietary models, giving enterprise customers more choice. The strategic shift here is that open models are now part of the \"frontier models\" conversation, not just supporting players. We see organizations leveraging R1 to build advanced applications entirely on open infrastructure, which pressures closed model providers to justify their cost and policies. In essence, DeepSeek R1 fueled a greater openness in AI, forcing a reevaluation of the balance between proprietary advantage and community collaboration.</p> </li> <li> <p>Influence on Research Directions: DeepSeek R1's successful use of reinforcement learning for reasoning has spurred new research into LLM training techniques. The fact that R1-Zero showed emergent reasoning purely from RL rewards (no initial human examples) was a groundbreaking result (deepseek-ai/DeepSeek-R1 - Hugging Face). This has encouraged other AI labs to experiment with reinforcement learning, self-play, and other beyond-supervised methods to push reasoning capabilities. For example, OpenAI's rumored \"o3\" series and projects like Anthropic's next Claude are likely incorporating lessons from R1, focusing on inference-time reasoning loops, self-reflection, and multi-stage training (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). We also see academic interest in the idea of Large Reasoning Models (LRMs) as a category, which R1 exemplifies -- models that explicitly perform internal reasoning steps to improve answer quality (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). Additionally, R1's distillation of reasoning into smaller models validated a path to efficiency: rather than training a small model to do reasoning from scratch, use a big RL-tuned teacher to generate high-quality reasoning traces and fine-tune the small model on that. This approach achieved better results for the 32B model than training it with RL directly (deepseek-ai/DeepSeek-R1 - Hugging Face). As a consequence, we're seeing a strategic shift in how new models are developed -- many teams are now adopting a two-stage approach (train a very large model with new techniques, then compress it). The open release of R1 (and its data) has given researchers a wealth of material to study, leading to a flurry of papers analyzing its chain-of-thought outputs, its reward model design, and even its failures, which ultimately advances the field's understanding of LLM cognition.</p> </li> <li> <p>Acceleration of Open-Source AI: R1's impact on the open-source AI movement cannot be overstated. It demonstrated that an relatively small lab (DeepSeek-AI, a spin-off from a finance firm (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat)) could not only build a model competitive with those from tech giants, but also open-source it under a permissive license. This has encouraged other companies and organizations to open-source their strong models as well. Within months of R1's release, we saw Alibaba announce Qwen 2.5 Max (72B) and QwQ-32B, open-sourced under Apache 2.0, claiming performance on par with R1 (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). Likewise, Meta pushed forward with Llama 3.1 405B and made it available to the public (What Is Meta's Llama 3.1 405B? How It Works, Use Cases &amp; More). Each of these moves is a strategic response to R1 -- essentially a realization that open models can drive rapid adoption. DeepSeek R1 quickly became one of the most visited AI model websites globally (second only to OpenAI's), indicating enormous interest and community uptake (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). This user base in turn contributes back improvements, extensions (such as integrations into applications, or fine-tuned domain-specific versions), creating a positive feedback loop for the model's evolution. We also see governments and NGOs taking note: for instance, R1 being open means countries concerned about access to GPT-4 have an alternative they can control. Strategically, this could reduce reliance on a handful of AI providers and distribute AI capabilities more evenly worldwide.</p> </li> <li> <p>Competitive Pressure on Proprietary Models: Strategically, OpenAI, Google, and Anthropic now face a more level playing field. While they still hold some advantages (compute resources, multi-modal capabilities, proprietary data), the margin of AI superiority they enjoyed shrank because of models like R1. This has likely spurred them to invest in next-generation technologies -- e.g., focusing on multimodality (images, video, tools) where open models haven't yet caught up, or massively increasing context length (as Google did with a 2 million token context experiment) (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat). It may also influence pricing and openness: OpenAI, for example, might eventually consider offering on-premise versions or weight licenses of their models to compete with the free availability of R1 (something that was previously unthinkable). We have already seen a \"model proliferation\" effect: many new startups releasing specialized LLMs (for reasoning, for dialogue, for coding) because R1 proved that with innovation, newcomers can beat established models in niches. Strategically, Big Tech can no longer assume their lead in LLMs will translate to usage monopoly -- the community can and will catch up. This dynamic could lead to faster progress overall, but also raises questions of safety: powerful models are now widely available, so ensuring responsible use becomes a distributed challenge. In response, initiatives for open model evaluation and alignment (some sponsored by governments) have ramped up, treating models like R1 as benchmarks for what open models can do.</p> </li> <li> <p>Ecosystem and Applications: DeepSeek R1 has enriched the AI ecosystem by being integrated into various platforms and pipelines. Its strong performance in retrieval-augmented generation (RAG) scenarios has been noted -- one guide calls it a way to \"supercharge RAG projects with DeepSeek R1\", thanks to its precise reasoning over retrieved documents (Supercharge RAG Projects with DeepSeek R1 AI Reasoning Model). This is steering application developers to consider using R1 for tasks like enterprise Q&amp;A, analytics, and decision support, often in place of closed models. The strategic consequence is that organizations can build sophisticated AI features without dependency on a single vendor. We also see R1's influence in education and research: universities use R1 as a teaching tool for AI courses (since it's inspectable), and researchers build on top of R1 for things like agent frameworks (knowing that R1 can handle long-horizon reasoning). Open-source community projects, from chatbots to coding assistants, now frequently include DeepSeek R1 or its distilled variants as a backbone. This broad adoption is shaping AI development priorities -- for example, there is increased interest in scaling laws for reinforcement learning (thanks to R1's approach) and in safety for open models (making sure models like R1 are as rigorously tested as closed ones).</p> </li> </ul> <p>In conclusion, DeepSeek R1's launch and open-source release have acted as a catalyst in the AI world: accelerating innovation, encouraging openness, and driving competitive and collaborative responses in equal measure. It has proven that cutting-edge AI need not be locked behind corporate doors, and in doing so, it has shifted strategic priorities -- from how models are trained (more focus on reasoning and RL) to how they are shared (greater openness) to how they are deployed (embracing on-premise and hybrid solutions). The \"DeepSeek effect\" -- a small team achieving big AI breakthroughs and sharing them -- is inspiring a new wave of AI development globally, ensuring that the race for ever more intelligent systems benefits from the collective efforts of the whole research community, not just a few large players (deepseek-ai/DeepSeek-R1 - Hugging Face). The end result is a more vibrant, accessible, and fast-evolving AI landscape, with DeepSeek R1 having carved its name as one of the pivotal contributions to this shift.</p>"},{"location":"concepts/deepseek-r1/#sources","title":"Sources","text":"<ul> <li>DeepSeek-AI (2025). DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning (arXiv preprint) (deepseek-ai/DeepSeek-R1 - Hugging Face) (deepseek-ai/DeepSeek-R1 - Hugging Face).</li> <li>DeepSeek R1 HuggingFace Model Card (deepseek-ai/DeepSeek-R1 - Hugging Face) (deepseek-ai/DeepSeek-R1 - Hugging Face) (deepseek-ai/DeepSeek-R1 - Hugging Face).</li> <li>VentureBeat (2025). Alibaba's QwQ-32B matches DeepSeek-R1 with way smaller compute (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat) (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat).</li> <li>SimilarWeb via VentureBeat -- R1 web traffic and model popularity (qwq-32b-launches-high-efficiency-performance-reinforcement | VentureBeat).</li> <li>DeepSeek R1 GitHub &amp; Ollama Library -- usage and licensing info (deepseek-r1) (deepseek-r1).</li> <li>OneClick AI Solutions (2025). DeepSeek R1 vs Qwen 2.5 Max -- benchmark comparison (DeepSeek R1 vs Qwen 2.5 Max: A Detailed Comparison of Features and Performance) (DeepSeek R1 vs Qwen 2.5 Max: A Detailed Comparison of Features and Performance).</li> <li>Azure AI Foundry Blog (2025). DeepSeek R1 on Azure -- enterprise adoption (DeepSeek R1 is now available on Azure AI Foundry and GitHub | Microsoft Azure Blog).</li> <li>LifeArchitect.ai Model Leaderboard (2025) -- MMLU/GPQA rankings.</li> <li>DeepSeek-V3 GitHub -- technical details on MoE architecture (GitHub - deepseek-ai/DeepSeek-V3).</li> <li>Meta AI (2024). Llama 3.1 405B announcement -- scale of open models (What Is Meta's Llama 3.1 405B? How It Works, Use Cases &amp; More).</li> </ul>"},{"location":"concepts/docstring/","title":"Python Docstring and Decorators","text":"<p>A docstring in Python is a string literal that appears as the first statement in a module, function, class, or method definition. It is used to document the purpose, behavior, parameters, and return values of the code object. Python docstrings are enclosed by triple quotes (<code>\"\"\"</code> or <code>'''</code>) and can span multiple lines.</p> <p>When working with AI agents (particularly in frameworks like LangChain or SmoLAgents), the <code>@tool</code> decorator or annotation near a docstring is a special decorator or marker that designates a function as being callable by an agent. This process typically works as follows:</p> <ol> <li> <p>Function Definition: Create a Python function with clearly defined inputs and outputs.</p> </li> <li> <p>Docstring Documentation: Write a comprehensive docstring that describes:</p> </li> <li>What the function does</li> <li>The parameters it accepts</li> <li>The return value and type</li> <li> <p>Any exceptions it might raise</p> </li> <li> <p>@tool Annotation: Include the <code>@tool</code> marker within the docstring or as a decorator above the function definition.</p> </li> <li> <p>Registration: The function is automatically registered in the agent's available toolset.</p> </li> <li> <p>Discovery and Usage: When the agent needs to perform a task related to the function's capability, it can discover and call this function based on the description in the docstring.</p> </li> </ol> <p>Example: <pre><code>def search_database(query: str) -&gt; list:\n    \"\"\"\n    @tool\n    Search the database for information matching the query string.\n\n    Args:\n        query: The search term to look for in the database\n\n    Returns:\n        A list of matching records from the database\n    \"\"\"\n    # Implementation details\n    results = db.execute_search(query)\n    return results\n</code></pre></p> <p>This approach allows agents to understand not just what functions are available, but when and how to use them appropriately based on their documented purpose and parameters.</p>"},{"location":"concepts/docstring/#decorators","title":"Decorators","text":"<p>A decorator is a design pattern in Python implemented as a special syntax that allows a function, method, or class to be modified or extended without changing its source code. </p> <p>Decorators use the <code>@</code> symbol followed by a decorator name placed above the definition of the function to be decorated. They effectively wrap the target function, enabling pre-processing of arguments, post-processing of return values, modification of behavior, or registration within a larger system.</p>"},{"location":"concepts/docstring/#decorators-in-smolagents-for-tool-identification","title":"Decorators in SmoLAgents for Tool Identification","text":"<p>In the SmoLAgents library, decorators are commonly used to identify and register functions as tools that can be discovered and used by agents. Here are examples of how decorators are used in this context:</p> <pre><code>from smolagents import tool\n\n@tool\ndef search_web(query: str) -&gt; list:\n    \"\"\"\n    Search the web for information related to the query.\n\n    Args:\n        query: The search terms to look for\n\n    Returns:\n        A list of search results\n    \"\"\"\n    # Implementation details\n    return web_search_implementation(query)\n</code></pre> <p>More specific decorator examples in SmoLAgents might include:</p> <pre><code>from smolagents import final_answer_tool\n\n@final_answer_tool\ndef provide_solution(answer: str) -&gt; str:\n    \"\"\"\n    Provide the final answer to the user's question.\n\n    Args:\n        answer: The complete solution to return to the user\n\n    Returns:\n        Confirmation message\n    \"\"\"\n    # Implementation details\n    return f\"Final answer recorded: {answer}\"\n</code></pre> <p>The SmoLAgents library might also support specialized tool decorators for different capabilities:</p> <pre><code>from smolagents import data_tool, code_tool, io_tool\n\n@data_tool\ndef analyze_dataset(file_path: str) -&gt; dict:\n    \"\"\"Process and analyze the dataset at the given path.\"\"\"\n    # Implementation\n\n@code_tool\ndef execute_python(code_snippet: str) -&gt; dict:\n    \"\"\"Execute the provided Python code snippet.\"\"\"\n    # Implementation\n\n@io_tool\ndef save_result(filename: str, content: str) -&gt; bool:\n    \"\"\"Save content to a file with the given filename.\"\"\"\n    # Implementation\n</code></pre> <p>These decorators serve multiple purposes: 1. Registering the function in the agent's available toolset 2. Providing metadata about the tool's purpose and usage patterns 3. Potentially applying input validation or output formatting 4. Enabling the agent to reason about which tool to use based on its current task</p>"},{"location":"concepts/knowledge-representation/","title":"Knowledge Representation","text":"<p>Knowledge Representation are the methods and structures used to encode information about the world.</p> <p>In this book, we focus on knowledge representation in a form that an agent can utilize for reasoning and decision-making. It provides the foundation for an agent's understanding of its environment, goals, and possible actions.</p> <p>Knowledge representation encompasses:</p> <ol> <li> <p>Symbolic Structures: Formalized frameworks such as ontologies, semantic networks, frames, rules, or logic-based systems that capture entities, relationships, and constraints in the agent's domain.</p> </li> <li> <p>Information Organization: Methods for structuring knowledge in ways that facilitate efficient access, inference, and updates as the agent interacts with its environment.</p> </li> <li> <p>Reasoning Mechanisms: Techniques that allow agents to derive new knowledge from existing representations, evaluate truth values, and make decisions based on incomplete information.</p> </li> <li> <p>Semantic Grounding: Connections between abstract symbols and their real-world referents, allowing the agent to map internal representations to external phenomena.</p> </li> </ol> <p>Effective knowledge representation systems in agents balance expressiveness (the ability to represent complex knowledge) with computational efficiency (allowing the agent to reason within reasonable time and resource constraints).</p>"},{"location":"concepts/knowledge-representation/#public-vs-private-knowledge","title":"Public vs Private Knowledge","text":""},{"location":"concepts/project-rules/","title":"Project Rules for Intelligent Agents Course","text":"<p>These rules should be loaded into the LLM Projects area to provide context when we are generating content or code.</p>"},{"location":"concepts/project-rules/#definition-rule","title":"Definition Rule","text":"<p>When we ask for an ISO definition of a term or concept, always give use a term that follows the ISO 11179 metadata registry guidelines for terms.  These definitions should be:</p> <ol> <li>Precise - use words in the definition that have unambiguous meaning</li> <li>Concise - use the fewest words that will still provide a clear definition</li> <li>Distinct - check with the glossary.md file in the project to make sure the term is distinct from other terms</li> <li>Non-circular - do not place the term in the definition</li> <li>Unencumbered with business rules - do not places business rules within the definition</li> <li>Singular form - do not use plurals for terms</li> </ol> <p>After you define the term in 1 or 2 sentences, create a new paragraph and give some context of why this term is relevant to a course on intelligent software agents.  Then create a new paragraph that begins with **Example: ** and provide and example of usage within this context.</p> <p>Place the term in a level 4 markdown header.</p> <p>Example:</p>"},{"location":"concepts/project-rules/#graphics-processing-unit","title":"Graphics Processing Unit","text":"<p>A specialized device designed to accelerate matrix operations using parallel processing.</p> <p>In the context of intelligent software agents, GPUs are relevant as they provide parallel processing capabilities that significantly accelerate machine learning model training and inference operations, enabling more complex and performant AI systems. The parallel architecture of GPUs allows for efficient matrix operations that form the computational backbone of many neural network implementations.</p> <p>Example: When deploying a code agent for real-time coding tasks, a developer would typically configure the environment to utilize a GPU to ensure the agent can perform inference operations at sufficient speed, rather than relying solely on CPU processing.</p>"},{"location":"concepts/project-rules/#python-code-generation-rules","title":"Python Code Generation Rules","text":"<p>When generating Python code, always provide a brief comment at the beginning of the code that describes what the code should do.</p> <p>When creating a python function that could be used by an agent, make sure there is a docstring in the function that describes the name, description, input parameters and output type.</p> <p>Add the decorator <code>@tool</code> when generating a function that can be used by an agent.</p>"},{"location":"concepts/react-framework/","title":"Using the ReAct Framework for Textbook Generation","text":""},{"location":"concepts/react-framework/#overview","title":"Overview","text":"<p>This document demonstrates how to use the ReAct (Reasoning + Action) framework to generate a comprehensive AI textbook on \"Introduction to Large Language Models.\" The ReAct approach interleaves reasoning traces with concrete actions, allowing for a more dynamic and informed content creation process.</p>"},{"location":"concepts/react-framework/#implementation-example","title":"Implementation Example","text":"<pre><code>from smolagents import CodeAgent, FinalAnswerTool\nfrom smolagents.models import Model\nimport research_tools\nimport content_evaluation\nimport citation_manager\n\n# Initialize the model and tools\nmodel = Model(name=\"deepseek-r1:7b\")  # Using deepseek model as shown in documents\nagent = CodeAgent(\n    model=model,\n    tools=[\n        research_tools.SearchTool(),  # For finding relevant information\n        research_tools.SummarizeTool(),  # For condensing research\n        content_evaluation.AccuracyCheckerTool(),  # Verifies factual accuracy\n        content_evaluation.ReadabilityTool(),  # Ensures appropriate complexity level\n        citation_manager.CitationTool(),  # Manages references\n        FinalAnswerTool()  # Finalizes sections\n    ]\n)\n\ndef generate_textbook_chapter(topic, audience_level=\"undergraduate\"):\n    \"\"\"Generate a textbook chapter using the ReAct approach\"\"\"\n    prompt = f\"\"\"\n    Generate a textbook chapter on '{topic}' for {audience_level} students.\n    Follow these steps:\n    1. Research the topic thoroughly\n    2. Create an outline\n    3. Draft each section\n    4. Review for accuracy and readability\n    5. Add appropriate citations\n    6. Finalize the chapter\n\n    Use reasoning to plan your approach and actions to gather information and validate content.\n    \"\"\"\n\n    return agent.run(prompt)\n</code></pre>"},{"location":"concepts/react-framework/#react-process-breakdown","title":"ReAct Process Breakdown","text":"<p>Below is how the process unfolds for generating a chapter on \"Attention Mechanisms in Transformers\":</p>"},{"location":"concepts/react-framework/#1-initial-reasoning-trace","title":"1. Initial Reasoning Trace","text":"<pre><code>I need to create a comprehensive textbook chapter on attention mechanisms in Transformers. \nThis is a complex topic that requires clear explanations and examples.\nFirst, I should gather the latest research and fundamental concepts.\n</code></pre>"},{"location":"concepts/react-framework/#2-action-research","title":"2. Action: Research","text":"<pre><code>search_results = search_tool(\"attention mechanisms transformers technical explanation\")\nfoundational_papers = search_tool(\"Attention Is All You Need paper key concepts\")\nrecent_developments = search_tool(\"recent advances attention mechanisms 2023\")\n\n# Summarize the findings\ncore_concepts = summarize_tool(search_results + foundational_papers, \n                              focus=\"fundamental concepts for undergraduate level\")\n</code></pre>"},{"location":"concepts/react-framework/#3-reasoning-trace-for-outline","title":"3. Reasoning Trace for Outline","text":"<pre><code>Based on my research, I should structure this chapter to build knowledge progressively:\n- Start with the intuition behind attention\n- Move to mathematical formulation\n- Then cover different types of attention\n- Include practical examples\n- Discuss limitations and recent improvements\n</code></pre>"},{"location":"concepts/react-framework/#4-action-create-outline","title":"4. Action: Create Outline","text":"<pre><code>outline = {\n    \"title\": \"Attention Mechanisms in Transformer Models\",\n    \"sections\": [\n        {\n            \"title\": \"1. Introduction to Attention\",\n            \"subsections\": [\n                \"1.1 The Problem of Sequence Context\",\n                \"1.2 Intuition Behind Attention\"\n            ]\n        },\n        {\n            \"title\": \"2. Mathematics of Attention\",\n            \"subsections\": [\n                \"2.1 Queries, Keys, and Values\",\n                \"2.2 Scaled Dot-Product Attention\",\n                \"2.3 Multi-Head Attention\"\n            ]\n        },\n        # Additional sections...\n    ]\n}\n</code></pre>"},{"location":"concepts/react-framework/#5-reasoning-trace-for-content-creation","title":"5. Reasoning Trace for Content Creation","text":"<pre><code>For the introduction section, I need to make the concept accessible.\nI should use an analogy that undergraduate students can relate to.\nThe spotlight analogy works well for explaining how attention focuses on different parts of input.\n</code></pre>"},{"location":"concepts/react-framework/#6-action-draft-introduction","title":"6. Action: Draft Introduction","text":"<pre><code>introduction = \"\"\"\n# 1. Introduction to Attention\n\nImagine you're in a crowded room trying to follow a conversation. Despite many people talking simultaneously, you can focus on specific voices by directing your attention. Neural networks face a similar challenge when processing sequences like sentences - they need a mechanism to focus on relevant parts of the input. This is where attention mechanisms come in.\n\n## 1.1 The Problem of Sequence Context\n\nBefore attention mechanisms, sequence models like RNNs struggled with long-range dependencies...\n\"\"\"\n\n# Check readability\nreadability_score = readability_tool.analyze(introduction)\nif readability_score &lt; 0.7:  # Below target\n    introduction = readability_tool.improve(introduction, target_level=\"undergraduate\")\n</code></pre>"},{"location":"concepts/react-framework/#7-reasoning-trace-for-technical-content","title":"7. Reasoning Trace for Technical Content","text":"<pre><code>The mathematical section needs to balance rigor with accessibility.\nI should include the formulas but break them down step by step.\nVisual representations would help here - I'll include pseudocode and a diagram.\n</code></pre>"},{"location":"concepts/react-framework/#8-action-create-technical-section-with-verification","title":"8. Action: Create Technical Section with Verification","text":"<pre><code>math_section = \"\"\"\n# 2. Mathematics of Attention\n\n## 2.1 Queries, Keys, and Values\n\nThe attention mechanism revolves around three key concepts: queries, keys, and values...\n\"\"\"\n\n# Verify accuracy\naccuracy_issues = accuracy_checker_tool.verify(math_section, domain=\"machine learning\")\nif accuracy_issues:\n    for issue in accuracy_issues:\n        math_section = accuracy_checker_tool.suggest_correction(math_section, issue)\n</code></pre>"},{"location":"concepts/react-framework/#9-reasoning-trace-for-citations","title":"9. Reasoning Trace for Citations","text":"<pre><code>I need to properly cite the original Transformer paper and recent work.\nThe citations should follow academic standards and include DOI where available.\n</code></pre>"},{"location":"concepts/react-framework/#10-action-add-citations","title":"10. Action: Add Citations","text":"<pre><code>citations = citation_tool.generate([\n    {\"title\": \"Attention Is All You Need\", \"authors\": \"Vaswani et al.\", \"year\": 2017},\n    {\"title\": \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \n     \"authors\": \"Devlin et al.\", \"year\": 2019},\n    # Additional citations...\n])\n\nchapter_with_citations = citation_tool.insert_citations(full_chapter, citations)\n</code></pre>"},{"location":"concepts/react-framework/#11-final-reasoning-trace","title":"11. Final Reasoning Trace","text":"<pre><code>The chapter is nearly complete. I should review it once more for coherence and ensure\nthe progression of ideas flows naturally. The examples should reinforce the concepts.\n</code></pre>"},{"location":"concepts/react-framework/#12-action-finalize-content","title":"12. Action: Finalize Content","text":"<pre><code>final_chapter = final_answer_tool(\n    content=chapter_with_citations,\n    metadata={\n        \"title\": \"Attention Mechanisms in Transformer Models\",\n        \"audience\": \"undergraduate\",\n        \"prerequisites\": [\"Basic Neural Networks\", \"Sequence Models\"],\n        \"learning_objectives\": [\n            \"Understand the intuition behind attention mechanisms\",\n            \"Grasp the mathematical formulation of self-attention\",\n            \"Identify different types of attention and their applications\",\n            \"Implement a simple attention mechanism\"\n        ]\n    }\n)\n</code></pre>"},{"location":"concepts/react-framework/#benefits-of-the-react-approach-for-textbook-generation","title":"Benefits of the ReAct Approach for Textbook Generation","text":"<ol> <li> <p>Dynamic Research Integration: The ability to search for and incorporate the latest research ensures the textbook remains current and accurate.</p> </li> <li> <p>Quality Control: Interleaved reasoning and verification steps help catch errors and improve content quality throughout the creation process.</p> </li> <li> <p>Adaptive Content Creation: The framework can adjust explanations based on readability metrics, ensuring the content matches the intended audience level.</p> </li> <li> <p>Structured Development: The reasoning traces help maintain a coherent narrative and logical progression of ideas.</p> </li> <li> <p>Comprehensive Citations: Action steps for citation management ensure proper attribution and academic rigor.</p> </li> </ol>"},{"location":"concepts/react-framework/#implementation-considerations","title":"Implementation Considerations","text":"<ul> <li>Tool Selection: The specific tools integrated with the CodeAgent should match the subject matter of the textbook.</li> <li>Model Capabilities: More complex textbooks benefit from more capable foundation models like the deepseek-r1 model.</li> <li>Feedback Integration: The process can incorporate human expert feedback at key checkpoints.</li> <li>Multimodal Content: For subjects requiring diagrams or visualizations, additional tools for generating figures can be integrated.</li> </ul>"},{"location":"concepts/react-framework/#conclusion","title":"Conclusion","text":"<p>The ReAct framework provides a powerful approach to textbook generation by combining the reasoning capabilities of language models with the ability to take concrete actions like research, verification, and citation management. This interleaved process produces more comprehensive, accurate, and well-structured educational content than approaches that separate reasoning from action.</p>"},{"location":"concepts/workflows/","title":"Agentic Workflows","text":"<p>Here is a brief overview of the key workflows we use when building intelligent agents.</p>"},{"location":"concepts/workflows/#overview","title":"Overview","text":""},{"location":"concepts/workflows/#prompt-iteration","title":"Prompt Iteration","text":"<ul> <li>The LLM is trained on public knowledge but does not know anything about your goals, your products or your customers </li> <li>A human decides if the response is OK</li> <li>The human is \u201ctrained\u201d to write better prompts</li> </ul>"},{"location":"concepts/workflows/#prompt-enrichment-from-knowledge-base-rag","title":"Prompt Enrichment from Knowledge Base (RAG)","text":"<ul> <li>In this model, we enrich your prompt with content from a local knowledge based.</li> <li>The key to enrichment is to find the right content related to the question in your prompt.</li> <li>Enriching your prompt with local knowledge is much more cost effective than fine tuning your own language model.</li> </ul>"},{"location":"concepts/workflows/#prompt-enrichment-from-knowledge-graph","title":"Prompt Enrichment from Knowledge Graph","text":"<ul> <li>In this workflow we replace the generic knowledge based with a knowledge graph that has similarity indexes on reinvent components</li> <li>Knowledge graphs store models of the real world with better precision than tabular data such as CSV files or flat files with per-query relational JOIN operations </li> </ul>"},{"location":"concepts/workflows/types-of-coding/","title":"Types of Coding","text":""},{"location":"concepts/workflows/types-of-coding/#procedural-code","title":"Procedural Code","text":"<p>Procedural code is a programming paradigm that focuses on defining a sequence of operations for a computer to execute. It emphasizes how a program should accomplish tasks through explicit step-by-step instructions, control flow structures, and state changes. Procedural code typically organizes functionality into procedures or functions that are called in a predetermined order.</p>"},{"location":"concepts/workflows/types-of-coding/#comparison-with-declarative-and-agentic-code","title":"Comparison with Declarative and Agentic Code","text":"Aspect Procedural Code Declarative Code Agentic Code Focus How to perform tasks (step-by-step instructions) What should be accomplished (desired outcome) Goals to achieve (with autonomous execution) --- --- --- --- Control Flow Explicitly defined by the programmer Implicit, determined by the execution environment Dynamic, determined by the agent based on goals and environment State Management Direct manipulation of program state Minimized state changes, often immutable Internal state that influences decision-making Abstraction Level Lower-level, focused on operations Higher-level, focused on relationships High-level, focused on objectives Examples Imperative languages like C, traditional Python scripts SQL, HTML, CSS, functional programming AI agents, automated planning systems Decision Making Predetermined by the programmer Rules-based evaluation Autonomous reasoning and adaptation Adaptability Requires explicit programming for different scenarios Adapts within defined constraints Can discover novel approaches to solve problems <p>Procedural code explicitly states the sequence of steps to follow, while declarative code describes the desired result without specifying the exact implementation. Agentic code, the newest paradigm, introduces autonomy and goal-oriented behavior, allowing the system to determine appropriate actions based on its understanding of objectives and environmental conditions.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This section describes the steps you need to take to build and test your own intelligent agents on your own local computer.  The ideal configuration is to have a local GPU with at least 12GB of RAM on the GPU.  Although you can still run agents on a local computer without a GPU, the response times might be so slow that you will become frustrated.  My local GPU is a NVIDIA RTX 2080 Ti which returns 52 tokens per second.</p> <p>You can test the tokens per second on your local computer using our lab on the Deep Learning Course.</p> <p>If you are not an expert on setting up Python, then setting up your local system can be VERY frustrating.</p>"},{"location":"getting-started/#picking-the-right-agent-library","title":"Picking the Right Agent Library","text":"<p>Picking the Right Library</p>"},{"location":"getting-started/#installing-gpu-drivers","title":"Installing GPU Drivers","text":"<p>GPU Driver Version</p>"},{"location":"getting-started/#conda-setup","title":"Conda Setup","text":"<p>Conda Setup</p>"},{"location":"getting-started/#testing-python-libraries","title":"Testing Python Libraries","text":"<p>Testing Libraries</p>"},{"location":"getting-started/#ollama-setup","title":"Ollama Setup","text":"<p>Ollama Setup</p>"},{"location":"getting-started/conda-setup/","title":"Conda Setup","text":""},{"location":"getting-started/conda-setup/#install-conda","title":"Install Conda","text":""},{"location":"getting-started/conda-setup/#creating-a-conda-python-environment","title":"Creating a Conda Python Environment","text":"<pre><code>conda create -n \"agents\" python=3\nconda activate agents\n# note we are using cuda 12.6\npip install --upgrade torch torchvision torchaudio \\\n    --index-url https://download.pytorch.org/whl/cu126\npip install --upgrade transformers accelerate bitsandbytes\n</code></pre> <p>Before you activate the conda environment your prompt will have the string 'base' in it like this:</p> <pre><code>(base) $\n</code></pre> <pre><code>conda activate agents\n(agents) $\n</code></pre> <p>You can get a full list of all the Python libraries by using the <code>pip freeze</code> command.</p> <pre><code>pip freeze\n</code></pre>"},{"location":"getting-started/gpu-drivers/","title":"Installing GPU Drivers","text":"<p>We need to find out the exact version of CUDA we have on our local GPU before we install Python libraries.  We can find this out by running the <code>nvcc</code> command at the UNIX shell.</p> <pre><code>nvcc --version\n</code></pre> <p>Response</p> <pre><code>$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2024 NVIDIA Corporation\nBuilt on Fri_Jun_14_16:34:21_PDT_2024\nCuda compilation tools, release 12.6, V12.6.20\nBuild cuda_12.6.r12.6/compiler.34431801_0\n</code></pre> <p>Note that the CUDA release is 12.6.  We will use this later in our installation process.</p>"},{"location":"getting-started/install-smolagents/","title":"Intalling SmolAgents","text":""},{"location":"getting-started/install-smolagents/#why-smolagents","title":"Why SmolAgents","text":"<p>AI agent frameworks are often criticized with two points:</p> <ol> <li>They build too many layers of abstraction, making them rigid and challenging to learn and use.</li> <li>They focus on \"workflows\" rather than building agents that can dynamically collaborate on their own using Python data structures.</li> </ol> <p>Smolagents has qualities that make ideal for simple agentic applications.</p> <ul> <li>The framework's abstractions are kept at a minimum.</li> <li>While most frameworks have the agents define their actions in JSON/text format, smolagents' main approach is <code>Code Agents</code> in which actions are written as Python code snippets (this is different from agents that write code).</li> <li>Being a Hugging Face framework, smolagents integrates well with the Hub and the Transformers library. You can use many models from the hub (some of them you can only use as a Pro user), and you can also work with proprietary models from OpenAI, Anthropic, etc.</li> <li>You can easily utilize the already-provided tools, or define your custom tools with minimum effort, almost as simple as writing a Python function.</li> </ul> <p>These qualities are what, on paper, make smolagents a plug-and-play with AI agents with little effort, so let's see if they hold in practice.</p>"},{"location":"getting-started/install-smolagents/#installing-smolagents","title":"Installing SmolAgents","text":"<p>Make sure you are in the correct environment.  In this class we used conda to create an environment called \"agents\"</p> <pre><code>conda activate agents\n</code></pre> <p>Next, use <code>pip</code> to install the smolagents Python library</p> <pre><code>pip install smolagents\npip install 'smolagents[litellm]'\n</code></pre> <p>The second line also includes the litellm module.</p>"},{"location":"getting-started/install-smolagents/#testing-smolagents","title":"Testing SmolAgents","text":""},{"location":"getting-started/install-smolagents/#basic-test","title":"Basic Test","text":"<pre><code>from smolagents import SmoLAgent\nprint(\"SmoLAgents installed successfully!\")\n</code></pre>"},{"location":"getting-started/install-smolagents/#test-with-version-and-dir","title":"Test with Version and Dir","text":"<p>We can also use the Python <code>dir</code> function on the smolagents module to get a detailed list of the items in the smolagents Python library.</p> <pre><code>import smolagents\n\n# Print version and available modules\nprint(f\"SmoLAgents version: {smolagents.__version__}\")\nprint(f\"Available in smolagents: {dir(smolagents)}\")\n\n# Try to create a \"TestAgent\"\ntry:\n    from smolagents.agents import Agent\n    agent = Agent(name=\"TestAgent\")\n    print(f\"Agent created successfully with name: {agent.name}\")\nexcept ImportError as e:\n    print(f\"Could not import Agent: {e}\")\n\nprint(\"SmoLAgents is installed!\")\n</code></pre> <pre><code>SmoLAgents version: 1.9.2\nAvailable in smolagents: ['AUTHORIZED_TYPES', 'ActionStep', 'AgentAudio', 'AgentError', 'AgentExecutionError', 'AgentGenerationError', 'AgentImage', 'AgentLogger', 'AgentMaxStepsError', 'AgentMemory', 'AgentParsingError', 'AgentText', 'AgentType', 'Any', 'AzureOpenAIServerModel', 'BASE_BUILTIN_MODULES', 'Callable', 'ChatMessage', 'CodeAgent', 'Dict', 'DuckDuckGoSearchTool', 'E2BExecutor', 'EMPTY_PROMPT_TEMPLATES', 'FinalAnswerPromptTemplate', 'FinalAnswerTool', 'Generator', 'GoogleSearchTool', 'GradioUI', 'Group', 'HfApiModel', 'List', 'LiteLLMModel', 'LocalPythonInterpreter', 'LogLevel', 'MLXModel', 'ManagedAgentPromptTemplate', 'MessageRole', 'Model', 'Monitor', 'MultiStepAgent', 'OpenAIServerModel', 'Optional', 'Panel', 'Path', 'PlanningPromptTemplate', 'PlanningStep', 'PromptTemplates', 'PythonInterpreterTool', 'Rule', 'Set', 'SpeechToTextTool', 'StrictUndefined', 'SystemPromptStep', 'TOOL_MAPPING', 'TaskStep', 'Template', 'Text', 'Tool', 'ToolCall', 'ToolCallingAgent', 'ToolCollection', 'TransformersModel', 'Tuple', 'TypedDict', 'Union', 'UserInputTool', 'VisitWebpageTool', 'YELLOW_HEX', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_function_type_hints_utils', 'agent_types', 'agents', 'argparse', 'cli', 'create_repo', 'default_tools', 'deque', 'e2b_executor', 'evaluate_python_code', 'fix_final_answer_code', 'getLogger', 'get_clean_message_list', 'get_variable_names', 'gradio_ui', 'handle_agent_output_types', 'importlib', 'inspect', 'jinja2', 'json', 'launch_gradio_demo', 'leopard_prompt', 'load_dotenv', 'load_model', 'load_tool', 'local_python_executor', 'logger', 'main', 'make_init_file', 'memory', 'metadata_update', 'models', 'monitoring', 'os', 'parse_arguments', 'parse_code_blobs', 'parse_json_tool_call', 'populate_template', 're', 'snapshot_download', 'stream_to_gradio', 'tempfile', 'textwrap', 'time', 'tool', 'tool_role_conversions', 'tool_validation', 'tools', 'truncate_content', 'upload_folder', 'utils', 'yaml']\n\nCould not import Agent: cannot import name 'Agent' from 'smolagents.agents' (/home/dan/miniconda3/envs/agents/lib/python3.13/site-packages/smolagents/agents.py)\n\nSmoLAgents is installed!\n</code></pre>"},{"location":"getting-started/install-smolagents/#creating-a-model","title":"Creating a Model","text":"<p>We can get a list of the agent types by looking for all method names with the string \"Agent\" it it.</p> <pre><code># test-smolagents-final.py\nimport smolagents\nfrom smolagents import FinalAnswerTool  # Import directly from smolagents\nfrom smolagents.models import Model\n\nprint(f\"SmoLAgents version: {smolagents.__version__}\")\n\ntry:\n    # Create a dummy model\n    dummy_model = Model(name=\"dummy-model\")\n\n    # Create a basic tool\n    final_answer_tool = FinalAnswerTool()\n\n    print(\"Successfully created tool and model!\")\n\n    # Print available agent types\n    print(\"Available agent types:\")\n    print([name for name in dir(smolagents) if 'Agent' in name])\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\nprint(\"SmoLAgents installation test complete!\")\n</code></pre> <pre><code>SmoLAgents version: 1.9.2\nSuccessfully created tool and model!\n\nAvailable agent types:\n['AgentAudio', 'AgentError', 'AgentExecutionError', 'AgentGenerationError', 'AgentImage', 'AgentLogger', 'AgentMaxStepsError', 'AgentMemory', 'AgentParsingError', 'AgentText', 'AgentType', 'CodeAgent', 'ManagedAgentPromptTemplate', 'MultiStepAgent', 'ToolCallingAgent']\n\nSmoLAgents installation test complete!\n</code></pre>"},{"location":"getting-started/install-smolagents/#references","title":"References","text":"<p>Build Fast with AI Video</p>"},{"location":"getting-started/ollama-python/","title":"Ollama Python","text":""},{"location":"getting-started/ollama-python/#calling-deepseek-r1-via-ollama","title":"Calling DeepSeek R1 via Ollama","text":"<pre><code>#!/usr/bin/env python3\n# working-code-agent.py\nimport smolagents\nfrom smolagents import FinalAnswerTool, CodeAgent\nimport requests\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass OllamaModel:\n    \"\"\"Simple Ollama model that ignores unsupported parameters\"\"\"\n\n    def __init__(self, model_name, base_url=\"http://localhost:11434\"):\n        self.model_name = model_name\n        self.base_url = base_url\n        self.generate_endpoint = f\"{base_url}/api/generate\"\n        logger.info(f\"Initialized OllamaModel with model: {model_name}\")\n\n    def __call__(self, prompt, **kwargs):\n        \"\"\"Generate response from Ollama model, ignore any kwargs\"\"\"\n        # Handle various prompt formats\n        if isinstance(prompt, list):\n            # Convert list to text\n            if len(prompt) &gt; 0 and isinstance(prompt[0], dict):\n                # Extract from messages format\n                full_prompt = \"\\n\".join([msg.get(\"content\", \"\") for msg in prompt if isinstance(msg, dict) and \"content\" in msg])\n            else:\n                # Join list items\n                full_prompt = \"\\n\".join([str(item) for item in prompt if item])\n        else:\n            # Use as is\n            full_prompt = str(prompt)\n\n        payload = {\n            \"model\": self.model_name,\n            \"prompt\": full_prompt,\n            \"stream\": False\n        }\n\n        try:\n            logger.info(f\"Sending request to Ollama with prompt length: {len(full_prompt)} chars\")\n            response = requests.post(self.generate_endpoint, json=payload)\n\n            if response.status_code == 200:\n                result = response.json()\n                return result.get(\"response\", \"\")\n            else:\n                logger.error(f\"Ollama request failed: {response.status_code}\")\n                return \"Error: Failed to get response from Ollama\"\n        except Exception as e:\n            logger.error(f\"Error: {e}\")\n            return f\"Error: {str(e)}\"\n\ndef main():\n    try:\n        # Create the model\n        model = OllamaModel(\"deepseek-r1\")\n\n        # Test direct model use\n        response = model(\"What is 2 + 2?\")\n        logger.info(f\"Direct model response: {response[:100]}...\")\n\n        # Create a simple task for the agent\n        task = \"Calculate the sum of numbers from 1 to 5\"\n\n        # Create the agent with minimal setup - REMOVED verbose parameter\n        agent = CodeAgent(\n            model=model,\n            tools=[FinalAnswerTool()]\n        )\n\n        # Run the agent\n        logger.info(f\"Running agent with task: {task}\")\n        result = agent.run(task)\n\n        logger.info(f\"Agent result: {result}\")\n\n        # If that works, try a more complex task\n        complex_task = \"\"\"\n        I need to solve this problem:\n        1. Calculate the area of a rectangle with width 6 meters and length 9 meters\n        2. Then calculate what percentage of that area would be covered by a square with side length 4 meters\n        \"\"\"\n\n        logger.info(f\"\\nRunning agent with complex task...\")\n        complex_result = agent.run(complex_task)\n\n        logger.info(f\"Complex task result: {complex_result}\")\n\n    except Exception as e:\n        logger.error(f\"Error: {e}\")\n        import traceback\n        logger.error(traceback.format_exc())\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"getting-started/ollama-setup/","title":"Ollama Deepseek Setup","text":"<p>This is an example of installing Ollama and the DeepSeek R1 7b model on my local system that has a NVIDIA RTX 2070 GPU with 12GB RAM.</p>"},{"location":"getting-started/ollama-setup/#install-ollama","title":"Install Ollama","text":"<pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre>"},{"location":"getting-started/ollama-setup/#test-ollama","title":"Test Ollama","text":"<pre><code>ollama --version\n</code></pre> <p>Response:</p> <pre><code>ollama version is 0.3.6\n</code></pre>"},{"location":"getting-started/ollama-setup/#setup-ollama-as-a-unix-service","title":"Setup Ollama as a UNIX service","text":"<pre><code>sudo systemctl edit ollama.service\n</code></pre> <p>Add the following lines:</p> <pre><code>[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0\"\nEnvironment=\"OLLAMA_ORIGINS=*\"\n</code></pre>"},{"location":"getting-started/ollama-setup/#test-the-ollama-rest-service","title":"Test the Ollama REST Service","text":"<pre><code>curl http://localhost:11434/api/version\n</code></pre> <p>Result:</p> <pre><code>{\"version\":\"0.3.6\"}\n</code></pre>"},{"location":"getting-started/ollama-setup/#ollama-shell-commands","title":"Ollama Shell Commands","text":"<pre><code>ollama --help\n</code></pre> <pre><code>Large language model runner\n\nUsage:\n  ollama [flags]\n  ollama [command]\n\nAvailable Commands:\n  serve       Start ollama\n  create      Create a model from a Modelfile\n  show        Show information for a model\n  run         Run a model\n  pull        Pull a model from a registry\n  push        Push a model to a registry\n  list        List models\n  ps          List running models\n  cp          Copy a model\n  rm          Remove a model\n  help        Help about any command\n\nFlags:\n  -h, --help      help for ollama\n  -v, --version   Show version information\n\nUse \"ollama [command] --help\" for more information about a command.\n</code></pre>"},{"location":"getting-started/ollama-setup/#download-deepseek-r1","title":"Download DeepSeek-r1","text":"<p>You can download the full 7b model by adding the keyword <code>pull</code> and the model name to ollama.</p> <pre><code>ollama pull deepseek-r1:7b\n</code></pre> <p></p>"},{"location":"getting-started/ollama-setup/#test-that-deepseek-r1-7b-is-working","title":"Test that Deepseek-r1 7B is working","text":"<pre><code>ollama run deepseek-r1:7b\n&gt;&gt;&gt; /show info\n  Model                          \n    arch                qwen2            \n    parameters          7.6B             \n    quantization        Q4_K_M           \n    context length      131072           \n    embedding length    3584             \n\n  Parameters                     \n    stop    \"&lt;\uff5cbegin\u2581of\u2581sentence\uff5c&gt;\"   \n    stop    \"&lt;\uff5cend\u2581of\u2581sentence\uff5c&gt;\"     \n    stop    \"&lt;\uff5cUser\uff5c&gt;\"                \n    stop    \"&lt;\uff5cAssistant\uff5c&gt;\"           \n\n  License                        \n    MIT License                     \n    Copyright (c) 2023 DeepSeek     \n\n&gt;&gt;&gt; what are the 10 most difficult concepts in high-school geometry to understand?\n</code></pre>"},{"location":"getting-started/ollama-setup/#deepseek-response","title":"DeepSeek Response","text":""},{"location":"getting-started/ollama-setup/#thinking","title":"Thinking","text":"<p> Okay, so I need to figure out what the top 10 most difficult concepts in high school geometry are. Hmm, where do I start? I've taken geometry before, but it's been a while, so some of these might have faded from my  memory. Let me think step by step. <p>First off, when I was learning geometry, one topic that always confused me was proofs. There were so many theorems and properties to remember, and constructing a logical argument seemed like a whole other language. I can't even recall exactly what an auxiliary line is, but I know it's used in proofs somehow.</p> <p>Then there are circles. I remember they have circumference, area, radius, diameter, and all those formulas, but when you start talking about angles subtended by chords or tangents, things got really tangled for me. It was hard to keep track of the different theorems related to circles, like the inscribed angle theorem or  something.</p> <p>Triangles are another big topic in geometry. There are so many types\u2014scalene, isosceles, equilateral\u2014and each has its own properties. Then there's Pythagoras' theorem for right-angled triangles, which I sometimes mixed  up with other formulas because they all looked similar. Also, triangle congruence and similarity were confusing at times; remembering SAS and ASA rules was a struggle.</p> <p>Coordinate geometry also tripped me up. It combines algebra and geometry, so you have to visualize shapes on the coordinate plane and use equations to find distances, slopes, and intersections. I remember struggling with finding the distance between two points using the distance formula and calculating midpoints or perpendicular lines.</p> <p>Transformations like translations, rotations, reflections, and dilations were another area where I had trouble. Understanding how figures change position or size on the coordinate plane was confusing without a clear visual. Also, similarity transformations confused me because they involved scaling, which adds another  layer of complexity.</p> <p>Trigonometry in geometry felt separate from what I had learned before. Learning sine, cosine, and tangent functions to find side lengths and angles in right triangles was initially tough. Then, when we moved on to  the unit circle, everything shifted, and it became even more abstract. I had trouble understanding how trigonometric ratios extended beyond right triangles.</p> <p>Three-dimensional geometry confused me a lot. Moving from 2D shapes to 3D solids like prisms, pyramids, cylinders, cones, and spheres introduced new concepts and formulas. Calculating surface area and volume required knowing which formula to apply when, and I often mixed them up or used the wrong dimensions in  calculations.</p> <p>Parallelograms had their own set of properties that needed to be memorized for proofs. Proving whether a quadrilateral was a parallelogram based on given information was tricky because there were multiple ways to  show it, like using opposite sides equal or parallel lines and angles.</p> <p>Probability in geometry was another point where I struggled. It wasn't just basic probability; integrating geometric principles made the problems more complex. Calculating probabilities based on areas or lengths required a good understanding of both geometry and probability concepts.</p> <p>Lastly, constructions with a compass and straightedge were a nightmare. It involved precise steps without any measurements, which felt restrictive. Forgetting a step in a construction could lead to an incorrect figure, and figuring out the right sequence without trial and error was frustrating.</p> <p>Wait, did I miss anything? Let me think... Maybe locus problems where you have to find points satisfying certain conditions. Or maybe transformations again; they were mentioned before but perhaps not explicitly listed as top 10. Hmm, since it's about difficulty, maybe those are already covered.</p> <p>I should also consider if the order makes sense. Proofs seem foundational but can be really challenging because they require logical reasoning. Then coordinate geometry and constructions build on previous knowledge with algebra and precise drawing skills. Trigonometry and circles add layers of complexity with their formulas and applications. Three-dimensional geometry might come last as it's more abstract.</p> <p>Yeah, I think that covers the main areas where students usually struggle in high school geometry. Each topic has its own challenges, but some require a strong foundation in other areas like algebra or logical reasoning. </p>"},{"location":"getting-started/ollama-setup/#answer","title":"Answer","text":"<p>The top 10 most difficult concepts in high school geometry can be categorized based on their complexity and the skills they require:</p> <ol> <li> <p>Geometric Proofs: Constructing logical arguments using axioms and previously proven theorems is challenging due to its abstract nature.</p> </li> <li> <p>Circle Theorems: Understanding properties of circles, such as angles subtended by chords, tangents, and arcs requires memorization and application of multiple theorems.</p> </li> <li> <p>Triangle Properties and Congruence/Similarity: Distinguishing between different types of triangles and applying congruence/similarity rules (e.g., SAS, ASA) can be confusing.</p> </li> <li> <p>Coordinate Geometry: Combining algebra with geometry to find distances, slopes, and intersections on the coordinate plane is complex.</p> </li> <li> <p>Transformations: Grasping concepts like translations, rotations, reflections, and dilations, especially their effects on figures in the coordinate plane, is challenging.</p> </li> <li> <p>Trigonometry: Extending trigonometric ratios beyond right triangles using the unit circle abstracts the subject and complicates problem-solving.</p> </li> <li> <p>Three-Dimensional Geometry: Calculating surface areas and volumes of 3D solids like prisms and pyramids requires applying specific formulas with attention to detail.</p> </li> <li> <p>Parallelogram Properties: Proving properties of parallelograms based on given information involves multiple proof strategies.</p> </li> <li> <p>Geometric Probability: Integrating probability concepts with geometric principles, such as area-based calculations, adds complexity.</p> </li> <li> <p>Compass and Straightedge Constructions: Performing precise constructions without measurement tools demands careful step-by-step execution.</p> </li> </ol> <p>Each concept builds upon previous knowledge, requiring a blend of logical reasoning, algebraic skills, and spatial understanding. Mastery often necessitates practice and a solid grasp of foundational topics.</p>"},{"location":"getting-started/ollama-setup/#other-ollama-commands","title":"Other Ollama Commands","text":"<p>/help Available Commands:   /set            Set session variables   /show           Show model information   /load    Load a session or model   /save    Save your current session   /clear          Clear session context   /bye            Exit   /?, /help       Help for a command   /? shortcuts    Help for keyboard shortcuts <p>Use \"\"\" to begin a multi-line message.</p>"},{"location":"getting-started/picking-agent-library/","title":"Selecting Agent Libraries","text":"<p>As of February of 2025, there are three AI Agent Libraries that are popular: </p> <ol> <li>LangChain - the first popular agent library</li> <li>LlamaIndex - a more recent library that fixed many problems with LangChain</li> <li>[SmoLAgents] - a library created by HuggingFace to work with small models on your local GPU</li> </ol> <p>Let's compare these three AI agent libraries based on their features, strengths, and limitations:</p>"},{"location":"getting-started/picking-agent-library/#langchain","title":"LangChain","text":""},{"location":"getting-started/picking-agent-library/#pros","title":"Pros","text":"<ul> <li>Comprehensive framework for building LLM-powered applications</li> <li>Strong focus on chains and composability of components</li> <li>Extensive tool and service integrations (databases, APIs, etc.)</li> <li>Robust memory and state management capabilities</li> <li>Large community with abundant documentation and examples</li> <li>Flexible architecture allowing for customization</li> <li>Well-established ecosystem with many extensions</li> </ul>"},{"location":"getting-started/picking-agent-library/#cons","title":"Cons","text":"<ul> <li>Can be complex for beginners due to its extensive feature set</li> <li>Sometimes considered overly modular, leading to complexity</li> <li>Rapid development pace can lead to breaking changes</li> <li>Higher computational requirements for some components</li> <li>Can be overkill for simpler applications</li> </ul>"},{"location":"getting-started/picking-agent-library/#llamaindex","title":"LlamaIndex","text":""},{"location":"getting-started/picking-agent-library/#pros_1","title":"Pros","text":"<ul> <li>Specialized in data indexing and retrieval for LLMs</li> <li>Excellent for RAG (Retrieval-Augmented Generation) applications</li> <li>Strong document processing capabilities</li> <li>Efficient handling of various data sources and formats</li> <li>Good performance with large document collections</li> <li>Focused architecture making it easier to learn for specific use cases</li> <li>Growing integration ecosystem</li> </ul>"},{"location":"getting-started/picking-agent-library/#cons_1","title":"Cons","text":"<ul> <li>More focused on data retrieval than end-to-end agent capabilities</li> <li>Less comprehensive agent framework compared to LangChain</li> <li>May require additional libraries for complete agent implementations</li> <li>Fewer built-in tools for complex reasoning chains</li> <li>Still evolving, with some features in experimental stages</li> </ul>"},{"location":"getting-started/picking-agent-library/#smolagents","title":"SmoLAgents","text":""},{"location":"getting-started/picking-agent-library/#pros_2","title":"Pros","text":"<ul> <li>Focuses on small language model agents, making it more efficient for certain tasks</li> <li>Designed for resource-constrained environments such as a local GPU</li> <li>Lower computational requirements compared to frameworks requiring larger models</li> <li>Good for edge devices or applications with limited processing power</li> <li>Emphasizes agent simplicity and minimalism</li> </ul>"},{"location":"getting-started/picking-agent-library/#cons_2","title":"Cons","text":"<ul> <li>Less comprehensive feature set than more established frameworks</li> <li>Smaller community and ecosystem</li> <li>More limited documentation and tutorials</li> <li>Fewer integrations with external tools and services</li> <li>May not handle extremely complex reasoning tasks as effectively as libraries using larger models</li> </ul>"},{"location":"getting-started/picking-agent-library/#summary","title":"Summary","text":"<ul> <li>SmoLAgents is best for lightweight applications with limited resources where efficiency is critical.</li> <li>LangChain excels as a comprehensive framework for complex, feature-rich AI applications with extensive integration needs.</li> <li>LlamaIndex shines for data-intensive applications where efficient document retrieval and processing are the primary concerns.</li> </ul> <p>Your choice should depend on your specific use case, resource constraints, and whether you need a complete agent framework or specialized functionality for particular aspects of AI application development.</p>"},{"location":"getting-started/smolagents-ollama/","title":"Testing SmolAgents with Ollama","text":"<pre><code># minimal-smolagents-ollama.py\nimport smolagents\nfrom smolagents import FinalAnswerTool\nimport requests\nimport logging\nimport time\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass SimplerOllamaModel:\n    \"\"\"Simplified Ollama model adapter for smolagents\"\"\"\n\n    def __init__(self, model_name, base_url=\"http://localhost:11434\"):\n        self.model_name = model_name\n        self.base_url = base_url\n        self.generate_endpoint = f\"{base_url}/api/generate\"\n        logger.info(f\"Initialized SimplerOllamaModel with model: {model_name}\")\n\n    def __call__(self, prompt, **kwargs):\n        \"\"\"Handle both direct prompts and formatted messages\"\"\"\n        # Process the prompt to handle different formats\n        processed_prompt = self._process_prompt(prompt)\n\n        # Prepare the request payload\n        payload = {\n            \"model\": self.model_name,\n            \"prompt\": processed_prompt,\n            \"stream\": False\n        }\n\n        # Send request to Ollama\n        try:\n            logger.info(f\"Sending request to Ollama, prompt length: {len(processed_prompt)} chars\")\n            response = requests.post(self.generate_endpoint, json=payload)\n\n            if response.status_code == 200:\n                result = response.json()\n                return result.get(\"response\", \"\")\n            else:\n                logger.error(f\"Ollama request failed: {response.status_code}\")\n                return f\"Error: Ollama request failed with status {response.status_code}\"\n        except Exception as e:\n            logger.error(f\"Error: {e}\")\n            return f\"Error: {str(e)}\"\n\n    def _process_prompt(self, prompt):\n        \"\"\"Convert various prompt formats to a string\"\"\"\n        if prompt is None:\n            return \"\"\n\n        if isinstance(prompt, str):\n            return prompt\n\n        if isinstance(prompt, list):\n            # If it's a list of messages (dicts with role and content)\n            if len(prompt) &gt; 0 and isinstance(prompt[0], dict):\n                result = \"\"\n                for msg in prompt:\n                    if isinstance(msg, dict) and \"content\" in msg:\n                        role = msg.get(\"role\", \"\")\n                        content = msg.get(\"content\", \"\")\n                        if role and content:\n                            result += f\"{role.upper()}: {content}\\n\\n\"\n                        else:\n                            result += f\"{content}\\n\\n\"\n                return result.strip()\n\n            # If it's a list of strings\n            return \"\\n\".join([str(item) for item in prompt if item])\n\n        # Default case - convert to string\n        return str(prompt)\n\ndef use_direct_ollama():\n    \"\"\"Use Ollama directly without smolagents\"\"\"\n    try:\n        # Create model\n        model = SimplerOllamaModel(\"deepseek-r1\")\n\n        # Test with simple prompt\n        result = model(\"What is 2 + 2? Explain step by step.\")\n        logger.info(f\"Direct Ollama result: {result[:200]}...\")\n\n        # Test with ChatML format\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"What is the sum of numbers from 1 to 5?\"}\n        ]\n\n        result2 = model(messages)\n        logger.info(f\"Messages format result: {result2[:200]}...\")\n\n        return True\n    except Exception as e:\n        logger.error(f\"Direct Ollama test failed: {e}\")\n        return False\n\ndef create_custom_agent():\n    \"\"\"Create a custom agent that uses Ollama without relying on smolagents classes\"\"\"\n    try:\n        # Create our Ollama model\n        model = SimplerOllamaModel(\"deepseek-r1\")\n\n        # Define a simple agent function\n        def simple_agent(prompt):\n            # Format the prompt to encourage step-by-step thinking\n            enhanced_prompt = f\"\"\"\nPlease solve this problem step by step:\n\n{prompt}\n\nThink through this problem carefully. First, understand what is being asked.\nThen break it down into steps and solve each step.\nFinally, provide your answer.\n\"\"\"\n            # Get response from model\n            response = model(enhanced_prompt)\n\n            # Return the result\n            return response\n\n        # Test the simple agent\n        task = \"Calculate the sum of numbers from 1 to 5.\"\n        logger.info(f\"Running simple agent with task: {task}\")\n\n        result = simple_agent(task)\n        logger.info(f\"Simple agent result: {result[:200]}...\")\n\n        # Test with complex task\n        complex_task = \"\"\"\nI need to solve this problem:\n1. Calculate the area of a rectangle with width 6 meters and length 9 meters\n2. Then calculate what percentage of that area would be covered by a square with side length 4 meters\n\"\"\"\n        logger.info(f\"Running simple agent with complex task\")\n        complex_result = simple_agent(complex_task)\n        logger.info(f\"Complex task result: {complex_result[:200]}...\")\n\n        return True\n    except Exception as e:\n        logger.error(f\"Custom agent test failed: {e}\")\n        return False\n\ndef main():\n    logger.info(\"Starting Ollama integration test...\")\n\n    # First, test direct Ollama usage\n    if use_direct_ollama():\n        logger.info(\"Direct Ollama test successful!\")\n\n        # Then, test our custom agent\n        if create_custom_agent():\n            logger.info(\"Custom agent test successful!\")\n        else:\n            logger.warning(\"Custom agent test failed.\")\n    else:\n        logger.error(\"Direct Ollama test failed. Cannot proceed.\")\n\n    logger.info(\"All tests completed.\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"getting-started/testing-libraries/","title":"Testing Supporting Libraries","text":""},{"location":"getting-started/testing-libraries/#python-library-version-test","title":"Python Library Version Test","text":"<pre><code>import torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\n</code></pre>"},{"location":"getting-started/testing-libraries/#sample-response","title":"Sample Response","text":"<pre><code>PyTorch version: 2.6.0+cu126\nCUDA available: True\nCUDA version: 12.6\n</code></pre>"}]}